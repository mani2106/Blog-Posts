#!/usr/bin/env python3
"""
Comprehensive end-to-end integration testing suite for the GitHub Tweet Thread Generator.

This test suite validates:
1. Complete workflow with sample repositories (Jekyll, fastpages)
2. GitHub Actions execution environment simulation and validation
3. Configuration loading and validation from multiple sources
4. Error handling and edge cases
5. Performance and resource usage validation

Requirements covered: 1.4, 10.1, 10.6
"""

import os
import sys
import json
import tempfile
import shutil
import subprocess
import yaml
from pathlib import Path
from typing import Dict, List, Any, Optional
import unittest
from unittest.mock import Mock, patch, MagicMock
import logging

# Add src to path for imports
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

from models import BlogPost, StyleProfile, ThreadData, GeneratorConfig, ValidationStatus
from content_detector import ContentDetector
from style_analyzer import StyleAnalyzer
from ai_orchestrator import AIOrchestrator
from engagement_optimizer import EngagementOptimizer
from content_validator import ContentValidator
from output_manager import OutputManager
from config import ConfigManager
from utils import is_github_actions_environment, get_repository_info
from logger import setup_logging

class EndToEndTestSuite:
    """Comprehensive end-to-end integration testing suite."""

    def __init__(self):
        self.test_dir = None
        self.logger = setup_logging()
        self.results = {
            'tests_run': 0,
            'tests_passed': 0,
            'tests_failed': 0,
            'failures': []
        }
        self.original_env = {}
        self.github_actions_env = {
            'GITHUB_ACTIONS': 'true',
            'GITHUB_TOKEN': 'test_github_token',
            'GITHUB_REPOSITORY': 'test-user/test-repo',
            'GITHUB_REF': 'refs/heads/main',
            'GITHUB_SHA': 'abc123def456',
            'GITHUB_ACTOR': 'test-user',
            'GITHUB_WORKFLOW': 'Test Workflow',
            'GITHUB_RUN_ID': '12345',
            'GITHUB_RUN_NUMBER': '1',
            'GITHUB_WORKSPACE': '/github/workspace',
            'OPENROUTER_API_KEY': 'test_openrouter_key'
        }

    def setup_test_environment(self):
        """Set up temporary test environment with sample repositories."""
        self.test_dir = tempfile.mkdtemp(prefix="tweet_gen_e2e_")
        self.logger.info(f"Created test environment: {self.test_dir}")

        # Create Jekyll repository structure
        self.jekyll_repo = os.path.join(self.test_dir, "jekyll_blog")
        self.create_jekyll_test_repo()

        # Create fastpages repository structure
        self.fastpages_repo = os.path.join(self.test_dir, "fastpages_blog")
        self.create_fastpages_test_repo()

        return self.test_dir

    def create_jekyll_test_repo(self):
        """Create a sample Jekyll repository with various content types."""
        os.makedirs(self.jekyll_repo, exist_ok=True)

        # Create _posts directory with sample posts
        posts_dir = os.path.join(self.jekyll_repo, "_posts")
        os.makedirs(posts_dir, exist_ok=True)

        # Technical tutorial post
        technical_post = """---
title: "Advanced Python Decorators: A Deep Dive"
date: 2024-01-15
categories: [programming, python, tutorial]
tags: [python, decorators, advanced]
summary: "Learn how to create powerful Python decorators that can transform your code"
publish: true
auto_post: false
---

# Advanced Python Decorators

Python decorators are one of the most powerful features of the language. They allow you to modify or enhance functions and classes without permanently modifying their structure.

## What Are Decorators?

A decorator is essentially a function that takes another function as an argument and extends its behavior without explicitly modifying it.

```python
def my_decorator(func):
    def wrapper():
        print("Something is happening before the function is called.")
        func()
        print("Something is happening after the function is called.")
    return wrapper

@my_decorator
def say_hello():
    print("Hello!")
```

## Advanced Patterns

### 1. Decorators with Arguments

You can create decorators that accept arguments:

```python
def repeat(times):
    def decorator(func):
        def wrapper(*args, **kwargs):
            for _ in range(times):
                result = func(*args, **kwargs)
            return result
        return wrapper
    return decorator

@repeat(3)
def greet(name):
    print(f"Hello {name}!")
```

### 2. Class-based Decorators

Sometimes it's useful to implement decorators as classes:

```python
class CountCalls:
    def __init__(self, func):
        self.func = func
        self.count = 0

    def __call__(self, *args, **kwargs):
        self.count += 1
        print(f"Call {self.count} of {self.func.__name__!r}")
        return self.func(*args, **kwargs)
```

## Real-World Applications

Decorators are incredibly useful for:
- Logging and debugging
- Authentication and authorization
- Caching and memoization
- Rate limiting
- Input validation

## Conclusion

Mastering decorators will make you a more effective Python developer. They're a powerful tool for writing clean, reusable code.

What's your favorite use case for decorators? Let me know in the comments!
"""

        with open(os.path.join(posts_dir, "2024-01-15-python-decorators.md"), "w") as f:
            f.write(technical_post)

        # Personal experience post
        personal_post = """---
title: "My Journey from Bootcamp to Senior Developer"
date: 2024-01-20
categories: [career, personal, journey]
tags: [career, growth, experience]
summary: "The ups and downs of transitioning from a coding bootcamp to a senior developer role"
publish: true
auto_post: true
---

# My Journey from Bootcamp to Senior Developer

Three years ago, I made a life-changing decision to leave my marketing career and dive into software development through a coding bootcamp. Today, I'm reflecting on that journey and the lessons learned along the way.

## The Beginning: Bootcamp Days

The bootcamp was intense. 12-hour days, constant learning, and the imposter syndrome was real. I remember thinking "Everyone else seems to get this faster than me."

But here's what I learned: **everyone feels that way**.

## First Job: Junior Developer

Landing my first job was both exciting and terrifying. The codebase was massive, the team was experienced, and I felt like I was drowning in acronyms and frameworks I'd never heard of.

### Key Lessons from Year One:
- Ask questions (even the "dumb" ones)
- Document everything you learn
- Find a mentor
- Contribute to code reviews, even as a junior

## The Growth Phase

Years 2-3 were about building confidence and expertise. I started:
- Taking on more complex features
- Mentoring newer developers
- Contributing to architectural decisions
- Speaking at local meetups

## What I Wish I Knew Earlier

1. **Technical skills are just part of the equation** - Communication and collaboration matter just as much
2. **Imposter syndrome never fully goes away** - You just get better at managing it
3. **Your bootcamp background is a strength** - You bring fresh perspectives
4. **The learning never stops** - Embrace it

## Advice for Bootcamp Grads

- Be patient with yourself
- Build projects you're passionate about
- Network genuinely (not just for jobs)
- Contribute to open source when you can
- Remember: everyone's journey is different

## Looking Forward

Now as a senior developer, I'm focused on:
- System design and architecture
- Mentoring junior developers
- Contributing to technical strategy
- Continuous learning (currently diving deep into distributed systems)

The journey from bootcamp to senior developer isn't always linear, but it's definitely possible. Trust the process, stay curious, and remember that every expert was once a beginner.

What's been your biggest challenge in your development journey? I'd love to hear your story!
"""

        with open(os.path.join(posts_dir, "2024-01-20-bootcamp-journey.md"), "w") as f:
            f.write(personal_post)

        # Tutorial post
        tutorial_post = """---
title: "Building a REST API with FastAPI: Complete Guide"
date: 2024-01-25
categories: [tutorial, api, fastapi]
tags: [python, fastapi, api, tutorial]
summary: "Step-by-step guide to building a production-ready REST API with FastAPI"
publish: true
auto_post: false
---

# Building a REST API with FastAPI: Complete Guide

FastAPI has quickly become one of my favorite frameworks for building APIs in Python. It's fast, modern, and has excellent automatic documentation. Let's build a complete API together!

## Why FastAPI?

- **Fast**: High performance, on par with NodeJS and Go
- **Fast to code**: Increase development speed by 200-300%
- **Fewer bugs**: Reduce human-induced errors by 40%
- **Intuitive**: Great editor support with auto-completion
- **Standards-based**: Based on OpenAPI and JSON Schema

## Project Setup

First, let's set up our project:

```bash
mkdir fastapi-tutorial
cd fastapi-tutorial
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install fastapi uvicorn sqlalchemy alembic
```

## Basic API Structure

Create `main.py`:

```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Optional
import uvicorn

app = FastAPI(title="Task Manager API", version="1.0.0")

# Pydantic models
class TaskBase(BaseModel):
    title: str
    description: Optional[str] = None
    completed: bool = False

class TaskCreate(TaskBase):
    pass

class Task(TaskBase):
    id: int

    class Config:
        orm_mode = True

# In-memory storage (replace with database in production)
tasks_db = []
task_id_counter = 1

@app.get("/")
async def root():
    return {"message": "Welcome to Task Manager API"}

@app.get("/tasks", response_model=List[Task])
async def get_tasks():
    return tasks_db

@app.post("/tasks", response_model=Task)
async def create_task(task: TaskCreate):
    global task_id_counter
    new_task = Task(id=task_id_counter, **task.dict())
    tasks_db.append(new_task)
    task_id_counter += 1
    return new_task

@app.get("/tasks/{task_id}", response_model=Task)
async def get_task(task_id: int):
    task = next((t for t in tasks_db if t.id == task_id), None)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")
    return task

@app.put("/tasks/{task_id}", response_model=Task)
async def update_task(task_id: int, task_update: TaskCreate):
    task = next((t for t in tasks_db if t.id == task_id), None)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    updated_task = Task(id=task_id, **task_update.dict())
    tasks_db[tasks_db.index(task)] = updated_task
    return updated_task

@app.delete("/tasks/{task_id}")
async def delete_task(task_id: int):
    task = next((t for t in tasks_db if t.id == task_id), None)
    if not task:
        raise HTTPException(status_code=404, detail="Task not found")

    tasks_db.remove(task)
    return {"message": "Task deleted successfully"}

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

## Adding Database Integration

Let's add SQLAlchemy for database operations:

```python
from sqlalchemy import create_engine, Column, Integer, String, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker, Session
from fastapi import Depends

# Database setup
SQLALCHEMY_DATABASE_URL = "sqlite:///./tasks.db"
engine = create_engine(SQLALCHEMY_DATABASE_URL, connect_args={"check_same_thread": False})
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

# Database model
class TaskModel(Base):
    __tablename__ = "tasks"

    id = Column(Integer, primary_key=True, index=True)
    title = Column(String, index=True)
    description = Column(String)
    completed = Column(Boolean, default=False)

Base.metadata.create_all(bind=engine)

# Dependency
def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

## Testing Your API

Run the server:

```bash
uvicorn main:app --reload
```

Visit `http://localhost:8000/docs` for interactive API documentation!

## Next Steps

- Add authentication with JWT tokens
- Implement proper error handling
- Add input validation
- Set up automated testing
- Deploy to production

## Conclusion

FastAPI makes building APIs incredibly straightforward while maintaining high performance and excellent developer experience. The automatic documentation generation alone makes it worth considering for your next project.

Have you tried FastAPI yet? What's your experience been like? Drop a comment below!
"""

        with open(os.path.join(posts_dir, "2024-01-25-fastapi-tutorial.md"), "w") as f:
            f.write(tutorial_post)

        # Create .generated directory
        os.makedirs(os.path.join(self.jekyll_repo, ".generated"), exist_ok=True)

        # Create .posted directory
        os.makedirs(os.path.join(self.jekyll_repo, ".posted"), exist_ok=True)

    def create_fastpages_test_repo(self):
        """Create a sample fastpages repository with notebooks."""
        os.makedirs(self.fastpages_repo, exist_ok=True)

        # Create _notebooks directory
        notebooks_dir = os.path.join(self.fastpages_repo, "_notebooks")
        os.makedirs(notebooks_dir, exist_ok=True)

        # Create _posts directory
        posts_dir = os.path.join(self.fastpages_repo, "_posts")
        os.makedirs(posts_dir, exist_ok=True)

        # Sample notebook content (simplified)
        notebook_content = """---
title: "Data Science with Pandas: Essential Operations"
date: 2024-01-30
categories: [data-science, pandas, tutorial]
tags: [python, pandas, data-analysis]
summary: "Master essential pandas operations for data manipulation and analysis"
publish: true
auto_post: true
---

# Data Science with Pandas: Essential Operations

Pandas is the backbone of data science in Python. Let's explore the most important operations you'll use daily.

## Loading Data

```python
import pandas as pd
import numpy as np

# Load from CSV
df = pd.read_csv('data.csv')

# Load from JSON
df = pd.read_json('data.json')

# Create from dictionary
data = {'name': ['Alice', 'Bob', 'Charlie'], 'age': [25, 30, 35]}
df = pd.DataFrame(data)
```

## Data Exploration

```python
# Basic info
df.info()
df.describe()
df.head()

# Check for missing values
df.isnull().sum()

# Data types
df.dtypes
```

## Data Cleaning

```python
# Handle missing values
df.dropna()  # Remove rows with NaN
df.fillna(0)  # Fill NaN with 0
df.fillna(df.mean())  # Fill with mean

# Remove duplicates
df.drop_duplicates()

# Convert data types
df['column'] = df['column'].astype('int64')
```

## Data Manipulation

```python
# Filtering
df[df['age'] > 25]
df.query('age > 25 and name == "Alice"')

# Sorting
df.sort_values('age')
df.sort_values(['age', 'name'], ascending=[False, True])

# Grouping
df.groupby('category').mean()
df.groupby('category').agg({'price': 'mean', 'quantity': 'sum'})
```

## Advanced Operations

```python
# Merging DataFrames
pd.merge(df1, df2, on='key')
pd.concat([df1, df2])

# Pivot tables
df.pivot_table(values='sales', index='month', columns='product')

# Apply functions
df['new_column'] = df['old_column'].apply(lambda x: x * 2)
```

## Performance Tips

1. **Use vectorized operations** instead of loops
2. **Set appropriate data types** to save memory
3. **Use categorical data** for repeated strings
4. **Chunk large datasets** when memory is limited

## Conclusion

These pandas operations form the foundation of most data science workflows. Master these, and you'll be well-equipped to handle real-world data challenges.

What's your most-used pandas operation? Share in the comments!
"""

        with open(os.path.join(notebooks_dir, "2024-01-30-pandas-essentials.md"), "w") as f:
            f.write(notebook_content)

        # Create .generated and .posted directories
        os.makedirs(os.path.join(self.fastpages_repo, ".generated"), exist_ok=True)
        os.makedirs(os.path.join(self.fastpages_repo, ".posted"), exist_ok=True)

    def run_test(self, test_name: str, test_func):
        """Run a single test and track results."""
        self.results['tests_run'] += 1
        try:
            self.logger.info(f"Running test: {test_name}")
            test_func()
            self.results['tests_passed'] += 1
            self.logger.info(f"‚úì {test_name} PASSED")
        except Exception as e:
            self.results['tests_failed'] += 1
            self.results['failures'].append({
                'test': test_name,
                'error': str(e),
                'type': type(e).__name__
            })
            self.logger.error(f"‚úó {test_name} FAILED: {e}")

    def test_github_actions_environment_validation(self):
        """Test GitHub Actions environment detection and validation."""
        # Test non-GitHub Actions environment
        with patch.dict(os.environ, {}, clear=True):
            assert not is_github_actions_environment(), "Should not detect GitHub Actions environment"

            repo_info = get_repository_info()
            assert all(value == "" for value in repo_info.values()), "Should return empty repo info"

        # Test GitHub Actions environment
        with patch.dict(os.environ, self.github_actions_env):
            assert is_github_actions_environment(), "Should detect GitHub Actions environment"

            repo_info = get_repository_info()
            assert repo_info['repository'] == 'test-user/test-repo'
            assert repo_info['ref'] == 'refs/heads/main'
            assert repo_info['sha'] == 'abc123def456'
            assert repo_info['actor'] == 'test-user'
            assert repo_info['run_id'] == '12345'

            # Test environment validation
            validation_result = ConfigManager.validate_environment()
            assert validation_result.status in [ValidationStatus.VALID, ValidationStatus.WARNING], \
                f"Environment validation should pass, got: {validation_result.message}"

    def test_configuration_loading_and_validation(self):
        """Test configuration loading from multiple sources and validation."""
        os.chdir(self.jekyll_repo)

        # Test 1: Environment variables only
        with patch.dict(os.environ, {
            'OPENROUTER_API_KEY': 'test_key',
            'OPENROUTER_MODEL': 'z-ai/glm-4.5-air',
            'ENGAGEMENT_LEVEL': 'high',
            'MAX_TWEETS_PER_THREAD': '8',
            'DRY_RUN': 'true'
        }):
            config = ConfigManager.load_config()
            assert config.openrouter_api_key == 'test_key'
            assert config.openrouter_model == 'z-ai/glm-4.5-air'
            assert config.engagement_optimization_level.value == 'high'
            assert config.max_tweets_per_thread == 8
            assert config.dry_run_mode is True

            # Test configuration validation
            validation_result = config.validate()
            assert validation_result.status in [ValidationStatus.VALID, ValidationStatus.WARNING], \
                f"Config validation should pass, got: {validation_result.message}"

        # Test 2: YAML configuration file
        yaml_config = {
            'models': {
                'planning': 'z-ai/glm-4.5-air',
                'creative': 'z-ai/glm-4.5-air',
                'verification': 'z-ai/glm-4.5-air'
            },
            'engagement': {
                'optimization_level': 'medium',
                'hook_variations': 5,
                'max_hashtags': 3
            },
            'output': {
                'auto_post_enabled': False,
                'dry_run_mode': False,
                'max_tweets_per_thread': 12
            },
            'directories': {
                'posts': '_posts',
                'notebooks': '_notebooks',
                'generated': '.generated',
                'posted': '.posted'
            }
        }

        config_path = Path(self.jekyll_repo) / '.github' / 'tweet-generator-config.yml'
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            yaml.dump(yaml_config, f)

        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'test_key'}):
            config = ConfigManager.load_config(str(config_path))
            assert config.openrouter_model == 'z-ai/glm-4.5-air'
            assert config.creative_model == 'z-ai/glm-4.5-air'
            assert config.engagement_optimization_level.value == 'medium'
            assert config.hook_variations_count == 5
            assert config.max_tweets_per_thread == 12

        # Test 3: Environment variables override YAML
        with patch.dict(os.environ, {
            'OPENROUTER_API_KEY': 'test_key',
            'OPENROUTER_MODEL': 'anthropic/claude-3-opus',  # Override YAML
            'ENGAGEMENT_LEVEL': 'low'  # Override YAML
        }):
            config = ConfigManager.load_config(str(config_path))
            assert config.openrouter_model == 'anthropic/claude-3-opus'  # From env
            assert config.engagement_optimization_level.value == 'low'  # From env
            assert config.creative_model == 'z-ai/glm-4.5-air'  # From YAML

        # Test 4: Invalid configuration handling
        invalid_yaml = "invalid: yaml: content: [unclosed"
        invalid_config_path = Path(self.jekyll_repo) / 'invalid-config.yml'
        with open(invalid_config_path, 'w') as f:
            f.write(invalid_yaml)

        # Should fall back to environment config without crashing
        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'test_key'}):
            config = ConfigManager.load_config(str(invalid_config_path))
            assert config.openrouter_api_key == 'test_key'

        # Test 5: Missing required configuration
        with patch.dict(os.environ, {}, clear=True):
            config = ConfigManager.load_config()
            validation_result = config.validate()
            assert validation_result.status == ValidationStatus.ERROR, \
                "Should fail validation without required API key"

    def test_jekyll_workflow_complete(self):
        """Test complete workflow with Jekyll repository in GitHub Actions environment."""
        os.chdir(self.jekyll_repo)

        # Simulate GitHub Actions environment
        with patch.dict(os.environ, self.github_actions_env):
            # Test content detection
            detector = ContentDetector()
            posts = detector.detect_changed_posts()
            assert len(posts) >= 3, f"Expected at least 3 posts, got {len(posts)}"

            # Test style analysis
            analyzer = StyleAnalyzer()
            style_profile = analyzer.build_style_profile("_posts", "_notebooks")
            assert style_profile is not None, "Style profile should not be None"
            assert len(style_profile.vocabulary_patterns) > 0, "Should have vocabulary patterns"

            # Test AI orchestration (mocked)
            with patch('src.ai_orchestrator.httpx.post') as mock_post:
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.json.return_value = {
                    "choices": [{
                        "message": {
                            "content": json.dumps({
                                "tweets": [
                                    "üßµ Thread about Python decorators (1/5)",
                                    "Decorators are functions that modify other functions...",
                                    "Here's a simple example: @my_decorator",
                                    "Advanced patterns include decorators with arguments...",
                                    "What's your favorite decorator use case? ü§î"
                                ],
                                "hashtags": ["#Python", "#Programming"]
                            })
                        }
                    }]
                }
                mock_post.return_value = mock_response

                # Test with GitHub API mocking for PR creation
                with patch('src.output_manager.Github') as mock_github:
                    mock_repo = Mock()
                    mock_pr = Mock()
                    mock_pr.html_url = "https://github.com/test/repo/pull/1"
                    mock_repo.create_pull.return_value = mock_pr
                    mock_github.return_value.get_repo.return_value = mock_repo

                    orchestrator = AIOrchestrator(
                        api_key='test_key',
                        planning_model='z-ai/glm-4.5-air'
                    )

                    config = GeneratorConfig.from_env()
                    output_manager = OutputManager(config)

                    for post in posts[:1]:  # Test with first post
                        # Generate thread content
                        thread_plan = orchestrator.generate_thread_plan(post, style_profile)
                        assert thread_plan is not None, "Thread plan should be generated"

                        # Test PR creation
                        pr_url = output_manager.create_or_update_pr(thread_plan, post)
                        assert pr_url is not None, "PR URL should be returned"

    def test_fastpages_workflow(self):
        """Test complete workflow with fastpages repository."""
        os.chdir(self.fastpages_repo)

        with patch.dict(os.environ, {
            'OPENROUTER_API_KEY': 'test_key',
            'GITHUB_TOKEN': 'test_token',
            'GITHUB_REPOSITORY': 'test/repo'
        }):
            detector = ContentDetector()
            posts = detector.detect_changed_posts()
            assert len(posts) >= 1, f"Expected at least 1 post, got {len(posts)}"

            # Test notebook content processing
            for post in posts:
                assert post.content is not None, "Post content should not be None"
                assert len(post.content) > 0, "Post content should not be empty"

    def test_style_analysis_variations(self):
        """Test style analysis with different content types."""
        os.chdir(self.jekyll_repo)

        analyzer = StyleAnalyzer()

        # Test with technical content
        technical_profile = analyzer.analyze_content_type("_posts/2024-01-15-python-decorators.md")
        assert "technical" in technical_profile.content_type_indicators

        # Test with personal content
        personal_profile = analyzer.analyze_content_type("_posts/2024-01-20-bootcamp-journey.md")
        assert "personal" in personal_profile.content_type_indicators

        # Test with tutorial content
        tutorial_profile = analyzer.analyze_content_type("_posts/2024-01-25-fastapi-tutorial.md")
        assert "tutorial" in tutorial_profile.content_type_indicators

    def test_engagement_optimization(self):
        """Test engagement optimization with different hook types."""
        optimizer = EngagementOptimizer()

        # Test curiosity gap hooks
        curiosity_hooks = optimizer.generate_curiosity_hooks("Learn advanced Python patterns")
        assert len(curiosity_hooks) > 0, "Should generate curiosity hooks"
        assert any("what if" in hook.lower() for hook in curiosity_hooks), "Should contain curiosity triggers"

        # Test contrarian hooks
        contrarian_hooks = optimizer.generate_contrarian_hooks("Everyone uses decorators wrong")
        assert len(contrarian_hooks) > 0, "Should generate contrarian hooks"

        # Test statistic hooks
        stat_hooks = optimizer.generate_statistic_hooks("90% of developers don't know this")
        assert len(stat_hooks) > 0, "Should generate statistic hooks"

        # Test story hooks
        story_hooks = optimizer.generate_story_hooks("My journey learning Python")
        assert len(story_hooks) > 0, "Should generate story hooks"

    def test_error_handling(self):
        """Test error handling and edge cases."""
        # Test API failure handling
        with patch('src.ai_orchestrator.httpx.post') as mock_post:
            mock_post.side_effect = Exception("API Error")

            orchestrator = AIOrchestrator()
            try:
                result = orchestrator.generate_thread_content(None, None)
                # Should handle gracefully
            except Exception as e:
                assert "API Error" in str(e) or isinstance(e, (ConnectionError, TimeoutError))

        # Test invalid content handling
        validator = ContentValidator()

        # Test character limit validation
        long_tweet = "x" * 300  # Over 280 character limit
        result = validator.validate_character_limits([long_tweet])
        assert not result.is_valid, "Should fail character limit validation"

        # Test content safety
        inappropriate_content = "This contains profanity and inappropriate content"
        safety_result = validator.check_content_safety(inappropriate_content)
        # Should flag or filter appropriately

    def test_github_actions_workflow_integration(self):
        """Test GitHub Actions workflow integration and main script execution."""
        os.chdir(self.jekyll_repo)

        # Test main script execution in dry-run mode
        with patch.dict(os.environ, {**self.github_actions_env, 'DRY_RUN': 'true'}):
            # Mock all external API calls
            with patch('src.ai_orchestrator.httpx.post') as mock_openrouter, \
                 patch('src.output_manager.Github') as mock_github, \
                 patch('src.output_manager.tweepy.Client') as mock_twitter:

                # Setup OpenRouter mock
                mock_openrouter_response = Mock()
                mock_openrouter_response.status_code = 200
                mock_openrouter_response.json.return_value = {
                    "choices": [{
                        "message": {
                            "content": json.dumps({
                                "tweets": ["Test tweet 1", "Test tweet 2"],
                                "hashtags": ["#Test"]
                            })
                        }
                    }]
                }
                mock_openrouter.return_value = mock_openrouter_response

                # Setup GitHub mock
                mock_repo = Mock()
                mock_pr = Mock()
                mock_pr.html_url = "https://github.com/test/repo/pull/1"
                mock_repo.create_pull.return_value = mock_pr
                mock_github.return_value.get_repo.return_value = mock_repo

                # Import and run main script
                sys.path.insert(0, str(Path(__file__).parent))
                try:
                    import generate_and_commit
                    result = generate_and_commit.main()
                    assert result == 0, "Main script should complete successfully"
                except ImportError as e:
                    # If import fails, test the core workflow components directly
                    self.logger.warning(f"Could not import main script: {e}")
                    self._test_workflow_components_directly()

    def _test_workflow_components_directly(self):
        """Test workflow components directly when main script import fails."""
        # Test configuration loading
        config = ConfigManager.load_config()
        assert config is not None, "Configuration should load"

        # Test content detection
        detector = ContentDetector()
        posts = detector.detect_changed_posts()
        assert isinstance(posts, list), "Should return list of posts"

        # Test style analysis
        analyzer = StyleAnalyzer()
        style_profile = analyzer.build_style_profile("_posts", "_notebooks")
        assert style_profile is not None, "Style profile should be created"

    def test_github_actions_outputs(self):
        """Test GitHub Actions outputs are properly set."""
        os.chdir(self.jekyll_repo)

        # Create a mock GITHUB_OUTPUT file
        output_file = Path(self.test_dir) / "github_output"

        with patch.dict(os.environ, {
            **self.github_actions_env,
            'GITHUB_OUTPUT': str(output_file),
            'DRY_RUN': 'true'
        }):
            # Mock the main workflow
            with patch('src.ai_orchestrator.httpx.post') as mock_post:
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.json.return_value = {
                    "choices": [{
                        "message": {
                            "content": json.dumps({
                                "tweets": ["Test tweet"],
                                "hashtags": ["#Test"]
                            })
                        }
                    }]
                }
                mock_post.return_value = mock_response

                # Test that outputs are written
                sys.path.insert(0, str(Path(__file__).parent))
                try:
                    import generate_and_commit

                    # Mock the set_github_actions_output function
                    with patch.object(generate_and_commit, 'set_github_actions_output') as mock_set_output:
                        generate_and_commit.main()

                        # Verify outputs were set
                        expected_calls = [
                            ('threads_generated', '0'),
                            ('posts_processed', '0'),
                            ('pr_created', 'false')
                        ]

                        for expected_call in expected_calls:
                            mock_set_output.assert_any_call(*expected_call)

                except ImportError:
                    # If we can't import the main script, just test the output function
                    def set_github_actions_output(key: str, value: str) -> None:
                        with open(output_file, "a") as f:
                            f.write(f"{key}={value}\n")

                    set_github_actions_output("threads_generated", "1")
                    set_github_actions_output("posts_processed", "2")
                    set_github_actions_output("pr_created", "true")

                    # Verify outputs were written
                    assert output_file.exists(), "GitHub output file should be created"
                    content = output_file.read_text()
                    assert "threads_generated=1" in content
                    assert "posts_processed=2" in content
                    assert "pr_created=true" in content

    def test_different_repository_structures(self):
        """Test with different repository structures and configurations."""
        # Test with missing directories
        temp_repo = os.path.join(self.test_dir, "minimal_repo")
        os.makedirs(temp_repo, exist_ok=True)
        os.chdir(temp_repo)

        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'test_key'}):
            # Mock git operations since we're not in a real git repository
            with patch('subprocess.run') as mock_run:
                mock_run.return_value.returncode = 0
                mock_run.return_value.stdout = ""

                detector = ContentDetector()
                posts = detector.detect_changed_posts()
                assert isinstance(posts, list), "Should return empty list for no posts"

            # Test environment validation with missing directories
            validation_result = ConfigManager.validate_environment()
            assert validation_result.status in [ValidationStatus.WARNING, ValidationStatus.VALID], \
                "Should handle missing directories gracefully"

        # Test with custom configuration
        config_content = """
models:
  planning: z-ai/glm-4.5-air
  creative: z-ai/glm-4.5-air
  verification: z-ai/glm-4.5-air

engagement:
  optimization_level: high
  hook_variations: 5
  max_hashtags: 3

output:
  auto_post_enabled: false
  dry_run_mode: true
  max_tweets_per_thread: 8

directories:
  posts: custom_posts
  notebooks: custom_notebooks
  generated: custom_generated
  posted: custom_posted
"""

        config_dir = os.path.join(temp_repo, ".github")
        os.makedirs(config_dir, exist_ok=True)
        config_path = os.path.join(config_dir, "tweet-generator-config.yml")
        with open(config_path, "w") as f:
            f.write(config_content)

        # Test configuration loading
        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'test_key'}):
            config = ConfigManager.load_config(config_path)
            assert config.engagement_optimization_level.value == "high"
            assert config.max_tweets_per_thread == 8
            assert config.posts_directory == "custom_posts"
            assert config.notebooks_directory == "custom_notebooks"

    def test_performance_and_resource_validation(self):
        """Test performance characteristics and resource usage."""
        os.chdir(self.jekyll_repo)

        # Create additional test posts to simulate larger repository
        posts_dir = Path(self.jekyll_repo) / "_posts"
        for i in range(10):  # Create 10 additional posts
            post_content = f"""---
title: "Test Post {i}"
date: 2024-01-{i+1:02d}
categories: [test]
summary: "Test post {i} for performance testing"
publish: true
---

# Test Post {i}

This is test content for post {i}. It contains enough text to test style analysis
and content processing performance. The content includes technical terms,
casual language, and various formatting elements.

## Section 1

Some technical content with code examples and explanations.

## Section 2

More content to analyze for style patterns and vocabulary.
"""
            with open(posts_dir / f"2024-01-{i+1:02d}-test-post-{i}.md", "w") as f:
                f.write(post_content)

        with patch.dict(os.environ, {'OPENROUTER_API_KEY': 'test_key'}):
            import time

            # Test style analysis performance
            start_time = time.time()
            analyzer = StyleAnalyzer()
            style_profile = analyzer.build_style_profile("_posts", "_notebooks")
            analysis_time = time.time() - start_time

            assert analysis_time < 30.0, f"Style analysis took too long: {analysis_time:.2f}s"
            assert style_profile is not None, "Style profile should be created"
            assert style_profile.posts_analyzed >= 10, "Should analyze multiple posts"

            # Test content detection performance
            start_time = time.time()
            detector = ContentDetector()
            posts = detector.detect_changed_posts()
            detection_time = time.time() - start_time

            assert detection_time < 10.0, f"Content detection took too long: {detection_time:.2f}s"
            assert len(posts) >= 10, "Should detect multiple posts"

            # Test memory usage (basic check)
            import psutil
            process = psutil.Process()
            memory_mb = process.memory_info().rss / 1024 / 1024
            assert memory_mb < 500, f"Memory usage too high: {memory_mb:.1f}MB"

    def cleanup_test_environment(self):
        """Clean up test environment."""
        if self.test_dir and os.path.exists(self.test_dir):
            try:
                # On Windows, we need to handle file locks more carefully
                import time
                import stat

                def handle_remove_readonly(func, path, exc):
                    """Handle readonly files on Windows."""
                    if os.path.exists(path):
                        os.chmod(path, stat.S_IWRITE)
                        func(path)

                # Wait a bit for any file handles to close
                time.sleep(0.1)

                # Remove with error handler for Windows
                shutil.rmtree(self.test_dir, onerror=handle_remove_readonly)
                self.logger.info(f"Cleaned up test environment: {self.test_dir}")
            except Exception as e:
                self.logger.warning(f"Failed to clean up test environment: {e}")
                # Try to clean up individual files
                try:
                    for root, dirs, files in os.walk(self.test_dir, topdown=False):
                        for file in files:
                            try:
                                file_path = os.path.join(root, file)
                                os.chmod(file_path, stat.S_IWRITE)
                                os.remove(file_path)
                            except:
                                pass
                        for dir in dirs:
                            try:
                                os.rmdir(os.path.join(root, dir))
                            except:
                                pass
                    os.rmdir(self.test_dir)
                except:
                    self.logger.warning(f"Could not fully clean up test directory: {self.test_dir}")

    def backup_environment(self):
        """Backup current environment variables."""
        self.original_env = dict(os.environ)

    def restore_environment(self):
        """Restore original environment variables."""
        os.environ.clear()
        os.environ.update(self.original_env)

    def run_all_tests(self):
        """Run all end-to-end integration tests."""
        self.logger.info("Starting comprehensive end-to-end integration testing...")
        self.logger.info("Testing requirements: 1.4 (GitHub Actions integration), 10.1 (configuration), 10.6 (validation)")

        try:
            self.backup_environment()
            self.setup_test_environment()

            # Core integration tests
            self.run_test("GitHub Actions Environment Validation", self.test_github_actions_environment_validation)
            self.run_test("Configuration Loading and Validation", self.test_configuration_loading_and_validation)
            self.run_test("Jekyll Workflow Complete", self.test_jekyll_workflow_complete)
            self.run_test("Fastpages Workflow", self.test_fastpages_workflow)

            # GitHub Actions specific tests
            self.run_test("GitHub Actions Workflow Integration", self.test_github_actions_workflow_integration)
            self.run_test("GitHub Actions Outputs", self.test_github_actions_outputs)

            # Edge case and performance tests
            self.run_test("Style Analysis Variations", self.test_style_analysis_variations)
            self.run_test("Engagement Optimization", self.test_engagement_optimization)
            self.run_test("Error Handling", self.test_error_handling)
            self.run_test("Different Repository Structures", self.test_different_repository_structures)
            self.run_test("Performance and Resource Validation", self.test_performance_and_resource_validation)

        finally:
            self.cleanup_test_environment()
            self.restore_environment()

        # Print results
        self.print_results()
        return self.results

    def print_results(self):
        """Print test results summary."""
        print("\n" + "="*60)
        print("END-TO-END TEST RESULTS")
        print("="*60)
        print(f"Tests Run: {self.results['tests_run']}")
        print(f"Tests Passed: {self.results['tests_passed']}")
        print(f"Tests Failed: {self.results['tests_failed']}")

        if self.results['failures']:
            print("\nFAILURES:")
            for failure in self.results['failures']:
                print(f"  ‚úó {failure['test']}: {failure['type']} - {failure['error']}")

        success_rate = (self.results['tests_passed'] / self.results['tests_run']) * 100
        print(f"\nSuccess Rate: {success_rate:.1f}%")

        if success_rate >= 80:
            print("üéâ End-to-end testing PASSED!")
        else:
            print("‚ùå End-to-end testing FAILED!")

        print("="*60)

if __name__ == "__main__":
    suite = EndToEndTestSuite()
    results = suite.run_all_tests()

    # Exit with appropriate code
    sys.exit(0 if results['tests_failed'] == 0 else 1)