{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Testing out தமிழ் language tokenizer\"\n",
    "> \"In this notebook I test out the sentencepiece tokenizers built previously\"\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [nlp, language-model, தமிழ்]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the which I built the tokenizers [here](https://www.kaggle.com/manimaranp/building-a-tokenizer-for-tamil-with-sentencepiece)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "This notebook is intended to experiment with different tokenizers built previously, with varying `vocab_size` values like `8000`, `16000`, `20000`, `30000`, So how do we exactly test a **tokenizer**?, this brings us to why do we need a tokenizer in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "In `English` language we can easily find meaningful units of a sentence, by **whitespaces**. But it is not that easy in other languages, Like in `தமிழ்`, Consider the following sentence,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "**இந்த தொழிற்சாலை பணப்பற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.**, this loosely translates to something like **The factory has come in to operation after over 17 years of series of disruptions, including lack of cash.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "We need to focus on the compound word `பணப்பற்றாக்குறை` which means `lack of cash`, that compound word is actually a combination of two words `பணம்` and `பற்றாக்குறை` representing `Cash` and `Deficiency/lack of` A tokenizer for `தமிழ்`, should split the words into two as mentioned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "Since Tokenization is `the process of demarcating and possibly classifying sections of a string of input characters`<sup>[1](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization)</sup> or in simple words identifying **linguistically meaningful units** <sup>[2](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language)</sup> for further processing. We need it for a language model which essentially built to understand the language that it is built for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we built the tokenizer in an **unsupervised**<sup>[3](https://github.com/google/sentencepiece#sentencepiece)</sup> way, There are no numerical ways of gauging the efficiency of the tokenization (AFAIK), so we are left with to try and tokenize some random sentences with the model, and check the tokenizer ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences for testing tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have chosen some sentences at random, Check the comments above each for their translation in `English`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # Sita is a mischievous girl\n",
    "    'சீதா ஒரு குறும்பு பெண்',\n",
    "    # I remember my childhood\n",
    "    'எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது',\n",
    "    # India has successfully tested the Agni-5 missile for the fourth time from Abdul Kalam Island (Wheeler Island) in Odisha.\n",
    "    'இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.',\n",
    "    # The European Union's Galileo satellite system is in operation. It is believed to be the world's most accurate high-precision positioning system.\n",
    "    'ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.',\n",
    "    # The factory has come in to operation after over 17 years of series of disruptions, including lack of cash.\n",
    "    'இந்த தொழிற்சாலை பணபற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.',\n",
    "    # Citizens, witnesses and warriors mourn the death of their king. It is up to the department to regret any loss.\n",
    "    'தம் மன்னன் இறந்ததற்கு குடிமக்களும் சான்றோரும் வீரர்களும் வருந்திப் பாடுவது கையறுநிலை என்னும் துறையாகும். எந்த இழப்பையும் எண்ணி வருந்துவது கையறுநிலைத் துறைக்குரியது.',\n",
    "    # The Poems from Sangam Tamil Literature portrays the trading feats of early Tamilian,Tamilians traded across seas and other countries\n",
    "    'சங்கத்தமிழ்க் கவிதைகள் பழந்தமிழர்தம் வணிகச்சிறப்பைப் பறைசாற்றி நிற்கின்றன. தமிழர் கடல்கடந்து, அயல்நாடுகளிலும் வணிகம் செய்தனர் என்ற செய்திகளைச் சங்கப்பாடல்கள்வழி அறிகின்றோம்.',\n",
    "    # Everyone stood up to call for a national flag at a school event.\n",
    "    'பள்ளி நிகழ்ச்சி ஒன்றில் தேசியக் கொடி ஏற்றுமாறு அழைக்க அவரும் எழுந்தார் அனைவரும் எழுந்து நின்றனர்',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to tokenize each one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "from pathlib import Path\n",
    "from IPython.core.display import display, HTML\n",
    "from string import Template\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "\n",
    "TOK_PATH = '/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer'\n",
    "\n",
    "MODEL_PATHS = [p for p in Path(TOK_PATH).glob('*.model')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer/tok_30000_size.model')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#hide\n",
    "# Just checking\n",
    "MODEL_PATHS[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "td_text = Template('<td>$text</td>')\n",
    "tr_text = Template('<tr><td>$name</td> $li_elem</tr>')\n",
    "\n",
    "# Utility to generate HTML text for a neat display\n",
    "def tokenize_and_display_results(text):\n",
    "    model_texts = []\n",
    "    for model in MODEL_PATHS:\n",
    "        # Load model\n",
    "        sp.Load(str(model))\n",
    "        \n",
    "        # tokenize\n",
    "        tok = sp.EncodeAsPieces(text)\n",
    "        \n",
    "        # Prepare html with string templates\n",
    "        word_html = ''.join([td_text.substitute(text=word) for word in tok])\n",
    "        list_html = tr_text.substitute(name=model.stem, li_elem=word_html)\n",
    "        model_texts.append(list_html)\n",
    "    \n",
    "    return display(HTML('<table>' + ''.join(model_texts) + '</table>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try to comment on the tokenization with a limited knowledge of தமிழ் grammar, I will refer to the model by the vocab size they are built with (ie) 8000, 16000, 20000, 30000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First sentence `சீதா ஒரு குறும்பு பெண்`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>tok_30000_size</td> <td>▁சீதா</td><td>▁ஒரு</td><td>▁குறும்ப</td><td>ு</td><td>▁பெண்</td></tr><tr><td>tok_20000_size</td> <td>▁சீதா</td><td>▁ஒரு</td><td>▁குறும்ப</td><td>ு</td><td>▁பெண்</td></tr><tr><td>tok_8000_size</td> <td>▁சீதா</td><td>▁ஒரு</td><td>▁குறு</td><td>ம்பு</td><td>▁பெண்</td></tr><tr><td>tok_16000_size</td> <td>▁சீதா</td><td>▁ஒரு</td><td>▁குறும்ப</td><td>ு</td><td>▁பெண்</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_and_display_results(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`குறும்பு` is actually not a compound word, so I think apart from the model with `8000` size all other models got it right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next sentence `எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>tok_30000_size</td> <td>▁எனக்கு</td><td>▁என்</td><td>▁குழந்தைப்</td><td>▁பருவம்</td><td>▁நினைவ</td><td>ிருக்கிறது</td></tr><tr><td>tok_20000_size</td> <td>▁எனக்கு</td><td>▁என்</td><td>▁குழந்தைப்</td><td>▁பருவம்</td><td>▁நினைவ</td><td>ிருக்கிறது</td></tr><tr><td>tok_8000_size</td> <td>▁என</td><td>க்கு</td><td>▁என்</td><td>▁குழந்தை</td><td>ப்</td><td>▁பருவ</td><td>ம்</td><td>▁நினைவ</td><td>ிருக்கிறது</td></tr><tr><td>tok_16000_size</td> <td>▁எனக்கு</td><td>▁என்</td><td>▁குழந்தை</td><td>ப்</td><td>▁பருவம்</td><td>▁நினைவ</td><td>ிருக்கிறது</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_and_display_results(sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this sentence `I remember my childhood` the model with `20000` and `30000` got the tokenization right.\n",
    "`16000` was just close, because the letter `ப்` does not have a meaning on it's own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next sentence `இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>tok_30000_size</td> <td>▁இந்தியா</td><td>▁அக்னி</td><td>-</td><td>5</td><td>▁வகை</td><td>▁ஏவுகணை</td><td>யை</td><td>▁நான்காவது</td><td>▁முறையாக</td><td>▁வெற்றிகரமாக</td><td>▁ஒடிசாவில</td><td>ுள்ள</td><td>▁அப்துல்</td><td>▁க</td><td>லாம்</td><td>▁தீ</td><td>விலிருந்து</td><td>▁</td><td>(</td><td>வீ</td><td>லர்</td><td>▁தீவு</td><td>)</td><td>▁சோதி</td><td>த்தது</td><td>.</td></tr><tr><td>tok_20000_size</td> <td>▁இந்தியா</td><td>▁அக்னி</td><td>-</td><td>5</td><td>▁வகை</td><td>▁ஏவுகணை</td><td>யை</td><td>▁நான்காவது</td><td>▁முறையாக</td><td>▁வெற்றிகரமாக</td><td>▁ஒடிசா</td><td>விலுள்ள</td><td>▁அப்துல்</td><td>▁க</td><td>லாம்</td><td>▁தீ</td><td>விலிருந்து</td><td>▁</td><td>(</td><td>வீ</td><td>லர்</td><td>▁தீவு</td><td>)</td><td>▁சோதி</td><td>த்தது</td><td>.</td></tr><tr><td>tok_8000_size</td> <td>▁இந்தியா</td><td>▁அக்</td><td>னி</td><td>-</td><td>5</td><td>▁வகை</td><td>▁ஏவுகணை</td><td>யை</td><td>▁நான்காவது</td><td>▁முறையாக</td><td>▁வெற்றிகரமாக</td><td>▁ஒடிசா</td><td>வ</td><td>ிலுள்ள</td><td>▁அப்துல்</td><td>▁க</td><td>லாம்</td><td>▁தீ</td><td>விலிருந்து</td><td>▁</td><td>(</td><td>வீ</td><td>லர்</td><td>▁தீவு</td><td>)</td><td>▁சோ</td><td>தி</td><td>த்தது</td><td>.</td></tr><tr><td>tok_16000_size</td> <td>▁இந்தியா</td><td>▁அக்னி</td><td>-</td><td>5</td><td>▁வகை</td><td>▁ஏவுகணை</td><td>யை</td><td>▁நான்காவது</td><td>▁முறையாக</td><td>▁வெற்றிகரமாக</td><td>▁ஒடிசா</td><td>விலுள்ள</td><td>▁அப்துல்</td><td>▁க</td><td>லாம்</td><td>▁தீ</td><td>விலிருந்து</td><td>▁</td><td>(</td><td>வீ</td><td>லர்</td><td>▁தீவு</td><td>)</td><td>▁சோதி</td><td>த்தது</td><td>.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_and_display_results(sentences[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting, We can probably say that `8000` vocab model is not doing so well. Looking at the others, both the `16000` vocab model and `20000` split `ஒடிசாவிலுள்ள` meaning to refer something `in Odisha` into `ஒடிசா` and `விலுள்ள`, I think the right split is `ஒடிசாவில்` and `உள்ள`, which the `30000` got right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Sentence\n",
    "\n",
    "`ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு\n",
    "செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான\n",
    "செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td>tok_30000_size</td> <td>▁ஐரோப்பிய</td><td>▁ஒன்றியத்தின்</td><td>▁கலி</td><td>லியோ</td><td>▁செயற்கை</td><td>கோள்</td><td>▁அமைப்பு</td><td>▁செயல்பாட்டு</td><td>க்கு</td><td>▁வந்துள்ளது</td><td>.</td><td>▁இது</td><td>▁உலகின்</td><td>▁மிக</td><td>▁துல்லியமான</td><td>▁செய்மதி</td><td>▁இட</td><td>ஞ்சு</td><td>ட்ட</td><td>லாக</td><td>▁இருக்கும்</td><td>▁என</td><td>▁நம்பப்படுகிறது</td><td>.</td></tr><tr><td>tok_20000_size</td> <td>▁ஐரோப்பிய</td><td>▁ஒன்றியத்தின்</td><td>▁கலி</td><td>லியோ</td><td>▁செயற்கை</td><td>கோள்</td><td>▁அமைப்பு</td><td>▁செயல்பாட்டு</td><td>க்கு</td><td>▁வந்துள்ளது</td><td>.</td><td>▁இது</td><td>▁உலகின்</td><td>▁மிக</td><td>▁துல்லியமான</td><td>▁செய்மதி</td><td>▁இட</td><td>ஞ்சு</td><td>ட்ட</td><td>லாக</td><td>▁இருக்கும்</td><td>▁என</td><td>▁நம்பப்படுகிறது</td><td>.</td></tr><tr><td>tok_8000_size</td> <td>▁ஐரோப்பிய</td><td>▁ஒன்றியத்தின்</td><td>▁கலி</td><td>லி</td><td>யோ</td><td>▁செயற்கை</td><td>கோள</td><td>்</td><td>▁அமைப்பு</td><td>▁செயல்பாட்ட</td><td>ுக்கு</td><td>▁வந்துள்ளது</td><td>.</td><td>▁இது</td><td>▁உலகின்</td><td>▁மிக</td><td>▁துல்லிய</td><td>மான</td><td>▁செய்</td><td>மதி</td><td>▁இட</td><td>ஞ்சு</td><td>ட்ட</td><td>லாக</td><td>▁இருக்கும்</td><td>▁என</td><td>▁நம்பப்படுகிறது</td><td>.</td></tr><tr><td>tok_16000_size</td> <td>▁ஐரோப்பிய</td><td>▁ஒன்றியத்தின்</td><td>▁கலி</td><td>லியோ</td><td>▁செயற்கை</td><td>கோள்</td><td>▁அமைப்பு</td><td>▁செயல்பாட்டு</td><td>க்கு</td><td>▁வந்துள்ளது</td><td>.</td><td>▁இது</td><td>▁உலகின்</td><td>▁மிக</td><td>▁துல்லியமான</td><td>▁செய்மதி</td><td>▁இட</td><td>ஞ்சு</td><td>ட்ட</td><td>லாக</td><td>▁இருக்கும்</td><td>▁என</td><td>▁நம்பப்படுகிறது</td><td>.</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_and_display_results(sentences[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of them may have got it wrong with `Galileo` which is split into `கலி`(Gali) and `லியோ`(leo), (Maybe I am wrong on this part) and in GPS `இடஞ்சுட்டலாக` may be called as `இடம்சுட்டல்`(can be loosely translated to Location Pointer) should have been split like `இடம்` and `சுட்டல்` and `ஆக`, Maybe I am expecting too much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will stop here for this blogpost, You can try the other sentences if you want by forking the notebook [here](https://www.kaggle.com/manimaranp/testing-sentencepiece-tokenizer-for-tamil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please comment if you think something is wrong somewhere, share it if you have found this interesting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
