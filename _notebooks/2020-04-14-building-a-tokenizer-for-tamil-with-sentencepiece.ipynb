{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Building `தமிழ்` language tokenizer\"\n",
    "> \"In this notebook I try to build sentencepiece tokenizers for built தமிழ்\"\n",
    "- toc: false\n",
    "- branch: master\n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [nlp, language-model, தமிழ்]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import sentencepiece as spm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Read data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48482</td>\n",
       "      <td>https://ta.wikipedia.org/wiki?curid=48482</td>\n",
       "      <td>தென் துருவம்</td>\n",
       "      <td>தென் துருவம் தென் முனை தென் துருவம் என்பது புவ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48485</td>\n",
       "      <td>https://ta.wikipedia.org/wiki?curid=48485</td>\n",
       "      <td>ஆர்க்டிக் வட்டம்</td>\n",
       "      <td>ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48486</td>\n",
       "      <td>https://ta.wikipedia.org/wiki?curid=48486</td>\n",
       "      <td>நாஞ்சில் நாடன்</td>\n",
       "      <td>நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48492</td>\n",
       "      <td>https://ta.wikipedia.org/wiki?curid=48492</td>\n",
       "      <td>டிக்கோயா</td>\n",
       "      <td>டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48493</td>\n",
       "      <td>https://ta.wikipedia.org/wiki?curid=48493</td>\n",
       "      <td>நள்ளிரவுச் சூரியன்</td>\n",
       "      <td>நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                        url               title  \\\n",
       "0  48482  https://ta.wikipedia.org/wiki?curid=48482        தென் துருவம்   \n",
       "1  48485  https://ta.wikipedia.org/wiki?curid=48485    ஆர்க்டிக் வட்டம்   \n",
       "2  48486  https://ta.wikipedia.org/wiki?curid=48486      நாஞ்சில் நாடன்   \n",
       "3  48492  https://ta.wikipedia.org/wiki?curid=48492            டிக்கோயா   \n",
       "4  48493  https://ta.wikipedia.org/wiki?curid=48493  நள்ளிரவுச் சூரியன்   \n",
       "\n",
       "                                                text  \n",
       "0  தென் துருவம் தென் முனை தென் துருவம் என்பது புவ...  \n",
       "1  ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து...  \n",
       "2  நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்...  \n",
       "3  டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின...  \n",
       "4  நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang_data = pd.read_csv('../input/tamil-wiki-data-extraction/filtered_data.csv.tar.gz', index_col=[0])\n",
    "lang_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 133412 entries, 0 to 133411\n",
      "Data columns (total 4 columns):\n",
      "id       133412 non-null int64\n",
      "url      133412 non-null object\n",
      "title    133412 non-null object\n",
      "text     133412 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 5.1+ MB\n"
     ]
    }
   ],
   "source": [
    "lang_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directories\n",
    "OUTPUT_DIR = Path('/kaggle/working')\n",
    "TEXTS_DIR = OUTPUT_DIR/'texts'\n",
    "TOK_DIR = OUTPUT_DIR/'tokenizer'\n",
    "\n",
    "# Create directories\n",
    "TOK_DIR.mkdir()\n",
    "TEXTS_DIR.mkdir()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass a list of files as a comma seperated string according to [documentation](https://github.com/google/sentencepiece#train-sentencepiece-model), So we can store each article in a text file and pass the names in a comma seperated string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all article texts in seperate files\n",
    "\n",
    "for t in lang_data.itertuples():\n",
    "    file_name = Path(TEXTS_DIR/f'text_{t.Index}.txt')\n",
    "    file_name.touch()\n",
    "    with file_name.open('w') as f:\n",
    "        f.write(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(133412, 133412)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check files in directory\n",
    "len([t for t in TEXTS_DIR.iterdir()]), lang_data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the files have been converted to texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train sentencepiece model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a comma seperated string of filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/texts/text_40902.txt,/kaggle/working/texts/text_44212.txt,/kaggle/working/texts/text'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = ','.join([str(t) for t in TEXTS_DIR.iterdir()])\n",
    "files[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must find the right `vocab_size` for the tokenizer, that can be done only by testing the tokenizer after building onw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with vocab set as: 8000\n",
      "Training with vocab set as: 16000\n",
      "Training with vocab set as: 20000\n",
      "Training with vocab set as: 30000\n"
     ]
    }
   ],
   "source": [
    "for v in 8000, 16000, 20000, 30000:\n",
    "    api_str = f\"\"\"--input={files} --vocab_size={v} --model_type=unigram --character_coverage=0.9995 --model_prefix={str(TOK_DIR)}/tok_{v}_size --max_sentence_length=20000\"\"\"\n",
    "    print(\"Training with vocab set as:\", v)\n",
    "    spm.SentencePieceTrainer.train(api_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/texts/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the models in another notebook, you can find the outputs in this kaggle [notebook](https://www.kaggle.com/manimaranp/building-a-tokenizer-for-tamil-with-sentencepiece)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
