{
  
    
        "post0": {
            "title": "Mother's day Sentiment analysis",
            "content": "This notebook explores the data from HackerEarth Machine learning challenge for Mother&#39;s day, The following is the Problem description. . You work in an event management company. On Mother&#39;s Day, your company has organized an event where they want to cast positive Mother&#39;s Day related tweets in a presentation. Data engineers have already collected the data related to Mother&#39;s Day that must be categorized into positive, negative, and neutral tweets. You are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral. . Download the data . import requests zip_file = requests.get(&#39;https://he-s3.s3.amazonaws.com/media/hackathon/hackerearth-test-draft-1-102/predicting-tweet-sentiments-231101b4/fa62f5d69a9f11ea.zip?Signature=v92IcNfljnopA9xQoCPCftwg1g0%3D&amp;Expires=1590318817&amp;AWSAccessKeyId=AKIA6I2ISGOYH7WWS3G5&#39;) with open(&#39;data.zip&#39;, &#39;wb&#39;) as f: f.write(zip_file.content) . !unzip data.zip . Archive: data.zip creating: dataset/ inflating: dataset/train.csv inflating: dataset/test.csv . %load_ext google.colab.data_table . Peek at the data . from pathlib import Path import pandas as pd DATA_PATH = Path(&#39;dataset/&#39;) train_data = pd.read_csv(DATA_PATH/&#39;train.csv&#39;, index_col=0) train_data.head(100) . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . ... ... | ... | ... | ... | ... | . 1.244092e+18 Happy Mothers Day from all of us at Kellyzola ... | en | 1 | design_pia | 0 | . 1.246529e+18 Happy Mothers Day katiebrooks_88 . I do my lev... | en | 1 | missny99 | 0 | . 1.244747e+18 RESPECT, GRATITUDE and ADORATION to all MOTHER... | en | 0 | EgbertsTreasure | 0 | . 1.245141e+18 It takes someone really brave to be a mother, ... | en | 0 | momaferd | -1 | . 1.245903e+18 On Mother&#39;s Day, I&#39;m sharing this video again ... | en | 0 | GotMommyBrain | -1 | . 100 rows × 5 columns . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Float64Index: 3235 entries, 1.24502457848689e+18 to 1.24540908968687e+18 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 original_text 3235 non-null object 1 lang 3231 non-null object 2 retweet_count 3231 non-null object 3 original_author 3235 non-null object 4 sentiment_class 3235 non-null int64 dtypes: int64(1), object(4) memory usage: 151.6+ KB . So there are four columns . 1. The tweet text 2. Language of the tweet 3. Number of Retweets 4. Sentiment group (+ve, -ve, neu) . There are also some missing values in lang and retweet_count columns. . Let&#39;s look at the number of languages of tweets this dataset has . train_data[&#39;lang&#39;].value_counts() . en 2994 pink Peruvian opal! via 4 Find More 2 &amp;gt 2 WORLDS OKAYEST MOTHER! &amp;lt 2 ... 0.4754834129 1 -0.0064143617 1 -0.3850425633 1 0.7885519508 1 -0.2758448854 1 Name: lang, Length: 232, dtype: int64 . I see that most tweets are in English, but it seems that some entries in the data are not actually indicating any language. . print(&#39;Total Other language tweets: &#39;, train_data.shape[0].item()-2994) . Total Other language tweets: 241 . Let&#39;s see if they are actually in different languages. . train_data.loc[train_data[&#39;lang&#39;] != &#39;en&#39;, :] . original_text lang retweet_count original_author sentiment_class . id . 1.244590e+18 Happy mothersday to all those celebrating toda... | -0.0138325017 | en | 11 | 0 | . 1.244823e+18 Exactly what my late mum aka hype mama would d... | -0.9677309496 | en | 0 | 0 | . 1.246515e+18 It&#39;s the world&#39;s most difficult job No sick le... | -0.3876905537 | en | 1 | 0 | . 1.244226e+18 Happy Mother’s Day! To all the amazing Mums ou... | 0.5309553602 | en | 0 | 0 | . 1.244419e+18 Happy Mothers Day , Mummy! Nearly 90 and still... | -0.045423609 | en | 2 | 0 | . ... ... | ... | ... | ... | ... | . 1.246356e+18 Happy Mothers Day All My Nigerian Massive Fami... | 0.2117897904 | en | 0 | 0 | . 1.245821e+18 HAPPY MOTHERS DAY ! HAPPY MOTHERS DAY !!Now th... | -0.8739088126 | en | 0 | 0 | . 1.246719e+18 Isan Elba celebrates Happy Mothers’ Day with h... | 0.4945825935 | en | 0 | 0 | . 1.245565e+18 Still miss my mom she passed 18th of March 201... | 0.6927740873 | 0 | 0 | 0 | . 1.245871e+18 I’m so thankful for my 5, healthy happy joy br... | 0.2522315249 | en | 1 | 0 | . 241 rows × 5 columns . I think all the tweets are in English, you can see that the en value representing English is misplaced in other columns retweet_count and original_author. They are either filled with random float numbers or some other tweet text, maybe the data was not scraped properly? . prHowever from what I know, I think we can ignore the columns other than the original_text column which has the tweet text, which is the most important for analysis for sentiment of text. You can see that almost all of the texts have some link embedded to them, They are not likely to help in getting to know the sentiment of the tweeter. . The pattern here is that most of the links either are images which start with pic.twitter.* or links referring to other sites like http://www.instagram.*, We should be able to identify the pattern with a Regular expression. Let&#39;s try to test the assumption. . Cleaning links . import re sample_tweet = &quot;&quot;&quot; Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... https://www. facebook.com/36426361058377 0/posts/1130869587256498/ … #happymothersday2020 pic.twitter.com/ZCZOF1xb6K wo&quot;&quot;&quot; . print(re.sub(&#39;(https?: / /.*|pic.*)[ r n]*&#39;, &#39;&#39;, sample_tweet)) . Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... . This will remove most of the links, but it will also remove the text between the links like in the case above the #happymothersday2020 hashtag is removed. . Let&#39;s apply the regex to the texts . (train_data[&#39;original_text&#39;].replace({&#39;(https?: / /.*|pic.*)[ r n]*&#39;:&#39;&#39;}, regex=True).to_frame()).head() . original_text . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | . Visualize the words . Prepare the mask . # Download an image to use as mask for wordcloud import numpy as np from io import BytesIO from PIL import Image from PIL.ImageOps import invert img_file = requests.get(&#39;http://www.agirlandagluegun.com/wp-content/uploads/blogger/-ox_bazyTgmQ/TcNwpMfduLI/AAAAAAAAOX4/hcxXcz0A8-A/s1600/scan0001.jpg&#39;) img = BytesIO(img_file.content) img_mask = np.array(Image.open(img)) # Check if the image was downloaded properly img_mask.shape . (718, 1600, 3) . # To transform the mask image to display proper output # From https://www.datacamp.com/community/tutorials/wordcloud-python def transform_format(val): # Just trying to invert pixels if any(v == 255 for v in val): return 0 else: return 255 # Transform the mask into a new one that will work with the function: transformed_mask = np.ndarray((img_mask.shape[0], img_mask.shape[1]), np.int32) for i in range(len(img_mask)): transformed_mask[i] = list(map(transform_format, img_mask[i])) . from wordcloud import WordCloud, STOPWORDS import matplotlib.pyplot as plt wc = WordCloud(background_color=&quot;white&quot;, max_words=2000, mask=transformed_mask, stopwords=set(STOPWORDS), contour_width=1, contour_color=&#39;steelblue&#39;) # generate word cloud wc.generate(&#39; n&#39;.join(train_data[&#39;original_text&#39;].values.tolist())) . &lt;wordcloud.wordcloud.WordCloud at 0x7f0482536358&gt; . %matplotlib inline plt.figure(figsize=(18,18)) plt.axis(&quot;off&quot;) plt.imshow(wc) . &lt;matplotlib.image.AxesImage at 0x7f04825f85f8&gt; . Now we can try to model this text with any method that we would like! and that is coming up next. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "relUrl": "/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Building தமிழ் language model",
            "content": "Introduction . In this post, I will try to model தமிழ் (Tamil), I have already prepared the data for the language model in the kaggle notebook here, A language model will be useful for many tasks such as text classification, information retrieval etc. . What is a language model? . From what I know, Language model is a machine&#39;s way of understanding a language, technically it is defined as a probability distribution over a sequence of words[^1], by helping the machine to understand language, we can use them text-based classifiers, chatbots and for other NLP tasks in that language. . How do we train a language model? . I recall the times when you was going for school, I was given language lessons for English and தமிழ், The languages were different in grammar, dialects, sounds etc., after some lessons about the words and letters present in the language, both of them were taught to all in the same manner. . We would have lessons in textbooks, of which most of them are stories, biographies and history. Most of the exercises at the end of each lesson are . Fill in the blanks like The ____ rises in the east. or சூரியன் உதிக்கும் திசை _____ | Write short descriptive answers for question based on the lesson. | Maybe longer essays on general What-if scenarios from the lesson. | . We know that to answer them, it required a decent understanding of the language&#39;s grammar, which in turn, is taught to the children by making them read and write the questions and answers. . Now how can we teach a machine to understand and learn the language?, We had textbooks to read and learn about the language, So the machine also needs data, like our textbooks (but not necessarily the same ones with which we learn) to learn the language. . So how can we ensure that the machine is learning properly?, We test it with Fill in the blanks kind of exercises and let it guess the next possible word for the sequence of words we give it. It will not be easy for the machine but a little bit of learning is enough to use the model for other purposes. . We cannot directly give the raw text data to the model, We need to convert them to sequence of words to help it learn the flow of the words . Things we need for a language model . A decent amount of raw text data, more about that here . | A language tokenizer, more about that here and here . | . This notebook is executed on kaggle, so the paths mentioned here will be needed to change if you run in your own environment. Setup libraries and paths . Set seed for reproducibility . Load text data from csv . LANG_DATA = pd.read_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, index_col=[0]) LANG_DATA.head() . We have the url, article_id and title as additional information about the text, Let&#39;s check the average length of the article text. . Exploration . The total articles we have are 131162 . Remove empty articles from the dataframe . LANG_DATA.dropna(axis=&#39;rows&#39;, inplace=True) LANG_DATA.info() . Length of articles . The average length of each article is 1370 words . We had one empty article, I suppose. . Prepare Text data for Language model . processor = SPProcessor(lang=&#39;ta&#39;, sp_model=DATA_PATH/&#39;tamil_tok.model&#39;, sp_vocab=DATA_PATH/&#39;tamil_tok.vocab&#39;) . Set batch size . bs = 16 . data_lm = (TextList.from_df(LANG_DATA, path=DATA_PATH, cols=&#39;text&#39;, processor=processor) # Split out some data in a random fashion for testing .split_by_rand_pct(0.1) # This is where we convert the raw text data to # sequences of words .label_for_lm() # We want to do a language model so we label accordingly .databunch(bs=bs)) . # Check if data is loadable data_lm.sanity_check() . Let&#39;s save the language model data to skip the processing above next time. . data_lm.save(DATA_PATH/&#39;data_lm.pkl&#39;) . Let&#39;s have a look at the tokenized data from the sentencepiece tokenizer. . data_lm.show_batch() . bos means beginning of the sentence. | eos means end of the sentence. | xx maj used to indicate the next word begins with a capital in the original text. more about this can be found here and here | . Train the language model . Initialize model . # To use qrnn config = awd_lstm_lm_config.copy() config[&#39;qrnn&#39;] = True config[&#39;n_hid&#39;] = 1550 config[&#39;n_layers&#39;] = 4 # This is a classification metric, # determines how well can the model # narrow the choice of words from it&#39;s # vocabulary for the next prediction. perplexity = Perplexity() learn = language_model_learner(data_lm, arch=AWD_LSTM, config=config, pretrained=False, metrics=[accuracy, perplexity], ).to_fp16() # gradient clipping learn.clip = 0.1 learn.model_dir=DATA_PATH . Find proper learning rate . learn.lr_find() . learn.recorder.plot(suggestion=True) . Get suggested learning rate . min_grad_lr = learn.recorder.min_grad_lr min_grad_lr . Start training . Stage - 1 . learn.fit_one_cycle(10, min_grad_lr, # Momentums, just a try! div_factor=10.0, pct_start=0.8, moms=(0.75,0.65), callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;perplexity&#39;, name=&#39;best_st1&#39;), CSVLogger(learn, filename=DATA_PATH/&#39;history&#39;, append=True)]) . Save the intermediate results . learn.load(&#39;best_st1&#39;); learn.save(&#39;ta-wiki-stage1&#39;) learn.save_encoder(&#39;ta-wiki-enc-stage1&#39;) . You can chop and change the parameters, to get a better model, find the latest run of the notebook on kaggle, please upvote there if you liked this. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing out தமிழ் language tokenizer",
            "content": "You can find the notebook in which I built the tokenizers here . Introduction . This notebook is intended to experiment with different tokenizers built previously, with varying vocab_size values like 8000, 16000, 20000, 30000, So how do we exactly test a tokenizer?, this brings us to why do we need a tokenizer in the first place. . In English language we can easily find meaningful units of a sentence, by whitespaces. But it is not that easy in other languages, Like in தமிழ், Consider the following sentence, . இந்த தொழிற்சாலை பணப்பற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது., this loosely translates to something like The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. . We need to focus on the compound word பணப்பற்றாக்குறை which means lack of cash, that compound word is actually a combination of two words பணம் and பற்றாக்குறை representing Cash and Deficiency/lack of A tokenizer for தமிழ், should split the words into two as mentioned above. . Since Tokenization is the process of demarcating and possibly classifying sections of a string of input characters[1](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) or in simple words identifying linguistically meaningful units [2](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language) for further processing. We need it for a language model which essentially built to understand the language that it is built for. . Since we built the tokenizer in an unsupervised[3](https://github.com/google/sentencepiece#sentencepiece) way, There are no numerical ways of gauging the efficiency of the tokenization (AFAIK), so we are left with to try and tokenize some random sentences with the model, and check the tokenizer ourselves. . Sentences for testing tokenization . I have chosen some sentences at random, Check the comments above each for their translation in English. . sentences = [ # Sita is a mischievous girl &#39;சீதா ஒரு குறும்பு பெண்&#39;, # I remember my childhood &#39;எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது&#39;, # India has successfully tested the Agni-5 missile for the fourth time from Abdul Kalam Island (Wheeler Island) in Odisha. &#39;இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.&#39;, # The European Union&#39;s Galileo satellite system is in operation. It is believed to be the world&#39;s most accurate high-precision positioning system. &#39;ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.&#39;, # The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. &#39;இந்த தொழிற்சாலை பணபற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.&#39;, # Citizens, witnesses and warriors mourn the death of their king. It is up to the department to regret any loss. &#39;தம் மன்னன் இறந்ததற்கு குடிமக்களும் சான்றோரும் வீரர்களும் வருந்திப் பாடுவது கையறுநிலை என்னும் துறையாகும். எந்த இழப்பையும் எண்ணி வருந்துவது கையறுநிலைத் துறைக்குரியது.&#39;, # The Poems from Sangam Tamil Literature portrays the trading feats of early Tamilian,Tamilians traded across seas and other countries &#39;சங்கத்தமிழ்க் கவிதைகள் பழந்தமிழர்தம் வணிகச்சிறப்பைப் பறைசாற்றி நிற்கின்றன. தமிழர் கடல்கடந்து, அயல்நாடுகளிலும் வணிகம் செய்தனர் என்ற செய்திகளைச் சங்கப்பாடல்கள்வழி அறிகின்றோம்.&#39;, # Everyone stood up to call for a national flag at a school event. &#39;பள்ளி நிகழ்ச்சி ஒன்றில் தேசியக் கொடி ஏற்றுமாறு அழைக்க அவரும் எழுந்தார் அனைவரும் எழுந்து நின்றனர்&#39;, ] . Let&#39;s try to tokenize each one of them. . Begin Experimentation . Initial setup . import sentencepiece as spm from pathlib import Path from IPython.core.display import display, HTML from string import Template sp = spm.SentencePieceProcessor() TOK_PATH = &#39;/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer&#39; MODEL_PATHS = [p for p in Path(TOK_PATH).glob(&#39;*.model&#39;)] . Sentence Tokenization . I will try to comment on the tokenization with a limited knowledge of தமிழ் grammar, I will refer to the model by the vocab size they are built with (ie) 8000, 16000, 20000, 30000 . First sentence சீதா ஒரு குறும்பு பெண் . tokenize_and_display_results(sentences[0]) . tok_30000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_20000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_8000_size | ▁சீதா | ▁ஒரு | ▁குறு | ம்பு | ▁பெண் | . tok_16000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . குறும்பு is actually not a compound word, so I think apart from the model with 8000 size all other models got it right. . Next sentence எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது . tokenize_and_display_results(sentences[1]) . tok_30000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_20000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_8000_size | ▁என | க்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவ | ம் | ▁நினைவ | ிருக்கிறது | . tok_16000_size | ▁எனக்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . In this sentence I remember my childhood the model with 20000 and 30000 got the tokenization right. 16000 was just close, because the letter ப் does not have a meaning on it&#39;s own. . Next sentence இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது. . tokenize_and_display_results(sentences[2]) . tok_30000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசாவில | ுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_20000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_8000_size | ▁இந்தியா | ▁அக் | னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | வ | ிலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோ | தி | த்தது | . | . tok_16000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . This is interesting, We can probably say that 8000 vocab model is not doing so well. Looking at the others, both the 16000 vocab model and 20000 split ஒடிசாவிலுள்ள meaning to refer something in Odisha into ஒடிசா and விலுள்ள, I think the right split is ஒடிசாவில் and உள்ள, which the 30000 got right. . Next Sentence . ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது. . tokenize_and_display_results(sentences[3]) . tok_30000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_20000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_8000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லி | யோ | ▁செயற்கை | கோள | ் | ▁அமைப்பு | ▁செயல்பாட்ட | ுக்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லிய | மான | ▁செய் | மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_16000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . All of them may have got it wrong with Galileo which is split into கலி(Gali) and லியோ(leo), (Maybe I am wrong on this part) and in GPS இடஞ்சுட்டலாக may be called as இடம்சுட்டல்(can be loosely translated to Location Pointer) should have been split like இடம் and சுட்டல் and ஆக, Maybe I am expecting too much. . Conclusion . I will stop here for this blogpost, You can try the other sentences if you want by forking the notebook here . Please comment if you think something is wrong somewhere, share it if you have found this interesting. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Wiki data extraction",
            "content": "This post explains how I downloaded and extracted wiki dump archive using wikiextractor. . This code was used on a kaggle environment, which can be found here. You can fork and change as per your needs. . Setup . Import required libraries . # For JSON data extraction import json # For path manipulations from pathlib import Path # For preprocessing import string # For deleting files and folders import shutil # To clone necessary files import git # To download the dump import requests as req # To use wikiextractor import subprocess # To clean and process data import pandas as pd . Setup output paths . DATA_PATH = Path(&#39;/kaggle/working/&#39;) EXTRACTED_PATH = DATA_PATH/&#39;extracted&#39; EXTRACTED_PATH.mkdir() . Data Extraction . Request file from wikipedia. . You can use a different link here. . bzip_file = req.get(&#39;https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2&#39;) . Save request content to a file . with open(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;, &#39;wb&#39;) as f: f.write(bzip_file.content) . Clone wiki extractor from github . Thanks to attardi and GitPython . git.Git(str(DATA_PATH)).clone(&quot;https://github.com/attardi/wikiextractor.git&quot;) . Use wikiextractor to get data from the dump . This runs the wikiextractor cloned from github. . run_stat = subprocess.run( [&#39;python&#39;, # File to run str(DATA_PATH/&#39;wikiextractor/WikiExtractor.py&#39;), # Processing parameters to get as json &#39;-s&#39;, &#39;--json&#39;, # Directory to store Extracted text &#39;-o&#39;, str(DATA_PATH/&#39;extracted&#39;), # Archive file to extract from str(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;)] ) . Get list of files extracted from the extraction folder . files_extracted = [str(f) for f in EXTRACTED_PATH.rglob(&quot;*/*&quot;)] . Load json data from the files, since all files are stored as json we can load them like below, This gives us a list of dictionaries . lang_text = [json.loads(line) for _file in files_extracted for line in open(_file)] . or this . lang_text = [] for _file in files_extracted: with open(_file, &#39;r&#39;) as f: file_lines = f.readlines() for line in file_lines: lang_text.append(json.loads(line)) . Preprocessing . You can use any of the following, or skip the preprocessing altogether if you wish so. . Filter English words from text . Check each word after removing their punctuations, if it is an english word . filter_english = lambda text: &#39; &#39;.join([ word for word in text.split() if word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)).isalpha() is False ]) . or . def filter_english(text): words = [] # Spltting words for word in text.split(): # Replace symbols trans_table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) word = word.translate(trans_table) if not word.isalpha(): words.append(word) return &#39; &#39;.join(words) . Form dataframe and apply preprocessing . # Since we have a list of dictionaries. lang_df = pd.DataFrame(lang_text) lang_df[&#39;text&#39;] = lang_df[&#39;text&#39;].apply(filter_english) . Store the output in compressed format . lang_df.to_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, header=True) . The above saved file can be loaded with pd.read_csv. . You can find the full code for this in Github gist or with the output in kaggle. . Clean up the downloaded files, (if required) . shutil.rmtree(str(EXTRACTED_PATH)) shutil.rmtree(str(DATA_PATH/&#39;wikiextractor&#39;)) Path(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;).unlink() .",
            "url": "https://mani2106.github.io/Blog-Posts/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "relUrl": "/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Building தமிழ் language tokenizer",
            "content": "You can find the blog post regarding extraction here and kaggle notebook with output here . Import required libraries . from pathlib import Path import sentencepiece as spm import pandas as pd . Read data from csv . lang_data = pd.read_csv(&#39;../input/tamil-wiki-data-extraction/filtered_data.csv.tar.gz&#39;, index_col=[0]) lang_data.head() . id url title text . 0 48482 | https://ta.wikipedia.org/wiki?curid=48482 | தென் துருவம் | தென் துருவம் தென் முனை தென் துருவம் என்பது புவ... | . 1 48485 | https://ta.wikipedia.org/wiki?curid=48485 | ஆர்க்டிக் வட்டம் | ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து... | . 2 48486 | https://ta.wikipedia.org/wiki?curid=48486 | நாஞ்சில் நாடன் | நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்... | . 3 48492 | https://ta.wikipedia.org/wiki?curid=48492 | டிக்கோயா | டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின... | . 4 48493 | https://ta.wikipedia.org/wiki?curid=48493 | நள்ளிரவுச் சூரியன் | நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த... | . lang_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 133412 entries, 0 to 133411 Data columns (total 4 columns): id 133412 non-null int64 url 133412 non-null object title 133412 non-null object text 133412 non-null object dtypes: int64(1), object(3) memory usage: 5.1+ MB . Setup paths . # Initialize directories OUTPUT_DIR = Path(&#39;/kaggle/working&#39;) TEXTS_DIR = OUTPUT_DIR/&#39;texts&#39; TOK_DIR = OUTPUT_DIR/&#39;tokenizer&#39; # Create directories TOK_DIR.mkdir() TEXTS_DIR.mkdir() . Prepare texts . We can pass a list of files as a comma seperated string according to documentation, So we can store each article in a text file and pass the names in a comma seperated string. . # Save all article texts in seperate files for t in lang_data.itertuples(): file_name = Path(TEXTS_DIR/f&#39;text_{t.Index}.txt&#39;) file_name.touch() with file_name.open(&#39;w&#39;) as f: f.write(t.text) . # Check files in directory len([t for t in TEXTS_DIR.iterdir()]), lang_data.shape[0] . (133412, 133412) . All the files have been converted to texts . Train sentencepiece model . Let&#39;s make a comma seperated string of filenames . files = &#39;,&#39;.join([str(t) for t in TEXTS_DIR.iterdir()]) files[:100] . &#39;/kaggle/working/texts/text_40902.txt,/kaggle/working/texts/text_44212.txt,/kaggle/working/texts/text&#39; . We must find the right vocab_size for the tokenizer, that can be done only by testing the tokenizer after building onw . for v in 8000, 16000, 20000, 30000: api_str = f&quot;&quot;&quot;--input={files} --vocab_size={v} --model_type=unigram --character_coverage=0.9995 --model_prefix={str(TOK_DIR)}/tok_{v}_size --max_sentence_length=20000&quot;&quot;&quot; print(&quot;Training with vocab set as:&quot;, v) spm.SentencePieceTrainer.train(api_str) . Training with vocab set as: 8000 Training with vocab set as: 16000 Training with vocab set as: 20000 Training with vocab set as: 30000 . Cleanup . !rm -rf /kaggle/working/texts/ . Let&#39;s test the models in another notebook, you can find the outputs in this kaggle notebook .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Building a Pokemon Classifier",
            "content": ". In this notebook, I used the pokemon images dataset from here but unfortuantely it is not available now. . Get data from Kaggle . # using kaggle api !kaggle datasets download -d mrgravelord/complete-pokemon-image-dataset . Downloading complete-pokemon-image-dataset.zip to /content 90% 121M/134M [00:01&lt;00:00, 77.3MB/s] 100% 134M/134M [00:01&lt;00:00, 95.2MB/s] . !unzip /content/complete-pokemon-image-dataset.zip . !rm /content/complete-pokemon-image-dataset.zip . Load required libraries . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import ReduceLROnPlateauCallback, SaveModelCallback from fastai.callbacks import CSVLogger . Prepare Data for training . path = Path(&quot;.&quot;) . Form data bunch object from the folders. . data = ImageDataBunch.from_folder(path, train=&quot;.&quot;, ds_tfms=get_transforms(), size=128, bs=64, valid_pct=0.2).normalize(imagenet_stats) . Check the number of different pokemon images that we have. . len(data.classes) . 928 . Creating a CNN model from architecture of resnet18. I could use a bigger model but I would not be able to serve them from Google or OneDrive because of the size. . Error Rate is 1-accuracy. | Using mixup for better regularization. | Converting the operations to be performed in a lower precision, more | . learn = cnn_learner(data, models.resnet18, metrics=error_rate).mixup().to_fp16() . Adding callbacks to monitor the training process and . Reduce the learning_rate by using the ReduceLROnPlateauCallback. | Saving the model on every improvement in error_rate | Log the training stats in a csv file. | . callbacks_list = [ ReduceLROnPlateauCallback(learn=learn, monitor=&#39;error_rate&#39;, factor=1e-6, patience=5, min_delta=1e-5), SaveModelCallback(learn, mode=&quot;min&quot;, every=&#39;improvement&#39;, monitor=&#39;error_rate&#39;, name=&#39;best&#39;), CSVLogger(learn=learn, append=True) ] . Start Training . Now, All the setup has been made, Let&#39;s train the model with default parameters, for 15 epochs. . learn.fit_one_cycle(15, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 7.173859 | 6.631011 | 0.985798 | 01:09 | . 1 | 6.246106 | 5.397111 | 0.870562 | 01:08 | . 2 | 5.001963 | 3.665833 | 0.672144 | 01:07 | . 3 | 4.327330 | 2.772682 | 0.540881 | 01:06 | . 4 | 3.941842 | 2.320177 | 0.469669 | 01:06 | . 5 | 3.648211 | 2.069086 | 0.420978 | 01:06 | . 6 | 3.423512 | 1.901359 | 0.372895 | 01:06 | . 7 | 3.328791 | 1.758360 | 0.343883 | 01:06 | . 8 | 3.140401 | 1.657776 | 0.326841 | 01:06 | . 9 | 3.044241 | 1.591135 | 0.313857 | 01:07 | . 10 | 2.940413 | 1.538893 | 0.300670 | 01:06 | . 11 | 2.759924 | 1.502491 | 0.290931 | 01:07 | . 12 | 2.781063 | 1.474272 | 0.283628 | 01:06 | . 13 | 2.761597 | 1.457427 | 0.282816 | 01:06 | . 14 | 2.700450 | 1.459171 | 0.280179 | 01:07 | . Better model found at epoch 0 with error_rate value: 0.9857983589172363. Better model found at epoch 1 with error_rate value: 0.870561957359314. Better model found at epoch 2 with error_rate value: 0.6721444725990295. Better model found at epoch 3 with error_rate value: 0.5408805012702942. Better model found at epoch 4 with error_rate value: 0.46966931223869324. Better model found at epoch 5 with error_rate value: 0.4209778904914856. Epoch 6: reducing lr to 2.599579409433508e-09 Better model found at epoch 6 with error_rate value: 0.37289512157440186. Better model found at epoch 7 with error_rate value: 0.34388312697410583. Better model found at epoch 8 with error_rate value: 0.3268411457538605. Better model found at epoch 9 with error_rate value: 0.3138567805290222. Better model found at epoch 10 with error_rate value: 0.30066952109336853. Better model found at epoch 11 with error_rate value: 0.29093122482299805. Epoch 12: reducing lr to 2.606527959586539e-10 Better model found at epoch 12 with error_rate value: 0.2836275100708008. Better model found at epoch 13 with error_rate value: 0.28281599283218384. Better model found at epoch 14 with error_rate value: 0.2801785469055176. . Now that we have got some decent accuracy let us try to save the model and interpret from it. . In the following cell, I . Load the best weights saved by the callbacks during training. | Convert the model back to use 32 bit precision. | Export the model as a whole. | Export the weights alone. | . learn.load(&quot;best&quot;); learn.to_fp32() learn.export(&quot;pokemon_resnet18_st1.pkl&quot;) learn.save(&quot;pokemon_resnet18_st1_wgts&quot;) . Model Interpretation . It is very important that we get to know what the model has learnt from the training process. We can do that with the help of ClassificationInterpretation class from the fastai library. . # Create interpretation object interp = ClassificationInterpretation.from_learner(learn) # Get the instances where the model has made the most error (by loss value) in the validation set. losses,idxs = interp.top_losses() # Check whether the values are all of same length as the validation set len(data.valid_ds)==len(losses)==len(idxs) . True . Interpret the images where the model made errors during the validation. . The cell below shows . the image. | the model&#39;s prediction of that image. | the actual label of that image. | the loss and probability(the extent to which the model is sure about it&#39;s prediction). | . . You can notice that the image has some of it&#39;s regions blighted, as far I know these are the regions that the model looked at to make the prediction for the corresponding image. . interp.plot_top_losses(9, figsize=(15,11)) . Let us also see which pokemon have confused the model the most. . interp.most_confused(min_val=3) . [(&#39;Sharpedo(Mega)&#39;, &#39;Sharpedo&#39;, 7), (&#39;Moltres&#39;, &#39;Rapidash&#39;, 4), (&#39;Thundurus(Incarnate)&#39;, &#39;Thundurus(Therian)&#39;, 4), (&#39;Charizard(Mega Y)&#39;, &#39;Charizard&#39;, 3), (&#39;Greninja&#39;, &#39;Greninja(Ash)&#39;, 3), (&#39;Groudon(Primal)&#39;, &#39;Incineroar&#39;, 3), (&#39;Latias(Mega)&#39;, &#39;Latios(Mega)&#39;, 3), (&#39;Nidoran(Female)&#39;, &#39;Nidorina&#39;, 3)] . Apart from the 2nd one in this list, You can see why the model was confused generally, most of it&#39;s confusion stem from the evolved species of the same pokemon. . . Let&#39;s try to train the model a little bit differently this time. . learn.load(&#39;best&#39;); . Till now we have been training only the tail region of the model (i.e.) only the last two/ three layers of our model, so essentially this model is almost same as the model which was pretrained on 1000 categories of the ImageNet dataset with some minor tweaks for our problem here. We have some options to improve the model, which are . Train all the layers so that the model can adapt to the current classification problem. We do that by unfreeze(). | Train with a very low learning rate so that it does&#39;nt forget the learnings from the pretrained weights. | . . Let&#39;s see how well we can improve the model. . learn.to_fp16() learn.unfreeze() . Before we start training again, We need to figure out at what speed the neural network should learn, this is controlled by the learning rate parameter and finding a value for is crucial to the training process. . Luckily the fastai&#39;s lr_find method will help us do just the same. . learn.lr_find(start_lr=1e-20) # Plot the learning rates and the corresponding losses. learn.recorder.plot(suggestion=True) # Get the suggested learning rate min_grad_lr = learn.recorder.min_grad_lr . Min numerical gradient: 9.77E-17 Min loss divided by 10: 6.46E-09 . Use the same callbacks as before and train for 30 epochs. . learn.fit_one_cycle(30, min_grad_lr, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 2.648827 | 1.461440 | 0.280179 | 01:08 | . 1 | 2.687755 | 1.460599 | 0.282004 | 01:08 | . 2 | 2.646746 | 1.471151 | 0.281802 | 01:07 | . 3 | 2.647440 | 1.466154 | 0.284033 | 01:07 | . 4 | 2.687051 | 1.459437 | 0.280179 | 01:07 | . 5 | 2.656536 | 1.468453 | 0.284236 | 01:07 | . 6 | 2.646480 | 1.469294 | 0.280787 | 01:08 | . 7 | 2.707206 | 1.462577 | 0.281802 | 01:08 | . 8 | 2.650942 | 1.462410 | 0.283222 | 01:07 | . 9 | 2.657768 | 1.457848 | 0.279976 | 01:07 | . 10 | 2.689249 | 1.459695 | 0.281193 | 01:07 | . 11 | 2.656215 | 1.463556 | 0.282613 | 01:07 | . 12 | 2.715505 | 1.461581 | 0.282410 | 01:09 | . 13 | 2.689469 | 1.462295 | 0.282410 | 01:08 | . 14 | 2.685328 | 1.460551 | 0.283222 | 01:08 | . 15 | 2.624705 | 1.458205 | 0.283222 | 01:10 | . 16 | 2.675736 | 1.468264 | 0.283628 | 01:11 | . 17 | 2.641450 | 1.461090 | 0.281193 | 01:10 | . 18 | 2.662758 | 1.455160 | 0.283425 | 01:12 | . 19 | 2.662972 | 1.459052 | 0.283019 | 01:13 | . 20 | 2.711507 | 1.464223 | 0.282207 | 01:13 | . 21 | 2.697404 | 1.463553 | 0.283425 | 01:13 | . 22 | 2.643310 | 1.462558 | 0.280584 | 01:12 | . 23 | 2.657411 | 1.463225 | 0.285048 | 01:12 | . 24 | 2.679297 | 1.467203 | 0.283425 | 01:13 | . 25 | 2.654091 | 1.464559 | 0.281599 | 01:12 | . 26 | 2.619208 | 1.465727 | 0.283222 | 01:12 | . 27 | 2.622938 | 1.466129 | 0.280990 | 01:12 | . 28 | 2.646025 | 1.465645 | 0.284236 | 01:13 | . 29 | 2.679323 | 1.458704 | 0.284033 | 01:13 | . Better model found at epoch 0 with error_rate value: 0.2801785469055176. Better model found at epoch 9 with error_rate value: 0.27997565269470215. Epoch 11: reducing lr to 9.288489603500534e-23 Epoch 17: reducing lr to 5.97347999592849e-23 Epoch 29: reducing lr to 3.9089488838232423e-28 . We can see that the model has improved slightly but not much, other ways that we can try are . Try using a different architecture rather than resnet18. | Add more Image augmentation methods (even though fastai has some reasonable defaults). | . Persist the environment so that we would be able to deploy the model without any problems . !pip freeze &gt; resnet18.txt . You can skip the following section, where I just save the model to my drive. . Save model to Google Drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . Try the model . Curious to try out the model, I have built a small Flask web app which is hosted here. You can find the code for the same in my github repo. . . The website may take some time to load since it was hosted on a free tier heroku site. . That&#39;s it for this post, Please share it if you have found it useful. Don&#39;t hesitate to leave a comment if you find that any of my explanation needs some clarification. .",
            "url": "https://mani2106.github.io/Blog-Posts/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "relUrl": "/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "date": " • Jun 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A passionate Software Engineer and Data Science Practitioner, interested in Machine Learning, Deep Learning and Data Science. I always like to do things hands on. . More of my work can be found here. . You can also contact me via email @ manimaran_p@outlook.com .",
          "url": "https://mani2106.github.io/Blog-Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mani2106.github.io/Blog-Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}