{
  
    
        "post0": {
            "title": "Building a Sticky Table of Contents: From Spec to Implementation",
            "content": "Transforming Blog Navigation: The Sticky TOC Journey . Reading long-form technical content can be challenging without proper navigation aids. When I noticed the empty right sidebar space on my Jekyll blog posts, I saw an opportunity to dramatically improve the reading experience by implementing a sticky table of contents (TOC) component. This post documents the complete journey from initial concept to production implementation using spec-driven development methodology. . The Problem: Lost in Long Content . My blog posts often contain detailed technical discussions with multiple sections and subsections. Readers would frequently lose their place or struggle to navigate to specific sections without scrolling back to find headers. The existing TOC implementation was basic and not optimally positioned, failing to provide the navigation assistance readers needed. . User Experience Pain Points . No persistent navigation: Readers had to scroll to find section headers | Wasted screen real estate: Empty right sidebar on desktop screens | Poor mobile experience: No responsive behavior for smaller screens | Accessibility gaps: Limited screen reader support and keyboard navigation | Inconsistent styling: TOC didnâ€™t match the blogâ€™s design system | . The Spec-Driven Approach . Rather than diving straight into code, I used Kiroâ€™s spec-driven development methodology to systematically transform this idea into a comprehensive feature. This approach proved invaluable for managing complexity and ensuring nothing was overlooked. . Why Spec-Driven Development? . The systematic approach provided several key benefits: . Clear requirements: Defined exactly what needed to be built and why | Comprehensive design: Identified technical challenges and solutions upfront | Actionable tasks: Broke down complex implementation into manageable steps | Quality assurance: Built-in validation and testing strategies | Future maintenance: Documented decisions and procedures for ongoing updates | . Phase 1: Requirements Gathering . The first phase involved creating detailed requirements using the EARS format (Easy Approach to Requirements Syntax). This systematic approach helped identify all the necessary functionality and edge cases. . Core User Stories . Navigation Enhancement . As a blog reader, I want to see a sticky table of contents on the right side of blog posts, so that I can quickly navigate to different sections without scrolling back to the top. . Automatic Generation . As a blog reader, I want the table of contents to automatically generate from the post headers, so that I can see the hierarchical structure of the content. . Interactive Navigation . As a blog reader, I want to click on TOC entries to jump to specific sections, so that I can quickly navigate to content that interests me. . Acceptance Criteria Highlights . The requirements phase established critical acceptance criteria: . Responsive Behavior: TOC displays on desktop (&gt;1024px) but hides on mobile/tablet | Content Threshold: TOC only appears when posts have 2+ headers | Sticky Positioning: TOC remains visible during scroll with proper offset | Active Highlighting: Current section highlighted based on scroll position | Accessibility: Full keyboard navigation and screen reader support | Performance: Smooth scrolling and 60fps scroll performance | Phase 2: Technical Design . The design phase involved researching the existing Jekyll setup and creating a comprehensive technical architecture that would integrate seamlessly with the current blog infrastructure. . Architecture Overview . The solution consists of three main components working together: . â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Post Header â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”‚ â”‚ Sticky TOC â”‚ Main Content â”‚ â”‚ (Left Side) â”‚ (Blog Post) â”‚ â”‚ â”‚ â”‚ â”‚ - Auto-gen â”‚ - Headers (H1-H6) â”‚ â”‚ - Clickable â”‚ - Paragraphs â”‚ â”‚ - Highlighted â”‚ - Images, etc. â”‚ â”‚ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ Comments Section â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ . Key Design Decisions . CSS Grid Layout: Chose CSS Grid for the main layout with graceful degradation to flexbox for older browsers. . Intersection Observer API: Selected for active section highlighting due to superior performance compared to scroll event handlers. . Progressive Enhancement: Built with fallbacks at every level - from jQuery TOC plugin to vanilla JavaScript, from CSS Grid to float-based layout. . Responsive Strategy: Hide TOC completely on screens &lt;1024px rather than trying to fit it in limited space. . Phase 3: Implementation Planning . The design was broken down into 8 major implementation phases with 15 specific coding tasks. Each task included clear acceptance criteria and requirement references. . Task Breakdown Strategy . Foundation First: CSS layout and responsive behavior | Core Functionality: TOC generation and basic navigation | Enhanced Features: Active highlighting and smooth scrolling | Polish &amp; Performance: Styling, optimization, and cross-browser testing | Documentation: Comprehensive testing and user documentation | Implementation Deep Dive . CSS Grid Layout Foundation . The layout system uses CSS Grid with comprehensive fallbacks: . .post-container { display: grid; grid-template-columns: 280px 1fr; // TOC width + Main content grid-gap: 2rem; max-width: 1200px; margin: 0 auto; // Cross-browser grid support -ms-grid-columns: 280px 2rem 1fr; -ms-grid-rows: auto; // Graceful degradation for browsers without CSS Grid &amp;.no-css-grid { display: block; .toc-sidebar { float: left; width: 280px; margin-right: 2rem; } .post-main { margin-left: 312px; // 280px + 2rem gap } } // Responsive behavior - hide TOC on smaller screens @media (max-width: 1024px) { grid-template-columns: 1fr; // Single column layout } } . Sticky Positioning with Fallbacks . The sticky positioning includes comprehensive browser support: . .toc-sidebar { position: -webkit-sticky; // Safari support position: sticky; top: 2rem; height: fit-content; max-height: calc(100vh - 4rem); overflow-y: auto; // Performance optimization: Hardware acceleration transform: translateZ(0); backface-visibility: hidden; // Graceful degradation for browsers without sticky positioning &amp;.no-sticky-position { position: static; max-height: none; } } . JavaScript Enhancement Architecture . The JavaScript implementation prioritizes performance and accessibility: . // Feature detection for progressive enhancement var features = { jquery: typeof jQuery !== &#39;undefined&#39;, cssGrid: CSS.supports(&#39;display&#39;, &#39;grid&#39;), stickyPosition: CSS.supports(&#39;position&#39;, &#39;sticky&#39;), intersectionObserver: &#39;IntersectionObserver&#39; in window, smoothScroll: &#39;scrollBehavior&#39; in document.documentElement.style }; // Intersection Observer for active section highlighting var observerOptions = { rootMargin: &#39;-20% 0px -35% 0px&#39;, // Trigger when section is in middle of viewport threshold: 0 }; window.tocObserver = new IntersectionObserver(function(entries) { // Update active section highlighting based on visible headers }, observerOptions); . Fallback TOC Generation . When the jQuery TOC plugin isnâ€™t available, the system falls back to vanilla JavaScript: . function generateFallbackTOC() { var headers = document.querySelectorAll(&#39;h2, h3, h4, h5, h6&#39;); // Hide TOC if fewer than 2 headers if (headers.length &lt; 2) { hideTOCContainer(); return false; } // Generate hierarchical TOC structure var tocList = document.createElement(&#39;ul&#39;); var currentLevel = 2; var levelStack = [tocList]; headers.forEach(function(header, index) { // Create TOC entry with proper nesting var level = parseInt(header.tagName.charAt(1)); var listItem = document.createElement(&#39;li&#39;); var link = document.createElement(&#39;a&#39;); // Handle nested levels and indentation // ... implementation details }); } . Accessibility Implementation . Accessibility was a core requirement throughout the implementation: . Semantic HTML Structure . &lt;div class=&quot;toc-container&quot; role=&quot;complementary&quot; aria-labelledby=&quot;toc-heading&quot;&gt; &lt;div class=&quot;toc-header&quot;&gt; &lt;h4 id=&quot;toc-heading&quot; class=&quot;toc-title&quot;&gt;Table of Contents&lt;/h4&gt; &lt;/div&gt; &lt;nav class=&quot;toc-nav&quot; aria-label=&quot;Table of Contents&quot;&gt; &lt;div id=&quot;toc&quot; role=&quot;navigation&quot;&gt;&lt;/div&gt; &lt;/nav&gt; &lt;/div&gt; . Keyboard Navigation Support . .toc-nav #toc ul li a { // Focus indicators for accessibility &amp;:focus { outline: 2px solid #0366d6; outline-offset: 2px; background-color: #f1f8ff; color: #0366d6; } } . Screen Reader Compatibility . Proper ARIA labels and landmarks | Semantic navigation structure | Clear focus indicators | Descriptive link text | . Performance Optimizations . Performance was critical for maintaining a smooth reading experience: . Debounced Scroll Handlers . // Performance optimization: Throttled scroll handler for 60fps var optimizedScrollHandler = throttle(function() { // Handle scroll events efficiently }, 16); // ~60fps . Hardware Acceleration . .toc-container .toc-nav #toc ul li a { // Enable hardware acceleration for smooth transitions transform: translateZ(0); backface-visibility: hidden; will-change: background-color, color, border-left-color; } . Intersection Observer Benefits . Using Intersection Observer instead of scroll events provided: . Better Performance: No need to calculate element positions on every scroll | Battery Efficiency: Reduced CPU usage on mobile devices | Smoother Animation: Native browser optimization for intersection detection | . Responsive Design Strategy . The responsive approach prioritizes usability across all device sizes: . Desktop Experience (&gt;1024px) . Full TOC visible in left sidebar | Sticky positioning active | Active section highlighting | Smooth scrolling navigation | . Tablet Experience (768px-1024px) . TOC hidden to preserve reading space | Single-column layout | Full content width utilization | . Mobile Experience (&lt;768px) . TOC completely hidden | Optimized typography and spacing | Touch-friendly navigation | . Cross-Browser Compatibility . The implementation includes comprehensive browser support: . Modern Browsers (Chrome 60+, Firefox 60+, Safari 12+) . Full functionality with all enhancements | CSS Grid layout | Intersection Observer API | Sticky positioning | . Older Browsers (IE 11, older Safari) . Graceful degradation with float-based layout | Fallback scroll-based highlighting | Static positioning with visual indicators | . Feature Detection and Polyfills . // Cross-browser compatibility: Array.from polyfill for IE11 if (!Array.from) { Array.from = function(arrayLike) { var result = []; for (var i = 0; i &lt; arrayLike.length; i++) { result.push(arrayLike[i]); } return result; }; } . Testing and Validation . Comprehensive testing ensured the feature met all requirements: . Automated Validation . The implementation includes built-in requirement validation: . function validateTOCImplementation() { var validationResults = { requirement1_1: false, // TOC displays in right sidebar requirement1_2: false, // TOC remains sticky during scroll requirement1_3: false, // TOC hidden on mobile/tablet requirement1_4: false, // TOC hidden when fewer than 2 headers // ... additional requirements }; // Validate each requirement programmatically // Log results for debugging and monitoring } . Manual Testing Scenarios . Content Variations: Tested with posts having different header structures | Device Testing: Verified responsive behavior across multiple screen sizes | Accessibility Testing: Validated keyboard navigation and screen reader compatibility | Performance Testing: Monitored scroll performance and memory usage | . Cross-Browser Testing . Tested across: . Chrome (latest and previous versions) | Firefox (latest and ESR) | Safari (macOS and iOS) | Edge (Chromium-based) | Internet Explorer 11 (graceful degradation) | . Results and Impact . The implementation successfully transformed the blog reading experience: . User Experience Improvements . Navigation Efficiency . Readers can now jump to any section instantly | Clear visual hierarchy shows content structure | Active highlighting shows current reading position | . Responsive Design . Optimal experience across all device sizes | No horizontal scrolling or layout issues | Appropriate content density for each screen size | . Accessibility Enhancement . Full keyboard navigation support | Screen reader compatibility with proper ARIA labels | High contrast mode support | Reduced motion support for accessibility preferences | . Technical Achievements . Performance Metrics . TOC generation: &lt;50ms for typical blog posts | Scroll performance: Maintains 60fps during navigation | Memory usage: &lt;1MB additional overhead | First paint: No impact on initial page load | . Browser Support . 100% functionality in modern browsers | Graceful degradation in older browsers | Zero JavaScript errors across all tested browsers | . Code Quality . Comprehensive error handling and fallbacks | Extensive documentation and comments | Modular, maintainable code structure | . Lessons Learned . Spec-Driven Development Benefits . Comprehensive Planning: Requirements phase identified edge cases that would have been missed in ad-hoc development | Risk Mitigation: Design phase revealed potential browser compatibility issues before implementation | Quality Assurance: Built-in validation ensured all requirements were met | Maintainable Code: Systematic approach resulted in well-documented, modular implementation | Technical Insights . Progressive Enhancement Works: Building with fallbacks at every level ensures broad compatibility | Performance Matters: Intersection Observer API provides significantly better performance than scroll events | Accessibility First: Considering accessibility from the start is easier than retrofitting | Responsive Strategy: Sometimes hiding features on mobile is better than trying to fit everything | Implementation Challenges . Cross-Browser Compatibility: CSS Grid and sticky positioning required extensive fallbacks | Performance Optimization: Balancing smooth animations with 60fps scroll performance | Accessibility Requirements: Ensuring full keyboard navigation while maintaining visual design | Content Variability: Handling posts with different header structures and edge cases | Future Enhancements . The spec-driven approach established a foundation for future improvements: . Planned Features . Collapsible Sections: Allow readers to collapse/expand TOC sections | Reading Progress: Visual indicator showing reading progress through the post | Bookmark Integration: Save reading position and favorite sections | Print Optimization: Enhanced print styles for TOC inclusion | . Technical Improvements . Service Worker Caching: Cache TOC generation for faster subsequent loads | WebP Image Support: Optimize any TOC-related images for better performance | Advanced Analytics: Track TOC usage patterns to optimize placement and features | . Implementation Guide . For developers wanting to implement similar functionality: . Key Dependencies . CSS Grid: For modern layout with float fallbacks | Intersection Observer API: For performance-optimized active highlighting | jQuery (optional): Can use existing TOC plugins or vanilla JavaScript fallback | . Critical Considerations . Content Threshold: Only show TOC when thereâ€™s enough content to justify it | Responsive Strategy: Consider hiding TOC on smaller screens rather than cramming it in | Performance: Use Intersection Observer instead of scroll events for better performance | Accessibility: Include proper ARIA labels and keyboard navigation from the start | Fallbacks: Plan for graceful degradation in older browsers | Code Structure . â”œâ”€â”€ _includes/toc.html # TOC HTML template and JavaScript â”œâ”€â”€ _sass/minima/custom-styles.scss # TOC styling and responsive behavior â””â”€â”€ _layouts/post.html # Post layout integration . Conclusion . The sticky TOC implementation demonstrates the power of spec-driven development for creating comprehensive, accessible, and performant web features. By taking time to properly define requirements, create a detailed design, and plan implementation tasks, the project delivered a feature that enhances the reading experience while maintaining broad browser compatibility and accessibility standards. . The systematic approach prevented common pitfalls like: . Missing edge cases (posts with few headers) | Performance issues (scroll event handlers) | Accessibility gaps (missing ARIA labels) | Browser compatibility problems (lack of fallbacks) | . Most importantly, the spec-driven methodology created comprehensive documentation that will make future maintenance and enhancements straightforward. The requirements, design decisions, and implementation details are all documented, providing a clear roadmap for ongoing development. . This project reinforces why Iâ€™m excited about spec-driven development: it transforms complex features from overwhelming challenges into manageable, systematic projects that deliver high-quality results. . The TOC feature is now live on all blog posts, providing readers with the navigation assistance they need to efficiently consume long-form technical content. The next phase will focus on gathering user feedback and implementing the planned enhancements to further improve the reading experience. . . The complete specification documentation for this TOC implementation, including requirements, design documents, task breakdowns, and validation procedures, is available in the blog repository under .kiro/specs/sticky-toc-component/. .",
            "url": "https://mani2106.github.io/Blog-Posts/web-development/jekyll/ux/accessibility/spec-driven-development/css/javascript/2025/10/04/sticky-toc-component-implementation.html",
            "relUrl": "/web-development/jekyll/ux/accessibility/spec-driven-development/css/javascript/2025/10/04/sticky-toc-component-implementation.html",
            "date": " â€¢ Oct 4, 2025"
        }
        
    
  
    
        ,"post1": {
            "title": "Back from Hiatus: Jekyll Blog Security Hardening with Spec-Driven Development",
            "content": "The Comeback: From Hiatus to Security Hardening . After a long hiatus from blogging, I decided it was time to dust off my Jekyll blog and get back to writing. But when I opened my repository, I was greeted by a concerning sight: 12 GitHub Dependabot security alerts ranging from critical to low severity. These werenâ€™t just minor updatesâ€”some were critical vulnerabilities with CVSS scores of 9.1 that could lead to memory corruption and use-after-free attacks. . This seemed like the perfect opportunity to not only get my blog back online securely, but also to try out a new approach Iâ€™d been exploring: spec-driven development using Kiro. Rather than diving straight into code changes, I decided to treat this security hardening as a proper software project with requirements, design, and implementation planning. . Why Spec-Driven Development? . Coming back to a project after months away, I realized I needed more than just quick fixes. I needed: . Clear documentation of what needed to be done and why | Systematic approach to prevent breaking changes | Maintainable procedures for future updates | Comprehensive testing strategy to ensure nothing broke | . This is where Kiroâ€™s spec-driven approach became invaluable. . The Security Landscape . The vulnerabilities spanned across multiple critical components: . Nokogiri (XML/HTML parser): 5 alerts including critical memory corruption issues | REXML (XML parser): 6 alerts mostly related to Denial of Service attacks | ActiveSupport (Rails component): 1 alert for potential file disclosure | . Hereâ€™s what I was facing: . Critical Priority ğŸš¨ . CVE-2025-49794, CVE-2025-49796: Use-after-free and memory corruption in Nokogiri (CVSS 9.1) | . High Priority âš ï¸ . CVE-2025-24855, CVE-2024-55549: Use-after-free in libxslt | CVE-2024-43398: REXML DoS vulnerability with deep XML elements | . Medium Priority ğŸ“‹ . Multiple REXML DoS vulnerabilities | ActiveSupport file disclosure vulnerability | . The Spec-Driven Approach with Kiro . Instead of randomly updating gems, I used Kiro to develop a structured methodology through proper specification: . 1. Requirements Gathering . Using Kiro, I started by creating a comprehensive requirements document that captured: . User stories for each type of security concern | Acceptance criteria in EARS format (Easy Approach to Requirements Syntax) | Priority matrix based on vulnerability severity | Success metrics for the security hardening | . **User Story:** As a blog maintainer, I want to analyze the current GitHub security alerts for my gem dependencies, so that I can prioritize which vulnerabilities need immediate attention. #### Acceptance Criteria 1. WHEN analyzing GitHub alerts THEN the system SHALL categorize vulnerabilities by severity (critical, high, medium, low) 2. WHEN vulnerabilities are categorized THEN the system SHALL identify which gems require updates . 2. Design Document . Kiro helped me create a detailed design that outlined: . Vulnerability analysis framework with three-phase approach | Gem upgrade mapping with version constraints | Dependency resolution strategy to prevent conflicts | Error handling scenarios and rollback procedures | . 3. Implementation Planning . The spec process generated a detailed task breakdown: . 8 major phases with 15 sub-tasks | Each task linked to specific requirements | Clear validation steps for each phase | Rollback procedures documented upfront | . 4. Baseline Documentation . Through the spec process, I systematically documented the current state: . Ruby 2.7.1 (approaching EOL) | Jekyll 4.1.1 (outdated) | Bundler 2.1.4 (incompatible with modern Ruby) | Multiple outdated gems with restrictive version constraints | . 5. Priority-Based Upgrade Strategy . The design phase helped me categorize updates by security impact and dependency relationships: . # Phase 1: Ruby runtime upgrade Ruby 2.7.1 â†’ Ruby 3.3.9 # Phase 2: Critical security gems nokogiri: &quot;&lt; 1.18.9&quot; â†’ &quot;~&gt; 1.18.0&quot; rexml: &quot;&lt; 3.3.9&quot; â†’ &quot;~&gt; 3.4.0&quot; # Phase 3: Major framework updates activesupport: &quot;&lt; 6.1.7.5&quot; â†’ &quot;~&gt; 7.2.0&quot; # Phase 4: Supporting infrastructure faraday: &quot;&lt; 1.0&quot; â†’ &quot;~&gt; 2.12.0&quot; . 3. Constraint Optimization . One major issue was overly restrictive version constraints that prevented security updates: . # Before: Blocking security updates gem &quot;faraday&quot;, &quot;&lt; 1.0&quot; # Stuck on 0.17.5 from 2019! # After: Security-friendly constraints gem &quot;faraday&quot;, &quot;~&gt; 2.12&quot; # Allows patch updates . The Implementation Journey: Spec to Code . With Kiroâ€™s comprehensive spec in hand, the implementation became straightforward. Each task was clearly defined with acceptance criteria, making it easy to know exactly what needed to be done and how to validate success. . Task-by-Task Execution . The beauty of the spec-driven approach was that I could work through each task systematically: . - [x] 1. Establish baseline and prepare for upgrades - [x] 2. Upgrade Ruby version to latest stable - [x] 3. Update critical security gems (Nokogiri and REXML) - [x] 4. Update ActiveSupport to latest Rails 7.x series - [x] 5. Update supporting gems to latest versions - [x] 6. Optimize Gemfile version constraints for security - [x] 7. Comprehensive testing and validation - [x] 8. Document changes and create maintenance procedures . Ruby Version Upgrade . Following the specâ€™s Phase 1, the foundation needed to be solid. Ruby 2.7.1 was approaching end-of-life and causing bundler compatibility issues: . # Before FROM jekyll/jekyll:4.1.0 # Ruby 2.7.1 # After FROM ruby:3.3-alpine # Ruby 3.3.9 . This single change resolved multiple compatibility issues and enabled modern gem versions. . Critical Security Updates . Nokogiri: The Big One Nokogiri had the most severe vulnerabilities, including critical memory corruption issues: . # Updated from 1.16.3 to 1.18.10 gem &quot;nokogiri&quot;, &quot;~&gt; 1.18&quot; . This update resolved 5 security alerts, including the critical CVE-2025-49794 and CVE-2025-49796. . REXML: DoS Protection REXML had 6 different Denial of Service vulnerabilities: . # Updated from 3.2.5 to 3.4.4 gem &quot;rexml&quot;, &quot;~&gt; 3.4&quot; . ActiveSupport: Major Version Jump The most challenging update was ActiveSupport, requiring a major version upgrade: . # 6.0.6.1 â†’ 7.2.2.2 (major version jump) gem &quot;activesupport&quot;, &quot;~&gt; 7.2&quot; . Surprisingly, this major version upgrade had zero breaking changes for Jekyll usageâ€”a testament to Railsâ€™ commitment to backward compatibility. . Testing and Validation . Each phase included comprehensive testing: . Automated Checks . # Build verification bundle install jekyll build # Functionality testing docker-compose up -d curl -I http://localhost:4000 . Manual Verification . âœ… All blog posts render correctly | âœ… Math equations display properly (KaTeX) | âœ… Code syntax highlighting works | âœ… RSS feed generates correctly | âœ… SEO tags are present | âœ… Site navigation functions | . The Results . Security Impact . 12 vulnerabilities resolved: From critical to low severity | Zero known vulnerabilities: Clean security audit | Future-proof constraints: Automatic security patches enabled | . Performance Improvements . Ruby 3.3.x: Significant performance gains over 2.7.x | Modern HTTP/2: Faraday 2.x includes modern features | Updated plugins: Performance optimizations included | . Version Summary . | Component | Before | After | Security Impact | |â€”â€”â€”â€“|â€”â€”â€“|â€”â€”-|â€”â€”â€”â€”â€”â€“| | Ruby | 2.7.1 | 3.3.9 | Latest security patches | | Nokogiri | 1.16.3 | 1.18.10 | 5 CVEs resolved | | REXML | 3.2.5 | 3.4.4 | 6 DoS vulnerabilities fixed | | ActiveSupport | 6.0.6.1 | 7.2.2.2 | File disclosure vulnerability fixed | | Faraday | 0.17.5 | 2.12.3 | 5+ years of security updates | . The Power of Spec-Driven Development . This project reinforced why Iâ€™m excited about spec-driven development with Kiro: . Benefits I Experienced . Clear roadmap: Never wondered â€œwhat should I do next?â€ | Risk mitigation: Potential issues identified upfront in design phase | Comprehensive testing: Validation criteria defined before implementation | Documentation by design: Requirements and procedures created as part of the process | Maintainable outcomes: Future changes have a clear framework to follow | The Kiro Advantage . Iterative refinement: Requirements â†’ Design â†’ Tasks â†’ Implementation | Built-in validation: Each phase reviewed before proceeding | Comprehensive coverage: Nothing falls through the cracks | Future-ready: Maintenance procedures established from day one | . Lessons Learned . 1. Spec-Driven Beats Ad-Hoc . Taking time to create proper requirements and design prevented dependency conflicts and reduced risk significantly. . 2. Version Constraints Matter . Overly restrictive constraints (gem &quot;name&quot;, &quot;&lt; 1.0&quot;) can block critical security updates for years. . 3. Major Version Upgrades Arenâ€™t Always Breaking . ActiveSupport 6.x â†’ 7.x had zero breaking changes for our Jekyll usage. . 4. Documentation Is Critical . The spec process automatically generated detailed records of changes, rationale, and rollback procedures. . 5. Kiro Makes Complex Projects Manageable . What could have been an overwhelming security crisis became a systematic, well-documented project. . Whatâ€™s Next for This Blog . Now that the security foundation is solid, Iâ€™m excited about the future improvements planned: . Upcoming UX Enhancements . GitHub Discussions integration: Moving away from Utterances to GitHubâ€™s native discussion feature for better community engagement and moderation | Redesigned reading experience: Complete UX overhaul focusing on readability, accessibility, and modern design principles | Enhanced layout architecture: Left sidebar: Table of Contents for easy navigation within posts | Right sidebar: Comments section for contextual discussions | Responsive design: Seamless experience across desktop, tablet, and mobile | Progressive enhancement: Modern features that degrade gracefully | . | . Technical Roadmap . Performance optimization: Leveraging Ruby 3.3.x performance gains and modern Jekyll features | Enhanced SEO: Building on Jekyll-seo-tag 2.8.0 improvements with structured data | Better accessibility: WCAG 2.1 compliance and screen reader optimization | Modern web standards: HTTP/3, WebP images, and progressive loading | . Content Strategy Evolution . More technical deep-dives: Like this security hardening journey | Spec-driven development series: Sharing the Kiro methodology and real-world applications | Open source contributions: Documenting contributions and community learnings | Interactive tutorials: Hands-on guides with embedded examples | . Ongoing Maintenance Strategy . The spec process established comprehensive ongoing procedures: . Monthly Security Reviews . Monitor GitHub Dependabot alerts | Apply critical security patches following established procedures | Update security documentation | . Quarterly Dependency Updates . Update all gems to latest stable versions | Execute comprehensive testing protocols | Review and optimize version constraints | . Annual Major Updates . Evaluate Ruby version upgrades | Consider Jekyll major version updates | Comprehensive security audit and spec review | . The Maintenance Procedures . Kiro helped create comprehensive maintenance procedures covering: . Emergency Response: Critical vulnerability response within 24 hours | Regular Updates: Monthly and quarterly update schedules | Testing Protocols: Comprehensive validation checklists | Rollback Procedures: Quick recovery from failed updates | Documentation Standards: Keeping procedures current | . Key Takeaways . For Jekyll Users . Donâ€™t ignore Dependabot alerts: They represent real security risks | Use semantic versioning wisely: ~&gt; x.y allows security patches | Keep Ruby current: EOL versions create cascading problems | Test systematically: Automated + manual validation prevents issues | Document everything: Future you will thank present you | For Developers . Spec-driven development works: Taking time to plan saves time in execution | Requirements matter: Clear acceptance criteria prevent scope creep and missed edge cases | Design upfront: Identifying issues before coding saves debugging time | Maintenance is part of the spec: Donâ€™t just build it, plan to maintain it | Tools like Kiro are game-changers: Structured approaches scale better than ad-hoc methods | The Bottom Line . What started as a comeback blog post became a demonstration of modern development practices. The combination of returning from hiatus with 12 security alerts created the perfect opportunity to try spec-driven development with Kiro. . The systematic approach took more time upfront but resulted in: . Zero security vulnerabilities | Improved performance and modern dependencies | Comprehensive documentation and procedures | Clear roadmap for future improvements | Confidence in ongoing maintenance | . The blog is now running on Ruby 3.3.9 with the latest secure versions of all dependencies. More importantly, I have a proven methodology for future updatesâ€”both security and feature enhancements. . This is just the beginning of my return to regular blogging. With a solid, secure foundation and a clear development methodology, Iâ€™m excited to continue improving this blog and sharing more technical insights. . The next posts will dive deeper into spec-driven development, the upcoming UX redesign process, and how GitHub Discussions will transform the commenting experience. . I will try to do regular blogging after a multi-year dormancy! ğŸš€ . .",
            "url": "https://mani2106.github.io/Blog-Posts/security/jekyll/ruby/devops/maintenance/kiro/spec-driven-development/2025/09/30/jekyll-blog-security-hardening-journey.html",
            "relUrl": "/security/jekyll/ruby/devops/maintenance/kiro/spec-driven-development/2025/09/30/jekyll-blog-security-hardening-journey.html",
            "date": " â€¢ Sep 30, 2025"
        }
        
    
  
    
        ,"post2": {
            "title": "Explaining prediction models and individual predictions",
            "content": "The following script tries to reproduce one algorithm from the paper titled Explaining prediction models and individual predictions . You can have a look at the resultant json here . The json would have record level information of each feature and its impact on the prediction. . import os import pandas as pd from sklearn.pipeline import Pipeline import numpy as np from typing import List import json, joblib # The feature importances for each data instance # are calculated with four random samples drawn from the data # I also tried to use numba to compile to machine code but could # not achieve it file_path = &#39;.&#39; # LOAD your model model = joblib.load(os.path.join(file_path, &#39;model_pipeline.joblib&#39;)) # Bring in the data training and testing data X_train = pd.read_csv(os.path.join(file_path, &#39;train.csv&#39;), index_col=0) X_test = pd.read_csv(os.path.join(file_path, &#39;test.csv&#39;), index_col=0) # TODO try to complete numba usage # @numba.jit(nopython=True) def shuffle_feature_records( full_data: np.array, feature_data: np.array, feature_index: int, m: int) -&gt; List[np.array]: &quot;&quot;&quot; Randomly selects `m` no of records to compare and calculate feature importances &quot;&quot;&quot; feat_len = len(feature_data) feature_indices = np.arange(0, feat_len, dtype=np.uint8) # take random indices rand_indices = np.random.choice(full_data.shape[0], m, replace=False) # take random samples w = full_data[rand_indices] b1_vec: np.array = np.empty((m, feat_len), dtype=np.object) b2_vec: np.array = np.empty((m, feat_len), dtype=np.object) for i in np.arange(0, m): del_index = np.argwhere(feature_indices==feature_index) indices = np.delete(feature_indices, del_index) # remove current feature index to add later np.random.shuffle(indices) split_pt = indices.shape[0]//2 # split indices for 2 different vectors ind_x, ind_w = indices[:split_pt], indices[split_pt:] # form the indices for b1 and b2 vectors ind_x_1 = np.append(ind_x, feature_index) ind_w_1 = np.append(ind_w, feature_index) # to ensure the features are assigned in the order of training data b1_sort_indices = np.argsort(np.append(ind_x, ind_w_1)) b2_sort_indices = np.argsort(np.append(ind_x_1, ind_w)) b1_vec[i] = np.concatenate((feature_data[ind_x], w[i, ind_w_1]), axis=0)[b1_sort_indices] b2_vec[i] = np.concatenate((feature_data[ind_x_1], w[i, ind_w]), axis=0)[b2_sort_indices] arrs = [b1_vec, b2_vec] return arrs def get_feat_imp_data( model: Pipeline, X_train: pd.DataFrame, X_test: pd.DataFrame) -&gt; dict: &quot;&quot;&quot; Loops over the available data and sends them for calculation &quot;&quot;&quot; # Get features for 100 records tr_data = X_train[X_train[&#39;id&#39;].isin(range(0, 101))].copy() te_data = X_test[X_test[&#39;id&#39;].isin(range(0, 101))].copy() # join all the data data = pd.concat([tr_data, te_data], axis=&#39;rows&#39;) cols = data.columns.tolist() feat_imp = {} # send them one by one to calculate for d in data.values: col_imp = {} for feat_index in range(data.shape[1]): b1, b2 = shuffle_feature_records(X_train.values, d, feature_index=feat_index, m=4) b1 = pd.DataFrame(b1, columns=cols) b2 = pd.DataFrame(b2, columns=cols) b1_pred = model.predict(b1) b2_pred = model.predict(b2) col_imp[cols[feat_index]] = np.sum(b1_pred - b2_pred) / 4 feat_imp[d[0]] = col_imp return feat_imp feats = get_feat_imp_data(model, X_train, X_test) with open(&#39;feat_imps_1.json&#39;, &#39;w&#39;) as f: json.dump(feats, f) .",
            "url": "https://mani2106.github.io/Blog-Posts/take-home/implementation/2021/05/29/model-agnostic-featimp.html",
            "relUrl": "/take-home/implementation/2021/05/29/model-agnostic-featimp.html",
            "date": " â€¢ May 29, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Probability",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/05/23/probability.html",
            "relUrl": "/2021/05/23/probability.html",
            "date": " â€¢ May 23, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Autodiff",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/05/16/autodiff.html",
            "relUrl": "/2021/05/16/autodiff.html",
            "date": " â€¢ May 16, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Chemdner",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/04/26/chemdNER.html",
            "relUrl": "/2021/04/26/chemdNER.html",
            "date": " â€¢ Apr 26, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Calculus_nb",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/04/25/calculus_nb.html",
            "relUrl": "/2021/04/25/calculus_nb.html",
            "date": " â€¢ Apr 25, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Price_pred_notebook",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/04/10/price_pred_notebook.html",
            "relUrl": "/2021/04/10/price_pred_notebook.html",
            "date": " â€¢ Apr 10, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "D2lai Exercises Pt3",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/02/09/d2lai-exercises-pt3.html",
            "relUrl": "/2021/02/09/d2lai-exercises-pt3.html",
            "date": " â€¢ Feb 9, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "D2lai_exercises_pt2",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/02/07/d2lai_exercises_pt2.html",
            "relUrl": "/2021/02/07/d2lai_exercises_pt2.html",
            "date": " â€¢ Feb 7, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "D2lai Exercises Pt1",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2021/02/04/d2lai-exercises-pt1.html",
            "relUrl": "/2021/02/04/d2lai-exercises-pt1.html",
            "date": " â€¢ Feb 4, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Gradient Descent for Linear Regression",
            "content": "Implementing Linear Regression with Gradient Descent . This post is my understanding of Linear Regression, Please feel free to comment and point out any errors if seen. . The Cost Function . The cost function is used to measure the correctness of the current solution(hypothesis), the function can be mean of squares/square roots of the errors. It is represented as the following . J($ theta_0$, $ theta_1$,..$ theta_n$) . where $ theta_0$ is a constant, $ theta_1$â€¦$ theta_n$ are the parameters of the equation we are trying to solve . In simple ML terms(I think) Each one of the parameter represent the weight of each feature in the dataset, that we are using to build the model . This is the formula for the cost function with mean of square differences.$^0$ . 12mâˆ‘i=1m(hÎ¸(x)âˆ’y)2 frac{1}{2m} sum_{i=1}^{m} (h_ theta(x) - y)^22m1â€‹i=1âˆ‘mâ€‹(hÎ¸â€‹(x)âˆ’y)2 . where $h_0(x)$ is the hypothesis or the predicted value for $y$ and $m$ is the number of training examples . A sample pseudocode for the mean of squares of errors would be . Calculate the sum of square differences / errors between each value $(X* theta)$ vector and y vector For each training example Multiply the feature values $X_1$, $X_2$,..$X_n$ with itâ€™s corresponding weights $ theta_1$, $ theta_2 cdots theta_n$, and add the constant, $ theta_0$. . | Subtract the above value from the $y$ target value of that example and square the difference. . | . | Sum all the differences / errors | . | Take the mean of the differences by the dividing with the number of training examples. | . It can be represented like $h_0(x) = theta_0+ theta_1X_1+ theta_2X_2+ cdots+ theta_nX_n$ where $n$ is the number of features we are using for the problem. . Gradient Descent . We need to update the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ so that the cost function . J(Î¸0,Î¸1,â‹¯Î¸n)=12mâˆ‘i=1m(hÎ¸(xi)âˆ’yi)2J( theta_0, theta_1, cdots theta_n) = frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2J(Î¸0â€‹,Î¸1â€‹,â‹¯Î¸nâ€‹)=2m1â€‹âˆ‘i=1mâ€‹(hÎ¸â€‹(xi)âˆ’yi)2 can be minimized. . So to find the minimum for the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ the update is performed like below . Î¸j:=Î¸jâˆ—Î±âˆ‚âˆ‚Î¸jâˆ—J(Î¸0,Î¸1,â‹¯Î¸j) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)Î¸jâ€‹:=Î¸jâ€‹âˆ—Î±âˆ‚Î¸jâ€‹âˆ‚â€‹âˆ—J(Î¸0â€‹,Î¸1â€‹,â‹¯Î¸jâ€‹) . Where, . The := is assignment operator | The $ theta_j$ is the parameter to update, j is the feature index number | The $ alpha$ is the learning rate | The $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ is the derivative term of the cost function, it is like slope$^2$ of the line tangent$^1$ to the curve touching on where the $ theta$ is present in that curve. | . For each feature in the dataset the update has to be done simultaneously for each parameter $ theta$, until the convergence / error given by cost function at its minimum. . Deriving the derivative term $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ . $^3$To simplify the problem I am considering that we have 2 parameters $ theta_0$ and $ theta_1$, our hypothesis $h_0(x)$ function becomes $ theta_0+ theta_1X$ . Now let $g( theta_0, theta_1)$ be our derivative term . g(Î¸0,Î¸1)=J(Î¸0,Î¸1)g( theta_0, theta_1)=J( theta_0, theta_1)g(Î¸0â€‹,Î¸1â€‹)=J(Î¸0â€‹,Î¸1â€‹) . =(12mâˆ‘i=1m(hÎ¸(xi)âˆ’yi)2)= ( frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2)=(2m1â€‹i=1âˆ‘mâ€‹(hÎ¸â€‹(xi)âˆ’yi)2) . =(12mâˆ‘i=1m(Î¸0+Î¸1Xiâˆ’yi)2)=( frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2)=(2m1â€‹i=1âˆ‘mâ€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi)2) . consider f(Î¸0,Î¸1)=hÎ¸(xi)âˆ’yif( theta_0, theta_1) = h_ theta(x^i) - y^if(Î¸0â€‹,Î¸1â€‹)=hÎ¸â€‹(xi)âˆ’yi . now our equation becomes . g(Î¸0,Î¸1)=12mâˆ‘i=1m(f(Î¸0,Î¸1)i)2g( theta_0, theta_1)= frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2g(Î¸0â€‹,Î¸1â€‹)=2m1â€‹i=1âˆ‘mâ€‹(f(Î¸0â€‹,Î¸1â€‹)i)2 . subsutituting the value of $f( theta_0, theta_1)$ the equation becomes . g(f(Î¸0,Î¸1)i)=12mâˆ‘i=1m(Î¸0+Î¸1Xiâˆ’yi)2(1)g(f( theta_0, theta_1)^i)= frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2 tag{1}g(f(Î¸0â€‹,Î¸1â€‹)i)=2m1â€‹i=1âˆ‘mâ€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi)2(1) . Now let us derive the partial derivative for $(1)$ . âˆ‚âˆ‚Î¸jg(f(Î¸0,Î¸1)i)=âˆ‚âˆ‚Î¸j12mâˆ‘i=1m(f(Î¸0,Î¸1)i)2 frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_j} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2âˆ‚Î¸jâ€‹âˆ‚â€‹g(f(Î¸0â€‹,Î¸1â€‹)i)=âˆ‚Î¸jâ€‹âˆ‚â€‹2m1â€‹i=1âˆ‘mâ€‹(f(Î¸0â€‹,Î¸1â€‹)i)2 . Let j be 0 . âˆ‚âˆ‚Î¸0g(f(Î¸0,Î¸1)i)=âˆ‚âˆ‚Î¸012mâˆ‘i=1m(f(Î¸0,Î¸1)i)2 frac{ partial}{ partial theta_0}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_0} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2âˆ‚Î¸0â€‹âˆ‚â€‹g(f(Î¸0â€‹,Î¸1â€‹)i)=âˆ‚Î¸0â€‹âˆ‚â€‹2m1â€‹i=1âˆ‘mâ€‹(f(Î¸0â€‹,Î¸1â€‹)i)2 . since we are performing the partial derivative with respect to $ theta_0$ other variables are considered constant, the following is similar to $ frac{ partial}{ partial x}$ of $(x^2+y)$ which is $2x$ . =1mâˆ‘i=1mf(Î¸0,Î¸1)i= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i=m1â€‹i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)i . =1mâˆ‘i=1m(hÎ¸(x)âˆ’y)= frac{1}{m} sum_{i=1}^{m} (h_ theta(x) - y)=m1â€‹i=1âˆ‘mâ€‹(hÎ¸â€‹(x)âˆ’y) . This is because of the chain rule, when we take derivative of a function like $(1)$, we need to use this formula below . âˆ‚âˆ‚Î¸jg(f(Î¸0,Î¸1))=âˆ‚âˆ‚Î¸jg(Î¸0,Î¸1)âˆ—âˆ‚âˆ‚Î¸jf(Î¸0,Î¸1)(2) frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1) tag{2}âˆ‚Î¸jâ€‹âˆ‚â€‹g(f(Î¸0â€‹,Î¸1â€‹))=âˆ‚Î¸jâ€‹âˆ‚â€‹g(Î¸0â€‹,Î¸1â€‹)âˆ—âˆ‚Î¸jâ€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹)(2) . In case when j = 0 the partial derivative of $g$ becomes . âˆ‚âˆ‚Î¸0g(Î¸0,Î¸1)=12mâˆ—2(âˆ‘i=1mf(Î¸0,Î¸1)i)2âˆ’1=1mâˆ‘i=1mf(Î¸0,Î¸1)i frac{ partial}{ partial theta_0}g( theta_0, theta_1) = frac{1}{ cancel2m}* cancel2( sum_{i=1}^{m} f( theta_0, theta_1)^i)^{2-1}= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^iâˆ‚Î¸0â€‹âˆ‚â€‹g(Î¸0â€‹,Î¸1â€‹)=2 | â€‹m1â€‹âˆ—2 | â€‹(i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)i)2âˆ’1=m1â€‹i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)i . and the partial derivative of $f$ becomes . âˆ‚âˆ‚Î¸0f(Î¸0,Î¸1)=âˆ‚âˆ‚Î¸0(hÎ¸(xi)âˆ’yi)(3) frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}(h_ theta(x^i) - y^i) tag{3}âˆ‚Î¸0â€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹)=âˆ‚Î¸0â€‹âˆ‚â€‹(hÎ¸â€‹(xi)âˆ’yi)(3) . âˆ‚âˆ‚Î¸0f(Î¸0,Î¸1)=âˆ‚âˆ‚Î¸0(Î¸0+Î¸1Xiâˆ’yi)=1 frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}( theta_0+ theta_1X^i - y^i) = 1âˆ‚Î¸0â€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹)=âˆ‚Î¸0â€‹âˆ‚â€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi)=1 . since other variables are considered constants, that gives us . âˆ‚âˆ‚Î¸jg(Î¸0,Î¸1)âˆ—âˆ‚âˆ‚Î¸jf(Î¸0,Î¸1)=1mâˆ‘i=1mf(Î¸0,Î¸1)iâˆ—1=1mâˆ‘i=1mf(Î¸0,Î¸1)i frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1)= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * 1 = frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^iâˆ‚Î¸jâ€‹âˆ‚â€‹g(Î¸0â€‹,Î¸1â€‹)âˆ—âˆ‚Î¸jâ€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹)=m1â€‹i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)iâˆ—1=m1â€‹i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)i . Letâ€™s start from Equation $(1)$ to perform the partial derivative when j = 1 . The partial derivative of $g( theta_0, theta_1)$ with respect to $ theta_1$ is same from the last derivation but the partial derivative of $f( theta_0, theta_1)$ becomes . Consider $(3)$ when j = 1 . âˆ‚âˆ‚Î¸1f(Î¸1,Î¸1)=âˆ‚âˆ‚Î¸1(hÎ¸(xi)âˆ’yi)=âˆ‚âˆ‚Î¸1(Î¸0+Î¸1Xiâˆ’yi) frac{ partial}{ partial theta_1}f( theta_1, theta_1) = frac{ partial}{ partial theta_1}(h_ theta(x^i) - y^i) = frac{ partial}{ partial theta_1}( theta_0+ theta_1X^i - y^i)âˆ‚Î¸1â€‹âˆ‚â€‹f(Î¸1â€‹,Î¸1â€‹)=âˆ‚Î¸1â€‹âˆ‚â€‹(hÎ¸â€‹(xi)âˆ’yi)=âˆ‚Î¸1â€‹âˆ‚â€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi) . variables other than $ theta_1$ are considered constants, so they become 0 and $ frac{ partial}{ partial theta_1} theta_1$ = 1, so our equation becomes . âˆ‚âˆ‚Î¸1f(Î¸0,Î¸1)=0+1âˆ—Xiâˆ’0=Xi frac{ partial}{ partial theta_1}f( theta_0, theta_1)= 0+1* X^i-0 =X^iâˆ‚Î¸1â€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹)=0+1âˆ—Xiâˆ’0=Xi . and according to the chain rule $(2)$ and replacing the partials . âˆ‚âˆ‚Î¸1g(f(Î¸0,Î¸1))=âˆ‚âˆ‚Î¸1g(Î¸0,Î¸1)âˆ—âˆ‚âˆ‚Î¸1f(Î¸0,Î¸1) frac{ partial}{ partial theta_1}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_1}g( theta_0, theta_1) * frac{ partial}{ partial theta_1} f( theta_0, theta_1)âˆ‚Î¸1â€‹âˆ‚â€‹g(f(Î¸0â€‹,Î¸1â€‹))=âˆ‚Î¸1â€‹âˆ‚â€‹g(Î¸0â€‹,Î¸1â€‹)âˆ—âˆ‚Î¸1â€‹âˆ‚â€‹f(Î¸0â€‹,Î¸1â€‹) . =1mâˆ‘i=1mf(Î¸0,Î¸1)iâˆ—Xi= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * X^i=m1â€‹i=1âˆ‘mâ€‹f(Î¸0â€‹,Î¸1â€‹)iâˆ—Xi . =1mâˆ‘i=1m(Î¸0+Î¸1Xiâˆ’yi)âˆ—Xi= frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i) * X^i=m1â€‹i=1âˆ‘mâ€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi)âˆ—Xi . Now we can use the derivatives in the Gradient Descent algorithm . Î¸j:=Î¸jâˆ—Î±âˆ‚âˆ‚Î¸jâˆ—J(Î¸0,Î¸1,â‹¯Î¸j) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)Î¸jâ€‹:=Î¸jâ€‹âˆ—Î±âˆ‚Î¸jâ€‹âˆ‚â€‹âˆ—J(Î¸0â€‹,Î¸1â€‹,â‹¯Î¸jâ€‹) . repeat until convergence { Î¸0:=Î¸0âˆ—Î±1mâˆ‘i=1m(Î¸0+Î¸1Xiâˆ’yi) theta_0 := theta_0 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)Î¸0â€‹:=Î¸0â€‹âˆ—Î±m1â€‹âˆ‘i=1mâ€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi) . Î¸1:=Î¸1âˆ—Î±1mâˆ‘i=1m(Î¸0+Î¸1Xiâˆ’yi)âˆ—Xi theta_1 := theta_1 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)* X^iÎ¸1â€‹:=Î¸1â€‹âˆ—Î±m1â€‹âˆ‘i=1mâ€‹(Î¸0â€‹+Î¸1â€‹Xiâˆ’yi)âˆ—Xi } . One disadvantage in Gradient Descent is that depending on the position it is initialized at the start, but in linear regression the cost function (mean of sqaured errors) is a convex function (ie) it is in shape of a bowl when plotted on the graph. . I tried to implement this in python which can be found here . Resources and references . How to implement a machine learning algorithm | Understanding math in Machine learning . | $^0$ Most of the content and explanation is from Courseraâ€™s - Machine Learning class . | $^1$ Tangent is a line which touches exactly at one point of a curve. . | $^2$ Slope of a line given any two points on the line is the ratio number of points we need to rise/descend and move away/towards the origin to the meet the other point. | . . Image from wikihow . | $^3$ Derivation referred from here . | .",
            "url": "https://mani2106.github.io/Blog-Posts/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "relUrl": "/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "date": " â€¢ Aug 31, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Tabular_data_modeling_with_tensorflow",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "relUrl": "/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "date": " â€¢ Jun 24, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Deep_learning_approach_for_text_classification_with_spacy",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "relUrl": "/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "date": " â€¢ May 30, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Hackerearth_mothers_day_sentiment",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "relUrl": "/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "date": " â€¢ May 24, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Tamil Language Model",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/04/20/tamil-language-model.html",
            "relUrl": "/2020/04/20/tamil-language-model.html",
            "date": " â€¢ Apr 20, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Testing Sentencepiece Tokenizer For Tamil",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "relUrl": "/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "date": " â€¢ Apr 19, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Wiki data extraction",
            "content": "This post explains how I downloaded and extracted wiki dump archive using wikiextractor. . This code was used on a kaggle environment, which can be found here. You can fork and change as per your needs. . Setup . Import required libraries . # For JSON data extraction import json # For path manipulations from pathlib import Path # For preprocessing import string # For deleting files and folders import shutil # To clone necessary files import git # To download the dump import requests as req # To use wikiextractor import subprocess # To clean and process data import pandas as pd . Setup output paths . DATA_PATH = Path(&#39;/kaggle/working/&#39;) EXTRACTED_PATH = DATA_PATH/&#39;extracted&#39; EXTRACTED_PATH.mkdir() . Data Extraction . Request file from wikipedia. . You can use a different link here. . bzip_file = req.get(&#39;https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2&#39;) . Save request content to a file . with open(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;, &#39;wb&#39;) as f: f.write(bzip_file.content) . Clone wiki extractor from github . Thanks to attardi and GitPython . git.Git(str(DATA_PATH)).clone(&quot;https://github.com/attardi/wikiextractor.git&quot;) . Use wikiextractor to get data from the dump . This runs the wikiextractor cloned from github. . run_stat = subprocess.run( [&#39;python&#39;, # File to run str(DATA_PATH/&#39;wikiextractor/WikiExtractor.py&#39;), # Processing parameters to get as json &#39;-s&#39;, &#39;--json&#39;, # Directory to store Extracted text &#39;-o&#39;, str(DATA_PATH/&#39;extracted&#39;), # Archive file to extract from str(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;)] ) . Get list of files extracted from the extraction folder . files_extracted = [str(f) for f in EXTRACTED_PATH.rglob(&quot;*/*&quot;)] . Load json data from the files, since all files are stored as json we can load them like below, This gives us a list of dictionaries . lang_text = [json.loads(line) for _file in files_extracted for line in open(_file)] . or this . lang_text = [] for _file in files_extracted: with open(_file, &#39;r&#39;) as f: file_lines = f.readlines() for line in file_lines: lang_text.append(json.loads(line)) . Preprocessing . You can use any of the following, or skip the preprocessing altogether if you wish so. . Filter English words from text . Check each word after removing their punctuations, if it is an english word . filter_english = lambda text: &#39; &#39;.join([ word for word in text.split() if word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)).isalpha() is False ]) . or . def filter_english(text): words = [] # Spltting words for word in text.split(): # Replace symbols trans_table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) word = word.translate(trans_table) if not word.isalpha(): words.append(word) return &#39; &#39;.join(words) . Form dataframe and apply preprocessing . # Since we have a list of dictionaries. lang_df = pd.DataFrame(lang_text) lang_df[&#39;text&#39;] = lang_df[&#39;text&#39;].apply(filter_english) . Store the output in compressed format . lang_df.to_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, header=True) . The above saved file can be loaded with pd.read_csv. . You can find the full code for this in Github gist or with the output in kaggle. . Clean up the downloaded files, (if required) . shutil.rmtree(str(EXTRACTED_PATH)) shutil.rmtree(str(DATA_PATH/&#39;wikiextractor&#39;)) Path(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;).unlink() .",
            "url": "https://mani2106.github.io/Blog-Posts/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "relUrl": "/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "date": " â€¢ Apr 14, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Building A Tokenizer For Tamil With Sentencepiece",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "relUrl": "/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "date": " â€¢ Apr 14, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Fast_ai_lesson_2_pokemon_classifier",
            "content": "",
            "url": "https://mani2106.github.io/Blog-Posts/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "relUrl": "/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "date": " â€¢ Jun 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A passionate Data Scientist working on unstructured data to deliver business value, focussing on NLP and Computer vision. I always like to do things hands on. . Old pieces of my work can be found here. . You can also contact me via email @ manimaran_p@outlook.com .",
          "url": "https://mani2106.github.io/Blog-Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://mani2106.github.io/Blog-Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}