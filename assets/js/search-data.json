{
  
    
        "post0": {
            "title": "Testing out தமிழ் language tokenizer",
            "content": "You can find the which I built the tokenizers here . Introduction . This notebook is intended to experiment with different tokenizers built previously, with varying vocab_size values like 8000, 16000, 20000, 30000, So how do we exactly test a tokenizer?, this brings us to why do we need a tokenizer in the first place. . In English language we can easily find meaningful units of a sentence, by whitespaces. But it is not that easy in other languages, Like in தமிழ், Consider the following sentence, . இந்த தொழிற்சாலை பணப்பற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது., this loosely translates to something like The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. . We need to focus on the compound word பணப்பற்றாக்குறை which means lack of cash, that compound word is actually a combination of two words பணம் and பற்றாக்குறை representing Cash and Deficiency/lack of A tokenizer for தமிழ், should split the words into two as mentioned above. . Since Tokenization is the process of demarcating and possibly classifying sections of a string of input characters[1](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) or in simple words identifying linguistically meaningful units [2](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language) for further processing. We need it for a language model which essentially built to understand the language that it is built for. . Since we built the tokenizer in an unsupervised[3](https://github.com/google/sentencepiece#sentencepiece) way, There are no numerical ways of gauging the efficiency of the tokenization (AFAIK), so we are left with to try and tokenize some random sentences with the model, and check the tokenizer ourselves. . Sentences for testing tokenization . I have chosen some sentences at random, Check the comments above each for their translation in English. . sentences = [ # Sita is a mischievous girl &#39;சீதா ஒரு குறும்பு பெண்&#39;, # I remember my childhood &#39;எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது&#39;, # India has successfully tested the Agni-5 missile for the fourth time from Abdul Kalam Island (Wheeler Island) in Odisha. &#39;இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.&#39;, # The European Union&#39;s Galileo satellite system is in operation. It is believed to be the world&#39;s most accurate high-precision positioning system. &#39;ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.&#39;, # The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. &#39;இந்த தொழிற்சாலை பணபற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.&#39;, # Citizens, witnesses and warriors mourn the death of their king. It is up to the department to regret any loss. &#39;தம் மன்னன் இறந்ததற்கு குடிமக்களும் சான்றோரும் வீரர்களும் வருந்திப் பாடுவது கையறுநிலை என்னும் துறையாகும். எந்த இழப்பையும் எண்ணி வருந்துவது கையறுநிலைத் துறைக்குரியது.&#39;, # The Poems from Sangam Tamil Literature portrays the trading feats of early Tamilian,Tamilians traded across seas and other countries &#39;சங்கத்தமிழ்க் கவிதைகள் பழந்தமிழர்தம் வணிகச்சிறப்பைப் பறைசாற்றி நிற்கின்றன. தமிழர் கடல்கடந்து, அயல்நாடுகளிலும் வணிகம் செய்தனர் என்ற செய்திகளைச் சங்கப்பாடல்கள்வழி அறிகின்றோம்.&#39;, # Everyone stood up to call for a national flag at a school event. &#39;பள்ளி நிகழ்ச்சி ஒன்றில் தேசியக் கொடி ஏற்றுமாறு அழைக்க அவரும் எழுந்தார் அனைவரும் எழுந்து நின்றனர்&#39;, ] . Let&#39;s try to tokenize each one of them. . Begin Experimentation . Initial setup . import sentencepiece as spm from pathlib import Path from IPython.core.display import display, HTML from string import Template sp = spm.SentencePieceProcessor() TOK_PATH = &#39;/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer&#39; MODEL_PATHS = [p for p in Path(TOK_PATH).glob(&#39;*.model&#39;)] . Sentence Tokenization . I will try to comment on the tokenization with a limited knowledge of தமிழ் grammar, I will refer to the model by the vocab size they are built with (ie) 8000, 16000, 20000, 30000 . First sentence சீதா ஒரு குறும்பு பெண் . tokenize_and_display_results(sentences[0]) . tok_30000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_20000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_8000_size | ▁சீதா | ▁ஒரு | ▁குறு | ம்பு | ▁பெண் | . tok_16000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . குறும்பு is actually not a compound word, so I think apart from the model with 8000 size all other models got it right. . Next sentence எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது . tokenize_and_display_results(sentences[1]) . tok_30000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_20000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_8000_size | ▁என | க்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவ | ம் | ▁நினைவ | ிருக்கிறது | . tok_16000_size | ▁எனக்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . In this sentence I remember my childhood the model with 20000 and 30000 got the tokenization right. 16000 was just close, because the letter ப் does not have a meaning on it&#39;s own. . Next sentence இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது. . tokenize_and_display_results(sentences[2]) . tok_30000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசாவில | ுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_20000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_8000_size | ▁இந்தியா | ▁அக் | னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | வ | ிலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோ | தி | த்தது | . | . tok_16000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . This is interesting, We can probably say that 8000 vocab model is not doing so well. Looking at the others, both the 16000 vocab model and 20000 split ஒடிசாவிலுள்ள meaning to refer something in Odisha into ஒடிசா and விலுள்ள, I think the right split is ஒடிசாவில் and உள்ள, which the 30000 got right. . Next Sentence . ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது. . tokenize_and_display_results(sentences[3]) . tok_30000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_20000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_8000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லி | யோ | ▁செயற்கை | கோள | ் | ▁அமைப்பு | ▁செயல்பாட்ட | ுக்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லிய | மான | ▁செய் | மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_16000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . All of them may have got it wrong with Galileo which is split into கலி(Gali) and லியோ(leo), (Maybe I am wrong on this part) and in GPS இடஞ்சுட்டலாக may be called as இடம்சுட்டல்(can be loosely translated to Location Pointer) should have been split like இடம் and சுட்டல் and ஆக, Maybe I am expecting too much. . Conclusion . I will stop here for this blogpost, You can try the other sentences if you want by forking the notebook here . Please comment if you think something is wrong somewhere, share it if you have found this interesting. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Wiki data extraction",
            "content": "This post explains how I downloaded and extracted wiki dump archive using wikiextractor. . This code was used on a kaggle environment, which can be found here. You can fork and change as per your needs. . Setup . Import required libraries . # For JSON data extraction import json # For path manipulations from pathlib import Path # For preprocessing import string # For deleting files and folders import shutil # To clone necessary files import git # To download the dump import requests as req # To use wikiextractor import subprocess # To clean and process data import pandas as pd . Setup output paths . DATA_PATH = Path(&#39;/kaggle/working/&#39;) EXTRACTED_PATH = DATA_PATH/&#39;extracted&#39; EXTRACTED_PATH.mkdir() . Data Extraction . Request file from wikipedia. . You can use a different link here. . bzip_file = req.get(&#39;https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2&#39;) . Save request content to a file . with open(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;, &#39;wb&#39;) as f: f.write(bzip_file.content) . Clone wiki extractor from github . Thanks to attardi and GitPython . git.Git(str(DATA_PATH)).clone(&quot;https://github.com/attardi/wikiextractor.git&quot;) . Use wikiextractor to get data from the dump . This runs the wikiextractor cloned from github. . run_stat = subprocess.run( [&#39;python&#39;, # File to run str(DATA_PATH/&#39;wikiextractor/WikiExtractor.py&#39;), # Processing parameters get as json &#39;-s&#39;, &#39;--json&#39;, # Directory to store Extracted text &#39;-o&#39;, str(DATA_PATH/&#39;extracted&#39;), # Archive file to extract from str(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;)] ) . Get list of files extracted from the extraction folder . files_extracted = [str(f) for f in EXTRACTED_PATH.rglob(&quot;*/*&quot;)] . Load json data from the files, since all files are stored as json we can load them like below, This gives us a list of dictionaries . lang_text = [json.loads(line) for _file in files_extracted for line in open(_file)] . or this . lang_text = [] for _file in files_extracted: with open(_file, &#39;r&#39;) as f: file_lines = f.readlines() for line in file_lines: lang_text.append(json.loads(line)) . Preprocessing . You can use any of the following, or skip the preprocessing altogether if you wish so. . Filter English words from text . Check each word after removing their punctuations, if it is an english word . filter_english = lambda text: &#39; &#39;.join([ word for word in text.split() if word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)).isalpha() is False ]) . or . def filter_english(text): words = [] # Spltting words for word in text.split(): # Replace symbols trans_table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) word = word.translate(trans_table) if not word.isalpha(): words.append(word) return &#39; &#39;.join(words) . Form dataframe and apply preprocessing . # Since we have a list of dictionaries. lang_df = pd.DataFrame(lang_text) lang_df[&#39;text&#39;] = lang_df[&#39;text&#39;].apply(filter_english) . Store the output in compressed format . lang_df.to_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, header=True) . The above saved file can be loaded with pd.read_csv. . You can find the full code for this in Github gist or with the output in kaggle. . Clean up the downloaded files, (if required) . shutil.rmtree(str(EXTRACTED_PATH)) shutil.rmtree(str(DATA_PATH/&#39;wikiextractor&#39;)) Path(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;).unlink() .",
            "url": "https://mani2106.github.io/Blog-Posts/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "relUrl": "/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Building தமிழ் language tokenizer",
            "content": "Import required libraries . from pathlib import Path import sentencepiece as spm import pandas as pd . Read data from csv . lang_data = pd.read_csv(&#39;../input/tamil-wiki-data-extraction/filtered_data.csv.tar.gz&#39;, index_col=[0]) lang_data.head() . id url title text . 0 48482 | https://ta.wikipedia.org/wiki?curid=48482 | தென் துருவம் | தென் துருவம் தென் முனை தென் துருவம் என்பது புவ... | . 1 48485 | https://ta.wikipedia.org/wiki?curid=48485 | ஆர்க்டிக் வட்டம் | ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து... | . 2 48486 | https://ta.wikipedia.org/wiki?curid=48486 | நாஞ்சில் நாடன் | நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்... | . 3 48492 | https://ta.wikipedia.org/wiki?curid=48492 | டிக்கோயா | டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின... | . 4 48493 | https://ta.wikipedia.org/wiki?curid=48493 | நள்ளிரவுச் சூரியன் | நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த... | . lang_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 133412 entries, 0 to 133411 Data columns (total 4 columns): id 133412 non-null int64 url 133412 non-null object title 133412 non-null object text 133412 non-null object dtypes: int64(1), object(3) memory usage: 5.1+ MB . Setup paths . # Initialize directories OUTPUT_DIR = Path(&#39;/kaggle/working&#39;) TEXTS_DIR = OUTPUT_DIR/&#39;texts&#39; TOK_DIR = OUTPUT_DIR/&#39;tokenizer&#39; # Create directories TOK_DIR.mkdir() TEXTS_DIR.mkdir() . Prepare texts . We can pass a list of files as a comma seperated string according to documentation, So we can store each article in a text file and pass the names in a comma seperated string. . # Save all article texts in seperate files for t in lang_data.itertuples(): file_name = Path(TEXTS_DIR/f&#39;text_{t.Index}.txt&#39;) file_name.touch() with file_name.open(&#39;w&#39;) as f: f.write(t.text) . # Check files in directory len([t for t in TEXTS_DIR.iterdir()]), lang_data.shape[0] . (133412, 133412) . All the files have been converted to texts . Train sentencepiece model . Let&#39;s make a comma seperated string of filenames . files = &#39;,&#39;.join([str(t) for t in TEXTS_DIR.iterdir()]) files[:100] . &#39;/kaggle/working/texts/text_40902.txt,/kaggle/working/texts/text_44212.txt,/kaggle/working/texts/text&#39; . We must find the right vocab_size for the tokenizer, that can be done only by testing the tokenizer after building onw . for v in 8000, 16000, 20000, 30000: api_str = f&quot;&quot;&quot;--input={files} --vocab_size={v} --model_type=unigram --character_coverage=0.9995 --model_prefix={str(TOK_DIR)}/tok_{v}_size --max_sentence_length=20000&quot;&quot;&quot; print(&quot;Training with vocab set as:&quot;, v) spm.SentencePieceTrainer.train(api_str) . Training with vocab set as: 8000 Training with vocab set as: 16000 Training with vocab set as: 20000 Training with vocab set as: 30000 . Cleanup . !rm -rf /kaggle/working/texts/ . Let&#39;s test the models in another notebook, you can find the outputs in this kaggle notebook .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Building a Pokemon Classifier",
            "content": ". In this notebook, I used the pokemon images dataset from here but unfortuantely it is not available now. . Get data from Kaggle . # using kaggle api !kaggle datasets download -d mrgravelord/complete-pokemon-image-dataset . Downloading complete-pokemon-image-dataset.zip to /content 90% 121M/134M [00:01&lt;00:00, 77.3MB/s] 100% 134M/134M [00:01&lt;00:00, 95.2MB/s] . !unzip /content/complete-pokemon-image-dataset.zip . !rm /content/complete-pokemon-image-dataset.zip . Load required libraries . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import ReduceLROnPlateauCallback, SaveModelCallback from fastai.callbacks import CSVLogger . Prepare Data for training . path = Path(&quot;.&quot;) . Form data bunch object from the folders. . data = ImageDataBunch.from_folder(path, train=&quot;.&quot;, ds_tfms=get_transforms(), size=128, bs=64, valid_pct=0.2).normalize(imagenet_stats) . Check the number of different pokemon images that we have. . len(data.classes) . 928 . Creating a CNN model from architecture of resnet18. I could use a bigger model but I would not be able to serve them from Google or OneDrive because of the size. . Error Rate is 1-accuracy. | Using mixup for better regularization. | Converting the operations to be performed in a lower precision, more | . learn = cnn_learner(data, models.resnet18, metrics=error_rate).mixup().to_fp16() . Adding callbacks to monitor the training process and . Reduce the learning_rate by using the ReduceLROnPlateauCallback. | Saving the model on every improvement in error_rate | Log the training stats in a csv file. | . callbacks_list = [ ReduceLROnPlateauCallback(learn=learn, monitor=&#39;error_rate&#39;, factor=1e-6, patience=5, min_delta=1e-5), SaveModelCallback(learn, mode=&quot;min&quot;, every=&#39;improvement&#39;, monitor=&#39;error_rate&#39;, name=&#39;best&#39;), CSVLogger(learn=learn, append=True) ] . Start Training . Now, All the setup has been made, Let&#39;s train the model with default parameters, for 15 epochs. . learn.fit_one_cycle(15, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 7.173859 | 6.631011 | 0.985798 | 01:09 | . 1 | 6.246106 | 5.397111 | 0.870562 | 01:08 | . 2 | 5.001963 | 3.665833 | 0.672144 | 01:07 | . 3 | 4.327330 | 2.772682 | 0.540881 | 01:06 | . 4 | 3.941842 | 2.320177 | 0.469669 | 01:06 | . 5 | 3.648211 | 2.069086 | 0.420978 | 01:06 | . 6 | 3.423512 | 1.901359 | 0.372895 | 01:06 | . 7 | 3.328791 | 1.758360 | 0.343883 | 01:06 | . 8 | 3.140401 | 1.657776 | 0.326841 | 01:06 | . 9 | 3.044241 | 1.591135 | 0.313857 | 01:07 | . 10 | 2.940413 | 1.538893 | 0.300670 | 01:06 | . 11 | 2.759924 | 1.502491 | 0.290931 | 01:07 | . 12 | 2.781063 | 1.474272 | 0.283628 | 01:06 | . 13 | 2.761597 | 1.457427 | 0.282816 | 01:06 | . 14 | 2.700450 | 1.459171 | 0.280179 | 01:07 | . Better model found at epoch 0 with error_rate value: 0.9857983589172363. Better model found at epoch 1 with error_rate value: 0.870561957359314. Better model found at epoch 2 with error_rate value: 0.6721444725990295. Better model found at epoch 3 with error_rate value: 0.5408805012702942. Better model found at epoch 4 with error_rate value: 0.46966931223869324. Better model found at epoch 5 with error_rate value: 0.4209778904914856. Epoch 6: reducing lr to 2.599579409433508e-09 Better model found at epoch 6 with error_rate value: 0.37289512157440186. Better model found at epoch 7 with error_rate value: 0.34388312697410583. Better model found at epoch 8 with error_rate value: 0.3268411457538605. Better model found at epoch 9 with error_rate value: 0.3138567805290222. Better model found at epoch 10 with error_rate value: 0.30066952109336853. Better model found at epoch 11 with error_rate value: 0.29093122482299805. Epoch 12: reducing lr to 2.606527959586539e-10 Better model found at epoch 12 with error_rate value: 0.2836275100708008. Better model found at epoch 13 with error_rate value: 0.28281599283218384. Better model found at epoch 14 with error_rate value: 0.2801785469055176. . Now that we have got some decent accuracy let us try to save the model and interpret from it. . In the following cell, I . Load the best weights saved by the callbacks during training. | Convert the model back to use 32 bit precision. | Export the model as a whole. | Export the weights alone. | . learn.load(&quot;best&quot;); learn.to_fp32() learn.export(&quot;pokemon_resnet18_st1.pkl&quot;) learn.save(&quot;pokemon_resnet18_st1_wgts&quot;) . Model Interpretation . It is very important that we get to know what the model has learnt from the training process. We can do that with the help of ClassificationInterpretation class from the fastai library. . # Create interpretation object interp = ClassificationInterpretation.from_learner(learn) # Get the instances where the model has made the most error (by loss value) in the validation set. losses,idxs = interp.top_losses() # Check whether the values are all of same length as the validation set len(data.valid_ds)==len(losses)==len(idxs) . True . Interpret the images where the model made errors during the validation. . The cell below shows . the image. | the model&#39;s prediction of that image. | the actual label of that image. | the loss and probability(the extent to which the model is sure about it&#39;s prediction). | . . You can notice that the image has some of it&#39;s regions blighted, as far I know these are the regions that the model looked at to make the prediction for the corresponding image. . interp.plot_top_losses(9, figsize=(15,11)) . Let us also see which pokemon have confused the model the most. . interp.most_confused(min_val=3) . [(&#39;Sharpedo(Mega)&#39;, &#39;Sharpedo&#39;, 7), (&#39;Moltres&#39;, &#39;Rapidash&#39;, 4), (&#39;Thundurus(Incarnate)&#39;, &#39;Thundurus(Therian)&#39;, 4), (&#39;Charizard(Mega Y)&#39;, &#39;Charizard&#39;, 3), (&#39;Greninja&#39;, &#39;Greninja(Ash)&#39;, 3), (&#39;Groudon(Primal)&#39;, &#39;Incineroar&#39;, 3), (&#39;Latias(Mega)&#39;, &#39;Latios(Mega)&#39;, 3), (&#39;Nidoran(Female)&#39;, &#39;Nidorina&#39;, 3)] . Apart from the 2nd one in this list, You can see why the model was confused generally, most of it&#39;s confusion stem from the evolved species of the same pokemon. . . Let&#39;s try to train the model a little bit differently this time. . learn.load(&#39;best&#39;); . Till now we have been training only the tail region of the model (i.e.) only the last two/ three layers of our model, so essentially this model is almost same as the model which was pretrained on 1000 categories of the ImageNet dataset with some minor tweaks for our problem here. We have some options to improve the model, which are . Train all the layers so that the model can adapt to the current classification problem. We do that by unfreeze(). | Train with a very low learning rate so that it does&#39;nt forget the learnings from the pretrained weights. | . . Let&#39;s see how well we can improve the model. . learn.to_fp16() learn.unfreeze() . Before we start training again, We need to figure out at what speed the neural network should learn, this is controlled by the learning rate parameter and finding a value for is crucial to the training process. . Luckily the fastai&#39;s lr_find method will help us do just the same. . learn.lr_find(start_lr=1e-20) # Plot the learning rates and the corresponding losses. learn.recorder.plot(suggestion=True) # Get the suggested learning rate min_grad_lr = learn.recorder.min_grad_lr . Min numerical gradient: 9.77E-17 Min loss divided by 10: 6.46E-09 . Use the same callbacks as before and train for 30 epochs. . learn.fit_one_cycle(30, min_grad_lr, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 2.648827 | 1.461440 | 0.280179 | 01:08 | . 1 | 2.687755 | 1.460599 | 0.282004 | 01:08 | . 2 | 2.646746 | 1.471151 | 0.281802 | 01:07 | . 3 | 2.647440 | 1.466154 | 0.284033 | 01:07 | . 4 | 2.687051 | 1.459437 | 0.280179 | 01:07 | . 5 | 2.656536 | 1.468453 | 0.284236 | 01:07 | . 6 | 2.646480 | 1.469294 | 0.280787 | 01:08 | . 7 | 2.707206 | 1.462577 | 0.281802 | 01:08 | . 8 | 2.650942 | 1.462410 | 0.283222 | 01:07 | . 9 | 2.657768 | 1.457848 | 0.279976 | 01:07 | . 10 | 2.689249 | 1.459695 | 0.281193 | 01:07 | . 11 | 2.656215 | 1.463556 | 0.282613 | 01:07 | . 12 | 2.715505 | 1.461581 | 0.282410 | 01:09 | . 13 | 2.689469 | 1.462295 | 0.282410 | 01:08 | . 14 | 2.685328 | 1.460551 | 0.283222 | 01:08 | . 15 | 2.624705 | 1.458205 | 0.283222 | 01:10 | . 16 | 2.675736 | 1.468264 | 0.283628 | 01:11 | . 17 | 2.641450 | 1.461090 | 0.281193 | 01:10 | . 18 | 2.662758 | 1.455160 | 0.283425 | 01:12 | . 19 | 2.662972 | 1.459052 | 0.283019 | 01:13 | . 20 | 2.711507 | 1.464223 | 0.282207 | 01:13 | . 21 | 2.697404 | 1.463553 | 0.283425 | 01:13 | . 22 | 2.643310 | 1.462558 | 0.280584 | 01:12 | . 23 | 2.657411 | 1.463225 | 0.285048 | 01:12 | . 24 | 2.679297 | 1.467203 | 0.283425 | 01:13 | . 25 | 2.654091 | 1.464559 | 0.281599 | 01:12 | . 26 | 2.619208 | 1.465727 | 0.283222 | 01:12 | . 27 | 2.622938 | 1.466129 | 0.280990 | 01:12 | . 28 | 2.646025 | 1.465645 | 0.284236 | 01:13 | . 29 | 2.679323 | 1.458704 | 0.284033 | 01:13 | . Better model found at epoch 0 with error_rate value: 0.2801785469055176. Better model found at epoch 9 with error_rate value: 0.27997565269470215. Epoch 11: reducing lr to 9.288489603500534e-23 Epoch 17: reducing lr to 5.97347999592849e-23 Epoch 29: reducing lr to 3.9089488838232423e-28 . We can see that the model has improved slightly but not much, other ways that we can try are . Try using a different architecture rather than resnet18. | Add more Image augmentation methods (even though fastai has some reasonable defaults). | . Persist the environment so that we would be able to deploy the model without any problems . !pip freeze &gt; resnet18.txt . You can skip the following section, where I just save the model to my drive. . Save model to Google Drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . Try the model . Curious to try out the model, I have built a small Flask web app which is hosted here. You can find the code for the same in my github repo. . . The website may take some time to load since it was hosted on a free tier heroku site. . That&#39;s it for this post, Please share it if you have found it useful. Don&#39;t hesitate to leave a comment if you find that any of my explanation needs some clarification. .",
            "url": "https://mani2106.github.io/Blog-Posts/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "relUrl": "/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "date": " • Jun 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A passionate Software Engineer and Data Science Practitioner, interested in Machine Learning, Deep Learning and Data Science. More of my work can be found here. . You can also contact me with information in the bottom of this page. .",
          "url": "https://mani2106.github.io/Blog-Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mani2106.github.io/Blog-Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}