{
  
    
        "post0": {
            "title": "Linear Algebra - d2l.ai Exercises - Part 3",
            "content": "Exercise setup . import tensorflow as tf A = tf.reshape(tf.range(20, dtype=tf.float32), (5,4)) A . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . Problems and answers . Prove that the transpose of a matrix A &#8217;s transpose is A : $(A^{T})^T$=$A$. . This is straightforward, transposing is basically converting rows to columns and vice-versa, so when done twice we would end up what we started with. . At = tf.transpose(A) At . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 0., 4., 8., 12., 16.], [ 1., 5., 9., 13., 17.], [ 2., 6., 10., 14., 18.], [ 3., 7., 11., 15., 19.]], dtype=float32)&gt; . At_t = tf.transpose(At) At_t . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . At_t == A . &lt;tf.Tensor: shape=(5, 4), dtype=bool, numpy= array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])&gt; . Show that the sum of transposes is equal to the transpose of a sum: $A^T+B^T=(A+B)^T$. . Let&#39;s consider a second matrix, $B$ . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 7.975751 , 7.70928 , 8.388272 , 11.001523 ], [ 7.6716766, 11.476339 , 6.2204466, 5.6182394], [ 9.765643 , 6.7869806, 8.873018 , 5.6852665], [ 8.200825 , 4.9842663, 11.172729 , 11.063158 ], [ 8.75681 , 8.760315 , 4.151512 , 5.0749035]], dtype=float32)&gt; . The transpose would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751 , 7.6716766, 9.765643 , 8.200825 , 8.75681 ], [ 7.70928 , 11.476339 , 6.7869806, 4.9842663, 8.760315 ], [ 8.388272 , 6.2204466, 8.873018 , 11.172729 , 4.151512 ], [11.001523 , 5.6182394, 5.6852665, 11.063158 , 5.0749035]], dtype=float32)&gt; . $A^T+B^T$ . The sum of the transposed matrices would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751, 11.671677, 17.765644, 20.200825, 24.75681 ], [ 8.70928 , 16.47634 , 15.786981, 17.984266, 25.760315], [10.388272, 12.220447, 18.873018, 25.17273 , 22.151512], [14.001523, 12.618239, 16.685266, 26.063158, 24.074903]], dtype=float32)&gt; . $(A+B)^T$ . The transposed sum would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751, 11.671677, 17.765644, 20.200825, 24.75681 ], [ 8.70928 , 16.47634 , 15.786981, 17.984266, 25.760315], [10.388272, 12.220447, 18.873018, 25.17273 , 22.151512], [14.001523, 12.618239, 16.685266, 26.063158, 24.074903]], dtype=float32)&gt; . $A^T+B^T == (A+B)^T$ ? . &lt;tf.Tensor: shape=(4, 5), dtype=bool, numpy= array([[ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True]])&gt; . All the numbers are equal, we can see that by looking at the results . Given any square matrix $A$ , is $A+A^T$ always symmetric? Why? . Let&#39;s define a square matrix . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]], dtype=float32)&gt; . The tranpose of the same would be . At = tf.transpose(A) At . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 4., 8., 12.], [ 1., 5., 9., 13.], [ 2., 6., 10., 14.], [ 3., 7., 11., 15.]], dtype=float32)&gt; . The sum of the tensors . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 5., 10., 15.], [ 5., 10., 15., 20.], [10., 15., 20., 25.], [15., 20., 25., 30.]], dtype=float32)&gt; . Let&#39;s see if the condition stands . &lt;tf.Tensor: shape=(4, 4), dtype=bool, numpy= array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])&gt; . I guess since we add the rows and columns of the same matrix and its transpose and also since addition is commutative (ie) $A+B = B+A$, all the numbers we add endup becoming equal in terms of their respective positions, so even tranposing the resultant matrix ends up being equal to the former. . Output of len(X) for tensor $X$ shaped (2, 3, 4) and does len(X) always correspond to the length of a certain axis of $X$? What is that axis? . Let&#39;s consider the following as $X$ . &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=int32)&gt; . len(X) . 2 . We can see that the length returns the size of the first axis, let us see if it does the same for the other arbitrary tensors . &lt;tf.Tensor: shape=(8, 1, 3), dtype=float32, numpy= array([[[11.439855 , 5.880226 , 5.9797716]], [[11.37106 , 5.619686 , 5.9706793]], [[ 6.4085245, 11.867535 , 4.3086786]], [[ 7.1461754, 8.795105 , 8.864346 ]], [[ 9.952526 , 7.6806755, 7.7797728]], [[10.933958 , 11.748696 , 6.464444 ]], [[ 5.296891 , 6.7806816, 4.316203 ]], [[ 8.316187 , 7.272793 , 9.020613 ]]], dtype=float32)&gt; . len(X) . 8 . &lt;tf.Tensor: shape=(1, 2, 3, 9), dtype=float32, numpy= array([[[[ 5.9411087, 6.242239 , 4.4269447, 7.913884 , 7.8960876, 7.511854 , 6.3407526, 11.290615 , 4.5310717], [ 7.182088 , 5.086608 , 4.0900164, 4.7155457, 8.863187 , 4.1158237, 10.514992 , 9.662274 , 8.8960705], [11.142818 , 6.125886 , 9.6489105, 7.8091097, 9.66531 , 9.282991 , 8.218669 , 11.877634 , 8.727693 ]], [[ 4.3840303, 8.792656 , 9.48595 , 9.231619 , 5.972165 , 11.478173 , 10.220118 , 10.394747 , 4.430291 ], [ 7.198678 , 7.2096577, 5.8975067, 4.6933975, 6.6245346, 11.958464 , 10.320432 , 11.609855 , 7.1605587], [ 6.389407 , 5.9069185, 7.974592 , 5.289855 , 5.713969 , 6.6944523, 4.1094055, 4.077242 , 8.026564 ]]]], dtype=float32)&gt; . len(X) . 1 . No matter what the shape of the tensor len always picks the first/outermost axis. . What happens when we divide $A$ by the sum of it&#39;s second axis? A / A.sum(axis=1) . Let&#39;s define $A$ . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . A / tf.reduce_sum(A, axis=1) . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-12-6e6b244e22c4&gt; in &lt;module&gt;() -&gt; 1 A / tf.reduce_sum(A, axis=1) /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y) 1162 with ops.name_scope(None, op_name, [x, y]) as name: 1163 try: -&gt; 1164 return func(x, y, name=name) 1165 except (TypeError, ValueError) as e: 1166 # Even if dispatching the op failed, the RHS may be a tensor aware /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in truediv(x, y, name) 1334 TypeError: If `x` and `y` have different dtypes. 1335 &#34;&#34;&#34; -&gt; 1336 return _truediv_python3(x, y, name) 1337 1338 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _truediv_python3(x, y, name) 1273 x = cast(x, dtype) 1274 y = cast(y, dtype) -&gt; 1275 return gen_math_ops.real_div(x, y, name=name) 1276 1277 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in real_div(x, y, name) 7327 return _result 7328 except _core._NotOkStatusException as e: -&gt; 7329 _ops.raise_from_not_ok_status(e, name) 7330 except _core._FallbackException: 7331 pass /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 6860 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6861 # pylint: disable=protected-access -&gt; 6862 six.raise_from(core._status_to_exception(e.code, message), None) 6863 # pylint: enable=protected-access 6864 /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value) InvalidArgumentError: Incompatible shapes: [5,4] vs. [5] [Op:RealDiv] . Ok, there seems to be shape inconsistencies to the resultant sum tensor. Let&#39;s see the sum output for the axes in tensor. . tf.reduce_sum(A, axis=1) . &lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)&gt; . tf.reduce_sum(A, axis=0) . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)&gt; . So I think, When we sum a tensor on a particular axis, the shape of the resultant tensor will end taking the shape with the other remaining axes, for example a tensor with shape (5, 4 ,3) when summed up along the third axis (2) the resultant tensor would be of shape (5, 4) . The shapes for the tensors summed along the rest of the axes can be understood by the same. . (5, 4, 3) axis = 0 : (4, 3) axis = 1 : (5, 3) axis = 2 : (5, 4) . When we do the division with the other resultant tensor, we can easily divide with it since the shapes follow the broadcasting rules . a - 5 X 4 summed_a - 4 result - 5 X 4 . The following is the result of the division . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[0. , 0.02222222, 0.04 , 0.05454545], [0.1 , 0.11111111, 0.12 , 0.12727273], [0.2 , 0.2 , 0.2 , 0.2 ], [0.3 , 0.2888889 , 0.28 , 0.27272728], [0.4 , 0.37777779, 0.36 , 0.34545454]], dtype=float32)&gt; . Question related to traveling between two points in Manhattan . I am not able to get the underlying concept needed to answer this question, I have asked some help, or you can comment below to help me understand this one, thanks in advance. . The summation outputs for tensor with shape (2, 3, 4) along axis 0, 1, and 2. . I think I have answered this in the question about A / A.sum(axis=1), that should apply to the any arbitrary shape, for this one it would turnout to be the following . (2, 3, 4) axis = 0 : (3, 4) axis = 1 : (2, 4) axis = 2 : (2, 3) . Feed a tensor with 3 or more axes to the linalg.norm. . What does this function compute for tensors of arbitrary shape? . Let&#39;s take a 3-d tensor . &lt;tf.Tensor: shape=(2, 2, 5), dtype=float32, numpy= array([[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.]], [[10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]]], dtype=float32)&gt; . tf.norm(X) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=49.699093&gt; . Let&#39;s take an arbitrary shaped tensor . &lt;tf.Tensor: shape=(8, 1, 2, 5), dtype=float32, numpy= array([[[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.]]], [[[10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]]], [[[20., 21., 22., 23., 24.], [25., 26., 27., 28., 29.]]], [[[30., 31., 32., 33., 34.], [35., 36., 37., 38., 39.]]], [[[40., 41., 42., 43., 44.], [45., 46., 47., 48., 49.]]], [[[50., 51., 52., 53., 54.], [55., 56., 57., 58., 59.]]], [[[60., 61., 62., 63., 64.], [65., 66., 67., 68., 69.]]], [[[70., 71., 72., 73., 74.], [75., 76., 77., 78., 79.]]]], dtype=float32)&gt; . tf.norm(X) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=409.2432&gt; . tf.norm still calculates the square root of squared sum of all numbers in the tensor, equivalent to the following . tf.sqrt( float(sum( [x*x for x in range(80)] )) ) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=409.2432&gt; . I am not sure if there was any change of behaviour expected in this, so I should try using mxnet to see if there is a difference in the above calculation of l2 norm. .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/09/d2lai-exercises-pt3.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/09/d2lai-exercises-pt3.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Preprocessing - d2l.ai Exercises - Part 2",
            "content": "Exercise setup . Let&#39;s use the sample datasets offered directly in colab . import pandas as pd import numpy as np data = pd.read_csv(&#39;/content/sample_data/california_housing_train.csv&#39;) . . data.head() . . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1283.0 | 1015.0 | 472.0 | 1.4936 | 66900.0 | . 1 -114.47 | 34.40 | 19.0 | 7650.0 | 1901.0 | 1129.0 | 463.0 | 1.8200 | 80100.0 | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 174.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 337.0 | 515.0 | 226.0 | 3.1917 | 73400.0 | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 326.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . data.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17000 entries, 0 to 16999 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 17000 non-null float64 1 latitude 17000 non-null float64 2 housing_median_age 17000 non-null float64 3 total_rooms 17000 non-null float64 4 total_bedrooms 17000 non-null float64 5 population 17000 non-null float64 6 households 17000 non-null float64 7 median_income 17000 non-null float64 8 median_house_value 17000 non-null float64 dtypes: float64(9) memory usage: 1.2 MB . Since there is no data missing, we will add random missing entries in the data . df = data.mask(np.random.random(data.shape) &lt; .1) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17000 entries, 0 to 16999 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 15290 non-null float64 1 latitude 15376 non-null float64 2 housing_median_age 15281 non-null float64 3 total_rooms 15325 non-null float64 4 total_bedrooms 15246 non-null float64 5 population 15358 non-null float64 6 households 15351 non-null float64 7 median_income 15298 non-null float64 8 median_house_value 15275 non-null float64 dtypes: float64(9) memory usage: 1.2 MB . Questions . Delete the column with the most missing values. . | Convert the preprocessed dataset to the tensor format. . | . Delete column with most missing values . Peek at the data with na . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1283.0 | 1015.0 | 472.0 | NaN | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1901.0 | 1129.0 | NaN | 1.8200 | NaN | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 174.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 337.0 | 515.0 | 226.0 | 3.1917 | NaN | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 326.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . Find the column name with most nas . column_with_most_na = max(list(df.columns), key=lambda x: len(df.loc[df[x].isna(), x])) . &#39;total_bedrooms&#39; . Remove the column . longitude latitude housing_median_age total_rooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1015.0 | 472.0 | NaN | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1129.0 | NaN | 1.8200 | NaN | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 515.0 | 226.0 | 3.1917 | NaN | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . Conversion of the preprocessed dataset to the tensor format. . We can split the dataset to inputs and output, with the median_house_value as the output . inputs = df.iloc[:, :-1].copy() outputs = df.iloc[:, -1].copy() inputs.head() . longitude latitude housing_median_age total_rooms population households median_income . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1015.0 | 472.0 | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1129.0 | NaN | 1.8200 | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 333.0 | 117.0 | 1.6509 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 515.0 | 226.0 | 3.1917 | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 624.0 | 262.0 | 1.9250 | . outputs.head() . 0 NaN 1 NaN 2 85700.0 3 NaN 4 65500.0 Name: median_house_value, dtype: float64 . import tensorflow as tf X, y = tf.constant(inputs.values), tf.constant(outputs.values) X, y . (&lt;tf.Tensor: shape=(17000, 7), dtype=float64, numpy= array([[-114.31 , 34.19 , 15. , ..., 1015. , 472. , nan], [-114.47 , 34.4 , nan, ..., 1129. , nan, 1.82 ], [-114.56 , 33.69 , 17. , ..., 333. , 117. , 1.6509], ..., [-124.3 , 41.84 , 17. , ..., 1244. , 456. , 3.0313], [-124.3 , 41.8 , 19. , ..., 1298. , 478. , 1.9797], [-124.35 , 40.54 , 52. , ..., 806. , 270. , 3.0147]])&gt;, &lt;tf.Tensor: shape=(17000,), dtype=float64, numpy=array([ nan, nan, 85700., ..., 103600., 85800., 94600.])&gt;) . This completes the second part of the preliminaries. .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/07/d2lai_exercises_pt2.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/07/d2lai_exercises_pt2.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Data Manipulation - d2l.ai Exercises - Part 1",
            "content": "This is the first notebook in a series to be posted aiming to solve and understand exercises from d2l.ai curriculum on deep learning, the corresponding lesson reference for this notebook is this link. . This series of practice notebook posts may use the exercises and content provided from d2l.ai, I write these to get a good hands-on practice in deep learning. . import tensorflow as tf . tf.__version__ . &#39;2.4.1&#39; . Setup for exercises . Problem 1 . X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4)) Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) X, Y . (&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]], dtype=float32)&gt;) . Problem 2 . a = tf.reshape(tf.range(3), (3, 1)) b = tf.reshape(tf.range(2), (1, 2)) a, b . (&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[0], [1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)&gt;) . Solutions . Problem 1 . X == Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[False, True, False, True], [False, False, False, False], [False, False, False, False]])&gt; . X &lt; Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[ True, False, True, False], [False, False, False, False], [False, False, False, False]])&gt; . X &gt; Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[False, False, False, False], [ True, True, True, True], [ True, True, True, True]])&gt; . The operations are as expected of an elementwise comparison. Let&#39;s try to check if the operations are opposites of each other by trying to not one of them. . (X &gt; Y) == tf.math.logical_not(X &lt; Y) . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[ True, False, True, False], [ True, True, True, True], [ True, True, True, True]])&gt; . We can see that apart from two cases where the numbers were equal(1 and 3), all the other values matched . Problem 2 . a = tf.reshape(tf.range(16), (2, 4, -1)) b = tf.reshape(tf.range(16), (4, 2, -1)) a, b . (&lt;tf.Tensor: shape=(2, 4, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11], [12, 13], [14, 15]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4, 2, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]], [[12, 13], [14, 15]]], dtype=int32)&gt;) . a + b . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-12-bd58363a63fc&gt; in &lt;module&gt;() -&gt; 1 a + b /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y) 1162 with ops.name_scope(None, op_name, [x, y]) as name: 1163 try: -&gt; 1164 return func(x, y, name=name) 1165 except (TypeError, ValueError) as e: 1166 # Even if dispatching the op failed, the RHS may be a tensor aware /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _add_dispatch(x, y, name) 1484 return gen_math_ops.add(x, y, name=name) 1485 else: -&gt; 1486 return gen_math_ops.add_v2(x, y, name=name) 1487 1488 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in add_v2(x, y, name) 470 return _result 471 except _core._NotOkStatusException as e: --&gt; 472 _ops.raise_from_not_ok_status(e, name) 473 except _core._FallbackException: 474 pass /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 6860 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6861 # pylint: disable=protected-access -&gt; 6862 six.raise_from(core._status_to_exception(e.code, message), None) 6863 # pylint: enable=protected-access 6864 /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value) InvalidArgumentError: Incompatible shapes: [2,4,2] vs. [4,2,2] [Op:AddV2] . I tried using tensors of 3d shapes, thinking it might but it did&#39;nt, so I was searching about the rules to determine whether an array can be broadcasted or not and found this documentation, where the conditions are explained, the main points to consider broadcasting are, if the dimensions . are equal, or . | one of them is 1 . | . In the above case we hade shapes: [2,4,2] vs. [4,2,2], Let&#39;s try a different shape . a = tf.reshape(tf.range(12), (6, 2, -1)) b = tf.reshape(tf.range(16), (1, -1)) a, b . (&lt;tf.Tensor: shape=(6, 2, 1), dtype=int32, numpy= array([[[ 0], [ 1]], [[ 2], [ 3]], [[ 4], [ 5]], [[ 6], [ 7]], [[ 8], [ 9]], [[10], [11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(1, 16), dtype=int32, numpy= array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], dtype=int32)&gt;) . a + b . &lt;tf.Tensor: shape=(6, 2, 16), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]], [[ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [ 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]], [[ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [ 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]], [[ 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [ 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]], [[ 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]], [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]], dtype=int32)&gt; . Similar to the examples in the link, the above example followed the rules and operation(addition) could happen with the help of broadcasting. . a - 6 X 2 X 1 b - 1 X 16 a + b - 6 X 2 X 16 .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/04/d2lai-exercises-pt1.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/04/d2lai-exercises-pt1.html",
            "date": " • Feb 4, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Gradient Descent for Linear Regression",
            "content": "Implementing Linear Regression with Gradient Descent . This post is my understanding of Linear Regression, Please feel free to comment and point out any errors if seen. . The Cost Function . The cost function is used to measure the correctness of the current solution(hypothesis), the function can be mean of squares/square roots of the errors. It is represented as the following . J($ theta_0$, $ theta_1$,..$ theta_n$) . where $ theta_0$ is a constant, $ theta_1$…$ theta_n$ are the parameters of the equation we are trying to solve . In simple ML terms(I think) Each one of the parameter represent the weight of each feature in the dataset, that we are using to build the model . This is the formula for the cost function with mean of square differences.$^0$ . 12m∑i=1m(hθ(x)−y)2 frac{1}{2m} sum_{i=1}^{m} (h_ theta(x) - y)^22m1​i=1∑m​(hθ​(x)−y)2 . where $h_0(x)$ is the hypothesis or the predicted value for $y$ and $m$ is the number of training examples . A sample pseudocode for the mean of squares of errors would be . Calculate the sum of square differences / errors between each value $(X* theta)$ vector and y vector For each training example Multiply the feature values $X_1$, $X_2$,..$X_n$ with it’s corresponding weights $ theta_1$, $ theta_2 cdots theta_n$, and add the constant, $ theta_0$. . | Subtract the above value from the $y$ target value of that example and square the difference. . | . | Sum all the differences / errors | . | Take the mean of the differences by the dividing with the number of training examples. | . It can be represented like $h_0(x) = theta_0+ theta_1X_1+ theta_2X_2+ cdots+ theta_nX_n$ where $n$ is the number of features we are using for the problem. . Gradient Descent . We need to update the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ so that the cost function . J(θ0,θ1,⋯θn)=12m∑i=1m(hθ(xi)−yi)2J( theta_0, theta_1, cdots theta_n) = frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2J(θ0​,θ1​,⋯θn​)=2m1​∑i=1m​(hθ​(xi)−yi)2 can be minimized. . So to find the minimum for the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ the update is performed like below . θj:=θj∗α∂∂θj∗J(θ0,θ1,⋯θj) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)θj​:=θj​∗α∂θj​∂​∗J(θ0​,θ1​,⋯θj​) . Where, . The := is assignment operator | The $ theta_j$ is the parameter to update, j is the feature index number | The $ alpha$ is the learning rate | The $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ is the derivative term of the cost function, it is like slope$^2$ of the line tangent$^1$ to the curve touching on where the $ theta$ is present in that curve. | . For each feature in the dataset the update has to be done simultaneously for each parameter $ theta$, until the convergence / error given by cost function at its minimum. . Deriving the derivative term $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ . $^3$To simplify the problem I am considering that we have 2 parameters $ theta_0$ and $ theta_1$, our hypothesis $h_0(x)$ function becomes $ theta_0+ theta_1X$ . Now let $g( theta_0, theta_1)$ be our derivative term . g(θ0,θ1)=J(θ0,θ1)g( theta_0, theta_1)=J( theta_0, theta_1)g(θ0​,θ1​)=J(θ0​,θ1​) . =(12m∑i=1m(hθ(xi)−yi)2)= ( frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2)=(2m1​i=1∑m​(hθ​(xi)−yi)2) . =(12m∑i=1m(θ0+θ1Xi−yi)2)=( frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2)=(2m1​i=1∑m​(θ0​+θ1​Xi−yi)2) . consider f(θ0,θ1)=hθ(xi)−yif( theta_0, theta_1) = h_ theta(x^i) - y^if(θ0​,θ1​)=hθ​(xi)−yi . now our equation becomes . g(θ0,θ1)=12m∑i=1m(f(θ0,θ1)i)2g( theta_0, theta_1)= frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2g(θ0​,θ1​)=2m1​i=1∑m​(f(θ0​,θ1​)i)2 . subsutituting the value of $f( theta_0, theta_1)$ the equation becomes . g(f(θ0,θ1)i)=12m∑i=1m(θ0+θ1Xi−yi)2(1)g(f( theta_0, theta_1)^i)= frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2 tag{1}g(f(θ0​,θ1​)i)=2m1​i=1∑m​(θ0​+θ1​Xi−yi)2(1) . Now let us derive the partial derivative for $(1)$ . ∂∂θjg(f(θ0,θ1)i)=∂∂θj12m∑i=1m(f(θ0,θ1)i)2 frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_j} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2∂θj​∂​g(f(θ0​,θ1​)i)=∂θj​∂​2m1​i=1∑m​(f(θ0​,θ1​)i)2 . Let j be 0 . ∂∂θ0g(f(θ0,θ1)i)=∂∂θ012m∑i=1m(f(θ0,θ1)i)2 frac{ partial}{ partial theta_0}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_0} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2∂θ0​∂​g(f(θ0​,θ1​)i)=∂θ0​∂​2m1​i=1∑m​(f(θ0​,θ1​)i)2 . since we are performing the partial derivative with respect to $ theta_0$ other variables are considered constant, the following is similar to $ frac{ partial}{ partial x}$ of $(x^2+y)$ which is $2x$ . =1m∑i=1mf(θ0,θ1)i= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i=m1​i=1∑m​f(θ0​,θ1​)i . =1m∑i=1m(hθ(x)−y)= frac{1}{m} sum_{i=1}^{m} (h_ theta(x) - y)=m1​i=1∑m​(hθ​(x)−y) . This is because of the chain rule, when we take derivative of a function like $(1)$, we need to use this formula below . ∂∂θjg(f(θ0,θ1))=∂∂θjg(θ0,θ1)∗∂∂θjf(θ0,θ1)(2) frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1) tag{2}∂θj​∂​g(f(θ0​,θ1​))=∂θj​∂​g(θ0​,θ1​)∗∂θj​∂​f(θ0​,θ1​)(2) . In case when j = 0 the partial derivative of $g$ becomes . ∂∂θ0g(θ0,θ1)=12m∗2(∑i=1mf(θ0,θ1)i)2−1=1m∑i=1mf(θ0,θ1)i frac{ partial}{ partial theta_0}g( theta_0, theta_1) = frac{1}{ cancel2m}* cancel2( sum_{i=1}^{m} f( theta_0, theta_1)^i)^{2-1}= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i∂θ0​∂​g(θ0​,θ1​)=2 | ​m1​∗2 | ​(i=1∑m​f(θ0​,θ1​)i)2−1=m1​i=1∑m​f(θ0​,θ1​)i . and the partial derivative of $f$ becomes . ∂∂θ0f(θ0,θ1)=∂∂θ0(hθ(xi)−yi)(3) frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}(h_ theta(x^i) - y^i) tag{3}∂θ0​∂​f(θ0​,θ1​)=∂θ0​∂​(hθ​(xi)−yi)(3) . ∂∂θ0f(θ0,θ1)=∂∂θ0(θ0+θ1Xi−yi)=1 frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}( theta_0+ theta_1X^i - y^i) = 1∂θ0​∂​f(θ0​,θ1​)=∂θ0​∂​(θ0​+θ1​Xi−yi)=1 . since other variables are considered constants, that gives us . ∂∂θjg(θ0,θ1)∗∂∂θjf(θ0,θ1)=1m∑i=1mf(θ0,θ1)i∗1=1m∑i=1mf(θ0,θ1)i frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1)= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * 1 = frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i∂θj​∂​g(θ0​,θ1​)∗∂θj​∂​f(θ0​,θ1​)=m1​i=1∑m​f(θ0​,θ1​)i∗1=m1​i=1∑m​f(θ0​,θ1​)i . Let’s start from Equation $(1)$ to perform the partial derivative when j = 1 . The partial derivative of $g( theta_0, theta_1)$ with respect to $ theta_1$ is same from the last derivation but the partial derivative of $f( theta_0, theta_1)$ becomes . Consider $(3)$ when j = 1 . ∂∂θ1f(θ1,θ1)=∂∂θ1(hθ(xi)−yi)=∂∂θ1(θ0+θ1Xi−yi) frac{ partial}{ partial theta_1}f( theta_1, theta_1) = frac{ partial}{ partial theta_1}(h_ theta(x^i) - y^i) = frac{ partial}{ partial theta_1}( theta_0+ theta_1X^i - y^i)∂θ1​∂​f(θ1​,θ1​)=∂θ1​∂​(hθ​(xi)−yi)=∂θ1​∂​(θ0​+θ1​Xi−yi) . variables other than $ theta_1$ are considered constants, so they become 0 and $ frac{ partial}{ partial theta_1} theta_1$ = 1, so our equation becomes . ∂∂θ1f(θ0,θ1)=0+1∗Xi−0=Xi frac{ partial}{ partial theta_1}f( theta_0, theta_1)= 0+1* X^i-0 =X^i∂θ1​∂​f(θ0​,θ1​)=0+1∗Xi−0=Xi . and according to the chain rule $(2)$ and replacing the partials . ∂∂θ1g(f(θ0,θ1))=∂∂θ1g(θ0,θ1)∗∂∂θ1f(θ0,θ1) frac{ partial}{ partial theta_1}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_1}g( theta_0, theta_1) * frac{ partial}{ partial theta_1} f( theta_0, theta_1)∂θ1​∂​g(f(θ0​,θ1​))=∂θ1​∂​g(θ0​,θ1​)∗∂θ1​∂​f(θ0​,θ1​) . =1m∑i=1mf(θ0,θ1)i∗Xi= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * X^i=m1​i=1∑m​f(θ0​,θ1​)i∗Xi . =1m∑i=1m(θ0+θ1Xi−yi)∗Xi= frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i) * X^i=m1​i=1∑m​(θ0​+θ1​Xi−yi)∗Xi . Now we can use the derivatives in the Gradient Descent algorithm . θj:=θj∗α∂∂θj∗J(θ0,θ1,⋯θj) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)θj​:=θj​∗α∂θj​∂​∗J(θ0​,θ1​,⋯θj​) . repeat until convergence { θ0:=θ0∗α1m∑i=1m(θ0+θ1Xi−yi) theta_0 := theta_0 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)θ0​:=θ0​∗αm1​∑i=1m​(θ0​+θ1​Xi−yi) . θ1:=θ1∗α1m∑i=1m(θ0+θ1Xi−yi)∗Xi theta_1 := theta_1 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)* X^iθ1​:=θ1​∗αm1​∑i=1m​(θ0​+θ1​Xi−yi)∗Xi } . One disadvantage in Gradient Descent is that depending on the position it is initialized at the start, but in linear regression the cost function (mean of sqaured errors) is a convex function (ie) it is in shape of a bowl when plotted on the graph. . I tried to implement this in python which can be found here . Resources and references . How to implement a machine learning algorithm | Understanding math in Machine learning . | $^0$ Most of the content and explanation is from Coursera’s - Machine Learning class . | $^1$ Tangent is a line which touches exactly at one point of a curve. . | $^2$ Slope of a line given any two points on the line is the ratio number of points we need to rise/descend and move away/towards the origin to the meet the other point. | . . Image from wikihow . | $^3$ Derivation referred from here . | .",
            "url": "https://mani2106.github.io/Blog-Posts/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "relUrl": "/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Tabular data modeling - with Tensorflow",
            "content": "In this notebook, I was giving a shot with Tensorflow for tabular data modeling, this particular competition data is quite imbalanced as you will see in this notebook a little later, This example is more or else fully adapted from the example from Tensorflow docs . Import required libraries . For Modeling . import tensorflow as tf from tensorflow import keras . For Getting data from file to variable . import numpy as np import pandas as pd . For Visualization . import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . For Evaluation and preprocessing . import sklearn from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer . Download data . !unzip data.zip . Archive: data.zip creating: Dataset/ inflating: Dataset/Train.csv inflating: Dataset/sample_submission.csv inflating: Dataset/Test.csv . import pandas as pd from pathlib import Path DATA_PATH = Path(&#39;/content/Dataset&#39;) train_data = pd.read_csv(DATA_PATH/&#39;Train.csv&#39;, index_col=0, infer_datetime_format=True, converters={&#39;DATE&#39;:pd.to_datetime}) train_data.head() . DATE X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_10 X_11 X_12 X_13 X_14 X_15 MULTIPLE_OFFENSE . INCIDENT_ID . CR_102659 2004-07-04 | 0 | 36 | 34 | 2 | 1 | 5 | 6 | 1 | 6 | 1 | 174 | 1.0 | 92 | 29 | 36 | 0 | . CR_189752 2017-07-18 | 1 | 37 | 37 | 0 | 0 | 11 | 17 | 1 | 6 | 1 | 236 | 1.0 | 103 | 142 | 34 | 1 | . CR_184637 2017-03-15 | 0 | 3 | 2 | 3 | 5 | 1 | 0 | 2 | 3 | 1 | 174 | 1.0 | 110 | 93 | 34 | 1 | . CR_139071 2009-02-13 | 0 | 33 | 32 | 2 | 1 | 7 | 1 | 1 | 6 | 1 | 249 | 1.0 | 72 | 29 | 34 | 1 | . CR_109335 2005-04-13 | 0 | 33 | 32 | 2 | 1 | 8 | 3 | 0 | 5 | 1 | 174 | 0.0 | 112 | 29 | 43 | 1 | . Target Distribution . train_data[&#39;MULTIPLE_OFFENSE&#39;].value_counts().plot.bar() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f34852fcb70&gt; . neg, pos = np.bincount(train_data[&#39;MULTIPLE_OFFENSE&#39;]) total = neg + pos print(&#39;Examples: n Total: {} n Positive: {} ({:.2f}% of total) n&#39;.format( total, pos, 100 * pos / total)) . Examples: Total: 23856 Positive: 22788 (95.52% of total) . We do have an imbalanced dataset, as shown above, 95% of the targets are positive (Class 1). . Data Preprocessing . seed=98 np.random.seed(seed=seed) tf.random.set_seed(seed) . We shall drop the date column and try to model with only the logging parameters . cleaned_df = train_data.drop(&#39;DATE&#39;, axis=&#39;columns&#39;) . We split the training data to three train, test and eval sets . train_df, test_df = train_test_split(cleaned_df, test_size=0.2, random_state=seed, stratify=cleaned_df[&#39;MULTIPLE_OFFENSE&#39;]) . train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=seed, stratify=train_df[&#39;MULTIPLE_OFFENSE&#39;]) . train_labels = np.array(train_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) bool_train_labels = train_labels != 0 val_labels = np.array(val_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) test_labels = np.array(test_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) train_features = np.array(train_df) val_features = np.array(val_df) test_features = np.array(test_df) . Create a preprocessing pipeline with scikit-learn . from sklearn.pipeline import Pipeline preproc_pipe = Pipeline([ (&#39;median_imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler()) ]) . train_features = preproc_pipe.fit_transform(train_features) val_features = preproc_pipe.transform(val_features) test_features = preproc_pipe.transform(test_features) # ensure that the values are within a range train_features = np.clip(train_features, -5, 5) val_features = np.clip(val_features, -5, 5) test_features = np.clip(test_features, -5, 5) . Check the dimensions of our data . print(&#39;Training labels shape:&#39;, train_labels.shape) print(&#39;Validation labels shape:&#39;, val_labels.shape) print(&#39;Test labels shape:&#39;, test_labels.shape) print(&#39;Training features shape:&#39;, train_features.shape) print(&#39;Validation features shape:&#39;, val_features.shape) print(&#39;Test features shape:&#39;, test_features.shape) . Training labels shape: (15267,) Validation labels shape: (3817,) Test labels shape: (4772,) Training features shape: (15267, 15) Validation features shape: (3817, 15) Test features shape: (4772, 15) . Let&#39;s check if our preprocessing has shown a distinction between the classes of the dataset . pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns) neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns) sns.jointplot(pos_df[&#39;X_10&#39;], pos_df[&#39;X_15&#39;], kind=&#39;hex&#39;, xlim = (-1,1), ylim = (-1,1)) plt.suptitle(&quot;Positive distribution&quot;) sns.jointplot(neg_df[&#39;X_10&#39;], neg_df[&#39;X_15&#39;], kind=&#39;hex&#39;, xlim = (-1,1), ylim = (-1,1)) _ = plt.suptitle(&quot;Negative distribution&quot;) . There is a slight difference in terms of the value, (ie) for the positive class the distribution is slightly on the negative side of zero, and the negative class is slightly on the positive side of zero . Model setup . Let&#39;s setup the model layers and the evaluation metrics . def plot_loss(history, label, n): # Use a log scale to show the wide range of values. plt.semilogy(history.epoch, history.history[&#39;loss&#39;], color=colors[n], label=&#39;Train &#39;+label) plt.semilogy(history.epoch, history.history[&#39;val_loss&#39;], color=colors[n], label=&#39;Val &#39;+label, linestyle=&quot;--&quot;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() . METRICS = [ keras.metrics.TruePositives(name=&#39;tp&#39;), keras.metrics.FalsePositives(name=&#39;fp&#39;), keras.metrics.TrueNegatives(name=&#39;tn&#39;), keras.metrics.FalseNegatives(name=&#39;fn&#39;), keras.metrics.BinaryAccuracy(name=&#39;accuracy&#39;), keras.metrics.Precision(name=&#39;precision&#39;), keras.metrics.Recall(name=&#39;recall&#39;), keras.metrics.AUC(name=&#39;auc&#39;), ] def make_model(metrics=METRICS, output_bias=None): if output_bias is not None: output_bias = tf.keras.initializers.Constant(output_bias) model = keras.Sequential([ keras.layers.Dense( 16, activation=&#39;relu&#39;, input_shape=(train_features.shape[-1],)), keras.layers.Dropout(0.5), keras.layers.Dense(1, activation=&#39;sigmoid&#39;, bias_initializer=output_bias), ]) model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.BinaryCrossentropy(), metrics=metrics) return model . Set constants and callbacks . EPOCHS = 500 BATCH_SIZE = 4096 early_stopping = tf.keras.callbacks.EarlyStopping( monitor=&#39;val_recall&#39;, verbose=1, patience=10, mode=&#39;max&#39;, restore_best_weights=True) . Calculate initial bias for a smooth training . initial_bias = np.log([pos/neg]) initial_bias . array([3.06044634]) . model = make_model(output_bias=initial_bias) . results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0) print(&quot;Loss: {:0.4f}&quot;.format(results[0])) . Loss: 0.2112 . import os, tempfile initial_weights = os.path.join(tempfile.mkdtemp(),&#39;initial_weights&#39;) model.save_weights(initial_weights) . Calculate class weights to set to the model . weight_for_0 = (1 / neg)*(total)/2.0 weight_for_1 = (1 / pos)*(total)/2.0 class_weight = {0: weight_for_0, 1: weight_for_1} print(&#39;Weight for class 0: {:.2f}&#39;.format(weight_for_0)) print(&#39;Weight for class 1: {:.2f}&#39;.format(weight_for_1)) . Weight for class 0: 11.17 Weight for class 1: 0.52 . Train the model . weighted_model = make_model() weighted_model.load_weights(initial_weights) weighted_history = weighted_model.fit( train_features, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(val_features, val_labels), # The class weights go here class_weight=class_weight) . Epoch 1/500 4/4 [==============================] - 1s 260ms/step - loss: 2.1039 - tp: 32485.0000 - fp: 1406.0000 - tn: 131.0000 - fn: 329.0000 - accuracy: 0.9495 - precision: 0.9585 - recall: 0.9900 - auc: 0.4448 - val_loss: 0.2044 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4527 Epoch 2/500 4/4 [==============================] - 0s 21ms/step - loss: 2.0170 - tp: 14580.0000 - fp: 683.0000 - tn: 0.0000e+00 - fn: 4.0000 - accuracy: 0.9550 - precision: 0.9553 - recall: 0.9997 - auc: 0.4581 - val_loss: 0.2020 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4656 Epoch 3/500 4/4 [==============================] - 0s 20ms/step - loss: 1.9718 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4609 - val_loss: 0.1997 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4698 Epoch 4/500 4/4 [==============================] - 0s 21ms/step - loss: 1.9492 - tp: 14572.0000 - fp: 681.0000 - tn: 2.0000 - fn: 12.0000 - accuracy: 0.9546 - precision: 0.9554 - recall: 0.9992 - auc: 0.4661 - val_loss: 0.1975 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4828 Epoch 5/500 4/4 [==============================] - 0s 22ms/step - loss: 1.9312 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4605 - val_loss: 0.1955 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4899 Epoch 6/500 4/4 [==============================] - 0s 21ms/step - loss: 1.8575 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4825 - val_loss: 0.1936 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4988 Epoch 7/500 4/4 [==============================] - 0s 20ms/step - loss: 1.8257 - tp: 14560.0000 - fp: 679.0000 - tn: 4.0000 - fn: 24.0000 - accuracy: 0.9540 - precision: 0.9554 - recall: 0.9984 - auc: 0.4858 - val_loss: 0.1918 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5161 Epoch 8/500 4/4 [==============================] - 0s 20ms/step - loss: 1.7717 - tp: 14555.0000 - fp: 677.0000 - tn: 6.0000 - fn: 29.0000 - accuracy: 0.9538 - precision: 0.9556 - recall: 0.9980 - auc: 0.5054 - val_loss: 0.1902 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5260 Epoch 9/500 4/4 [==============================] - 0s 22ms/step - loss: 1.7657 - tp: 14551.0000 - fp: 678.0000 - tn: 5.0000 - fn: 33.0000 - accuracy: 0.9534 - precision: 0.9555 - recall: 0.9977 - auc: 0.5002 - val_loss: 0.1886 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5330 Epoch 10/500 4/4 [==============================] - 0s 25ms/step - loss: 1.7285 - tp: 14539.0000 - fp: 678.0000 - tn: 5.0000 - fn: 45.0000 - accuracy: 0.9526 - precision: 0.9554 - recall: 0.9969 - auc: 0.5037 - val_loss: 0.1872 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5431 Epoch 11/500 1/4 [======&gt;.......................] - ETA: 0s - loss: 1.5431 - tp: 3918.0000 - fp: 166.0000 - tn: 1.0000 - fn: 11.0000 - accuracy: 0.9568 - precision: 0.9594 - recall: 0.9972 - auc: 0.5130Restoring model weights from the end of the best epoch. 4/4 [==============================] - 0s 22ms/step - loss: 1.6912 - tp: 14541.0000 - fp: 675.0000 - tn: 8.0000 - fn: 43.0000 - accuracy: 0.9530 - precision: 0.9556 - recall: 0.9971 - auc: 0.5073 - val_loss: 0.1860 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5554 Epoch 00011: early stopping . We have got a good validation recall of 0.9026 . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 10) colors = plt.rcParams[&#39;axes.prop_cycle&#39;].by_key()[&#39;color&#39;] . plot_loss(weighted_history, &quot;Weighted&quot;, n=0) . Looks like there still could be some room for improvement . def plot_metrics(history): metrics = [&#39;loss&#39;, &#39;auc&#39;, &#39;precision&#39;, &#39;recall&#39;] for n, metric in enumerate(metrics): name = metric.replace(&quot;_&quot;,&quot; &quot;).capitalize() plt.subplot(2,2,n+1) plt.plot(history.epoch, history.history[metric], color=colors[0], label=&#39;Train&#39;) plt.plot(history.epoch, history.history[&#39;val_&#39;+metric], color=colors[0], linestyle=&quot;--&quot;, label=&#39;Val&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(name) if metric == &#39;loss&#39;: plt.ylim([0, plt.ylim()[1]]) elif metric == &#39;auc&#39;: plt.ylim([0.8,1]) else: plt.ylim([0,1]) plt.legend() . plot_metrics(weighted_history) . Predict with test data . test_data = pd.read_csv(DATA_PATH/&#39;Test.csv&#39;, index_col=0) test_data.drop(&#39;DATE&#39;, axis=&#39;columns&#39;, inplace=True) . proc_test = preproc_pipe.transform(np.array(test_data)) . preds = np.argmax(weighted_model.predict(proc_test), axis=-1) . sub_df = pd.DataFrame( {&#39;INCIDENT_ID&#39;:test_data.index, &#39;MULTIPLE_OFFENSE&#39;:preds}, ) sub_df.head() . INCIDENT_ID MULTIPLE_OFFENSE . 0 CR_195453 | 0 | . 1 CR_103520 | 0 | . 2 CR_196089 | 0 | . 3 CR_112195 | 0 | . 4 CR_149832 | 0 | . sub_df.to_csv(&#39;submission_df.csv&#39;, index=False) . sub_df[&#39;MULTIPLE_OFFENSE&#39;].value_counts() . 0 15903 Name: MULTIPLE_OFFENSE, dtype: int64 . I tried with various batch sizes and epochs to try to get it predict the classes differently maybe I should also try to change the model structure to achieve a different result because this submission only scored 50 recall on the competition test set. . I will also post other notebooks very soon, which will use machine learning based decision tree and random forest methods which performed way better on this problem. .",
            "url": "https://mani2106.github.io/Blog-Posts/tabular/tensorflow/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "relUrl": "/tabular/tensorflow/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "date": " • Jun 24, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Mother's day Sentiment analysis - with spaCy",
            "content": "Setup paths . We will use the method from my previous post to clean the text. . from pathlib import Path import pandas as pd DATA_PATH = Path(&#39;dataset/&#39;) DRIVE_PATH = Path(r&quot;/content/drive/My Drive/Spacy/Pretrained&quot;) . train_data = pd.read_csv(DATA_PATH/&#39;train.csv&#39;, index_col=0) train_data.head() . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . Let&#39;s check average length of text before cleaning. . print(sum( train_data[&#39;original_text&#39;].apply(len).tolist() )/train_data.shape[0]) . . 227.42102009273572 . Clean links with regex . train_data[&#39;original_text&#39;].replace( # Regex is match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . Let&#39;s check the average length again. . The regex did it&#39;s job I suppose. . train_data.head() . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . In my previous exploratory post, I have seen the data and I think that the features other than the text may not be required, (ie) . lang | retweet_count | original_author | . Class distribution . 0 must mean Neutral | 1 means Positive | -1 means Negative | . train_data[&#39;sentiment_class&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2681b9b438&gt; . Let&#39;s see some sentences with negative examples, I am interested why they should be negative on a happy day(Mother&#39;s day) . list_of_neg_sents = train_data.loc[train_data[&#39;sentiment_class&#39;] == -1, &#39;original_text&#39;].tolist() . pprint(list_of_neg_sents[:5]) . . [&#39;Happy mothers day To all This doing a mothers days work. Today been quiet &#39; &#39;but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few &#39; &#39;more guitar chords, drunk some strawberry gin and tonic and watch Lee evens &#39; &#39;on DVD. My favourite place to visit. #isolate &#39;, &#39;Remembering the 3 most amazing ladies who made me who I am! My late &#39; &#39;grandmother iris, mum carol and great grandmother Ethel. Missed but never &#39; &#39;forgotten! Happy mothers day to all those great mums out there! Love sent to &#39; &#39;all xxxx &#39;, &#39;Happy Mothers Day to everyone tuning in. This is the 4th Round game between &#39; &#39;me and @CastigersJ Live coverage on @Twitter , maybe one day @SkySportsRL or &#39; &#39;on the OurLeague app&#39;, &#34;Happy Mothers Day ! We hope your mums aren&#39;t planning to do any work around &#34; &#39;the house today! Surely it can wait until next week? #plumbers &#39; &#39;#heatingspecialists #mothersday #mothersday &#39;, &#34;Happy mothers day to all those mums whos children can&#39;t be with them today. &#34; &#39;My son Dylan lives in heaven I wish I could see him for one more hug. I wish &#39; &#39;I could tell him how much I love and miss him. Huge happy mothers day to &#39; &#39;your mum too.&#39;] . Well some tweets actually express their feelings for their deceased mothers. This is understandable. . We can use traditional NLP methods or deep learning methods to model the text. We will try the deep learning in this notebook . . Deep Learning approach with Spacy . It&#39;s recommended here that to improve performance of the classifier, Language model pretraining is one way to do so. . Spacy requires a .jsonl format of input to train text . Get texts from the dataframe and store in jsonl format more about that here. . We can also load the test data to get some more sample for the pretraining, this will not cause Data Leakage because we are not giving any labels to the model. . test_data = pd.read_csv(DATA_PATH/&#39;test.csv&#39;, index_col=0) test_data.head() . original_text lang retweet_count original_author . id . 1.246628e+18 3. Yeah, I once cooked potatoes when I was 3 y... | en | 0 | LToddWood | . 1.245898e+18 Happy Mother&#39;s Day to all the mums, step-mums,... | en | 0 | iiarushii | . 1.244717e+18 I love the people from the UK, however, when I... | en | 0 | andreaanderegg | . 1.245730e+18 Happy 81st Birthday Happy Mother’s Day to my m... | en | 1 | TheBookTweeters | . 1.244636e+18 Happy Mothers day to all those wonderful mothe... | en | 0 | andreaanderegg | . Let&#39;s clean the test set for links as well . test_data[&#39;original_text&#39;].replace( # Regex pattern to match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . test_data.shape . (1387, 4) . texts_series = pd.concat([train_data[&#39;original_text&#39;], test_data[&#39;original_text&#39;]], axis=&#39;rows&#39;) . Let&#39;s check the length . texts_series.shape[0], train_data.shape[0]+test_data.shape[0] . (4622, 4622) . So now we can use this texts_series to create the jsonl file. . list_of_texts = [ # Form dictionary with &#39;text&#39; key {&#39;text&#39;: value} for _, value in texts_series.items() ] . I will use srsly to write this list of dictionaries to a jsonl file . import srsly # saving to my Google drive srsly.write_jsonl(DRIVE_PATH/&#39;pretrain_texts.jsonl&#39;, list_of_texts) . We can see a few lines from the saved file. . from pprint import pprint with Path(DRIVE_PATH/&#39;pretrain_texts.jsonl&#39;).open() as f: lines = [next(f) for x in range(5)] pprint(lines) . . [&#39;{&#34;text&#34;:&#34;Happy #MothersDay to all you amazing mothers out there! I know &#39; &#34;it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to &#34; &#39;do what we can to protect the most vulnerable members of our society. &#39; &#39;#BeatCoronaVirus &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy Mothers Day Mum - I &#39;m sorry I can &#39;t be there to bring you &#39; &#34;Mothers day flowers &amp; a cwtch - honestly at this point I&#39;d walk on hot coals &#34; &#34;to be able to. But I&#39;ll be there with bells on as soon as I can be. Love you &#34; &#39;lots xxx (p.s we need more photos!) &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy mothers day To all This doing a mothers days work. Today been &#39; &#39;quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, &#39; &#39;learn few more guitar chords, drunk some strawberry gin and tonic and watch &#39; &#39;Lee evens on DVD. My favourite place to visit. #isolate &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy mothers day to this beautiful woman...royalty soothes you &#39; &#39;mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Remembering the 3 most amazing ladies who made me who I am! My late &#39; &#39;grandmother iris, mum carol and great grandmother Ethel. Missed but never &#39; &#39;forgotten! Happy mothers day to all those great mums out there! Love sent to &#39; &#39;all xxxx &#34;} n&#39;] . Start Pretraining . We should download a pretrained to model to use, Here I am using _en_core_webmd from Spacy. . This can be confusing (ie) Why should I train a pretrained model, if I can download one, The idea is that the downloaded pretrained model would have been trained with a very different type of dataset, but it already has some knowledge on interpreting words in English sentences. . But here we have dataset of tweets which the downloaded pretrained model may or may not have seen during it&#39;s training, So we use our dataset to fine-tune the downloaded model, so that with minimum training it can start understanding the tweets right away. . !python -m spacy download en_core_web_md . . Training results . %%bash # Command to pretrain a language model # Path to jsonl file with data # Using md model as the base # saving the model on my Drive folder # training for 50 iterations with seed set to 0 python -m spacy pretrain /content/drive/My Drive/Spacy/Pretrained/pretrain_texts.jsonl /usr/local/lib/python3.6/dist-packages/en_core_web_md/en_core_web_md-2.2.5 /content/drive/My Drive/Spacy/Pretrained/ -i 50 -s 0 . . ℹ Not using GPU ⚠ Output directory is not empty It is better to use an empty directory or refer to a new output path, then the new directory will be created for you. ✔ Saved settings to config.json ✔ Loaded input texts ✔ Loaded model &#39;/usr/local/lib/python3.6/dist-packages/en_core_web_md/en_core_web_md-2.2.5&#39; ============== Pre-training tok2vec layer - starting at epoch 0 ============== # # Words Total Loss Loss w/s 0 115177 114619.719 114619 7308 0 177090 174673.695 60053 8123 1 291933 282095.656 107421 8353 1 354180 337893.113 55797 8156 2 468951 432705.457 94812 8398 2 531270 479373.527 46668 8271 3 646206 557380.137 78006 8368 3 708360 595962.348 38582 8151 4 823108 662773.332 66810 8349 4 885450 696823.672 34050 8125 5 1000591 756743.684 59920 8254 5 1062540 787816.266 31072 7943 6 1177552 844198.828 56382 8380 6 1239630 874128.219 29929 7996 7 1354814 928725.262 54597 8291 7 1416720 957604.215 28878 8081 8 1531685 1010607.96 53003 8310 8 1593810 1039022.08 28414 8006 9 1708981 1091185.60 52163 8248 9 1770900 1118857.03 27671 8032 10 1885776 1169906.66 51049 8240 10 1947990 1197361.49 27454 8015 11 2062486 1247384.96 50023 8344 11 2125080 1274605.26 27220 8153 12 2239188 1323843.16 49237 8376 12 2302170 1350702.81 26859 7941 13 2416942 1399298.78 48595 8213 13 2479260 1425267.50 25968 7968 14 2594827 1473118.10 47850 8357 14 2656350 1498307.73 25189 7879 15 2770909 1544661.96 46354 7572 15 2833440 1569846.84 25184 7980 16 2948496 1615382.64 45535 8137 16 3010530 1639715.23 24332 7848 17 3125700 1684541.43 44826 8316 17 3187620 1708564.64 24023 7941 18 3302357 1753053.58 44488 8039 18 3364710 1776999.23 23945 7234 19 3480485 1821211.88 44212 8191 19 3541800 1844436.19 23224 7859 20 3657480 1887923.85 43487 8170 20 3718890 1911031.40 23107 7902 21 3833871 1954109.82 43078 8253 21 3895980 1977017.92 22908 7840 22 4011686 2019201.90 42183 8096 22 4073070 2041539.05 22337 7906 23 4188348 2083159.02 41619 8148 23 4250160 2105332.99 22173 7882 24 4364945 2146261.39 40928 8127 24 4427250 2168323.78 22062 8017 25 4542920 2209202.75 40878 8059 25 4604340 2230725.28 21522 7823 26 4719450 2271131.96 40406 8272 26 4781430 2292648.11 21516 7986 27 4896609 2332466.73 39818 8243 27 4958520 2353768.18 21301 8030 28 5073022 2392817.96 39049 8217 28 5135610 2414267.76 21449 8032 29 5250013 2452981.53 38713 8250 29 5312700 2474212.04 21230 8098 30 5427418 2512822.70 38610 8251 30 5489790 2533517.18 20694 8083 31 5604738 2572000.05 38482 8291 31 5666880 2592500.56 20500 8071 32 5781993 2630529.82 38029 8278 32 5843970 2651013.32 20483 8080 33 5959305 2689165.69 38152 8269 33 6021060 2709293.04 20127 8086 34 6136501 2746749.93 37456 8278 34 6198150 2766661.32 19911 8105 35 6313772 2803964.79 37303 8262 35 6375240 2823781.55 19816 8095 36 6490354 2860641.99 36860 8235 36 6552330 2880476.80 19834 8056 37 6667255 2917062.19 36585 8231 37 6729420 2936732.01 19669 8108 38 6844248 2972992.59 36260 8276 38 6906510 2992718.12 19725 8074 39 7021001 3028854.93 36136 8275 39 7083600 3048416.73 19561 8092 40 7199191 3084584.77 36168 8304 40 7260690 3103795.33 19210 8085 41 7375380 3139675.02 35879 8276 41 7437780 3158848.65 19173 8136 42 7552544 3194214.46 35365 8276 42 7614870 3213532.30 19317 8072 43 7730130 3248964.60 35432 7724 43 7791960 3267930.77 18966 8096 44 7906035 3302653.15 34722 8263 44 7969050 3321900.34 19247 8103 45 8083488 3356590.95 34690 8251 45 8146140 3375484.65 18893 8084 46 8261612 3410429.05 34944 8228 46 8323230 3429043.30 18614 8121 47 8437925 3463440.15 34396 8268 47 8500320 3482173.21 18733 7931 48 8615136 3516342.36 34169 8294 48 8677410 3534982.44 18640 8098 49 8791889 3568641.36 33658 8265 49 8854500 3587280.92 18639 8102 ⚠ Skipped 250 empty values ✔ Successfully finished pretrain . I have chosen to use the default parameters however one might need to change them for their problem. . We can see from the logs that the loss value in the last iteration is 18639, but since the batch_size was 3000 our data must have splitted to 2 batches, (number of texts are 4622) we should also take the previous log entry to account which is loss of 33658, So the average of them would be 26148.5, This number might be intimidating but the only way to check if it actually helps is to try to train a model with it. . If it doesn&#39;t then we can resume the training from the model saved on the last epoch. . We keep only the last model from the pretraining. . Let&#39;s train a text classifier with Spacy . Text classifier with Spacy . Now that we have a pretrained model, We now need to prepare data for training the text classifier. Let&#39;s have a look at the data format that Spacy expects the data to be in. . Data Generation . { &quot;entities&quot;: [(0, 4, &quot;ORG&quot;)], &quot;heads&quot;: [1, 1, 1, 5, 5, 2, 7, 5], &quot;deps&quot;: [&quot;nsubj&quot;, &quot;ROOT&quot;, &quot;prt&quot;, &quot;quantmod&quot;, &quot;compound&quot;, &quot;pobj&quot;, &quot;det&quot;, &quot;npadvmod&quot;], &quot;tags&quot;: [&quot;PROPN&quot;, &quot;VERB&quot;, &quot;ADP&quot;, &quot;SYM&quot;, &quot;NUM&quot;, &quot;NUM&quot;, &quot;DET&quot;, &quot;NOUN&quot;], &quot;cats&quot;: {&quot;BUSINESS&quot;: 1.0}, } . This format works for training via code, as given in the examples above, There is also another format mentioned here . cats is the only part we need to worry about, this must be where they look for categories/classes. . We have three classes in our dataset . 0 for Neutral | 1 for Positive | -1 for Negative | . and they are mutually-exclusive (There can be only one label for a sentence) . We also need to split the training data we have to training and evaluation sets so that we can see how well our model has learnt the problem. . Let&#39;s try to programmatically generate the training data from pandas dataframe . label_map = {1:&#39;POSITIVE&#39;, -1:&#39;NEGATIVE&#39;, 0:&#39;NEUTRAL&#39;} . We need list of tuples of text and the annotation details in a dictionary as mentioned above. . train_json = [ # Get the text from dataframe row (tweet.original_text, {&#39;cats&#39;:{ label_map[tweet.sentiment_class]:1.0 } }) for tweet in train_data[[&#39;original_text&#39;, &#39;sentiment_class&#39;]].itertuples(index=False, name=&#39;Tweet&#39;) ] . train_json[0] . (&#34;Happy #MothersDay to all you amazing mothers out there! I know it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus &#34;, {&#39;cats&#39;: {&#39;NEUTRAL&#39;: 1.0}}) . Now we will split the training data . from sklearn.model_selection import train_test_split # Stratified split with labels train_split, eval_split = train_test_split(train_json, test_size=0.2, stratify=train_data[&#39;sentiment_class&#39;]) . len(train_split), len(eval_split) . (2588, 647) . We should save them as json files to give them as input to the command line train utility in spacy. . import json with Path(DRIVE_PATH/&#39;train_clas.json&#39;).open(&#39;w&#39;) as f: json.dump(train_split, f) with Path(DRIVE_PATH/&#39;eval_clas.json&#39;).open(&#39;w&#39;) as f: json.dump(eval_split, f) . Validate data input for spacy . Now should if we have enough data to train the model with train spacy command in CLI, for that I will use Spacy&#39;s debug-data command in CLI. . !python -m spacy debug-data -h . usage: spacy debug-data [-h] [-tm None] [-b None] [-p tagger,parser,ner] [-IW] [-V] [-NF] lang train_path dev_path Analyze, debug and validate your training and development data, get useful stats, and find problems like invalid entity annotations, cyclic dependencies, low data labels and more. positional arguments: lang model language train_path location of JSON-formatted training data dev_path location of JSON-formatted development data optional arguments: -h, --help show this help message and exit -tm None, --tag-map-path None Location of JSON-formatted tag map -b None, --base-model None name of model to update (optional) -p tagger,parser,ner, --pipeline tagger,parser,ner Comma-separated names of pipeline components to train -IW, --ignore-warnings Ignore warnings, only show stats and errors -V, --verbose Print additional information and explanations -NF, --no-format Don&#39;t pretty-print the results . %%bash (python -m spacy debug-data en /content/drive/My Drive/Spacy/Pretrained/train_clas.json /content/drive/My Drive/Spacy/Pretrained/eval_clas.json -p &#39;textcat&#39; ) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 0 training docs 0 evaluation docs ✘ No evaluation docs ✔ No overlap between training and evaluation data ✘ Low number of examples to train from a blank model (0) ============================== Vocab &amp; Vectors ============================== ℹ 0 total words in the data (0 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 0 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 2 checks passed ✘ 2 errors . Data Generation (again) . There must be something I missed now, I asked a question on stackoverflow regarding this, turns out we need to get .jsonl format(again) and use the script provided in the repo to convert to the required json format for training, now I need to change the data generation a little bit to do that. . train_jsonl = [ # Get the text from dataframe row {&#39;text&#39;: tweet.original_text, &#39;cats&#39;: {v: 1.0 if tweet.sentiment_class == k else 0.0 for k, v in label_map.items()}, &#39;meta&#39;:{&quot;id&quot;: str(tweet.Index)} } for tweet in train_data[[&#39;original_text&#39;, &#39;sentiment_class&#39;]].itertuples(index=True, name=&#39;Tweet&#39;) ] . train_jsonl[0] . {&#39;cats&#39;: {&#39;NEGATIVE&#39;: 0.0, &#39;NEUTRAL&#39;: 1.0, &#39;POSITIVE&#39;: 0.0}, &#39;meta&#39;: {&#39;id&#39;: &#39;1.24502457848689e+18&#39;}, &#39;text&#39;: &#34;Happy #MothersDay to all you amazing mothers out there! I know it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus &#34;} . So instead of a list of tuples now I have a list of dictionaries. We need to split again to have an evaluation set . train_split, eval_split = train_test_split(train_jsonl, test_size=0.2, stratify=train_data[&#39;sentiment_class&#39;]) . len(train_split), len(eval_split) . (2588, 647) . We still need to convert the jsonl to the required json format, now for that I will use the script named textcatjsonl_to_trainjson.py in this repo. Let&#39;s download the script from the repo. . !wget -O script.py https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py . --2020-05-30 07:44:50-- https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1542 (1.5K) [text/plain] Saving to: ‘script.py’ script.py 100%[===================&gt;] 1.51K --.-KB/s in 0s 2020-05-30 07:44:51 (17.2 MB/s) - ‘script.py’ saved [1542/1542] . %%bash python script.py -m en /content/drive/My Drive/Spacy/train_texts.jsonl /content/drive/My Drive/Spacy python script.py -m en /content/drive/My Drive/Spacy/eval_texts.jsonl /content/drive/My Drive/Spacy . Let&#39;s try to debug again . Validate (again) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 2584 training docs 647 evaluation docs ⚠ 5 training examples also in evaluation data ============================== Vocab &amp; Vectors ============================== ℹ 98859 total words in the data (10688 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 3 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 1 check passed ⚠ 1 warning . It worked !, Thanks to the answerer of this question, now we know that our data format is correct. Turns out there is another command to convert our files to spacy&#39;s JSON format which is mentioned here. . The output is pointing out that the evaluation set has some data leakage. I will try to remove that now. . new_eval = [annot for annot in eval_split if all([annot[&#39;text&#39;] != t[&#39;text&#39;] for t in train_split])] . len(new_eval), len(eval_split) . (641, 647) . We thought there were 5 samples leaking into the training data, it is six here, anyway let&#39;s try to validate the data again. . %%bash (python -m spacy debug-data en /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/eval_texts.json -p &#39;textcat&#39; ) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 2584 training docs 641 evaluation docs ✔ No overlap between training and evaluation data ============================== Vocab &amp; Vectors ============================== ℹ 98859 total words in the data (10688 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 3 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 2 checks passed . We are all set to start training now! . Classifier Training . I have made the command to train in CLI, Please refer the comments for details in the order of the arguments given here . %%bash ## Arguement info # Language of text in which the Model is going to be trained # Path to store model # Training data json path # Evaluation data json path # Pipeline components that we are going to train # Number of iterations in total # Nummber of iterations to wait before improvement in eval accuracy # Pretrained model to start with # version # Augmentation for data(2 params) # Model Architecture for text classifier (cnn + bow) (python -m spacy train en -b en_core_web_sm /content/drive/My Drive/Spacy/Classifier /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/train_texts.json -p &quot;textcat&quot; -n 100 -ne 10 -t2v /content/drive/My Drive/Spacy/Pretrained/fifty_iter/model49.bin -V 0.1 -nl 0.1 -ovl 0.1) . Training pipeline: [&#39;textcat&#39;] Starting with base model &#39;en_core_web_sm&#39; Adding component to base model &#39;textcat&#39; Counting training words (limit=0) Loaded pretrained tok2vec for: [] Textcat evaluation score: F1-score macro-averaged across the labels &#39;POSITIVE, NEGATIVE, NEUTRAL&#39; Itn Textcat Loss Textcat Token % CPU WPS - - - 1 26.738 39.853 100.000 177034 2 5.179 65.120 100.000 157933 3 1.483 76.615 100.000 178008 4 0.686 83.266 100.000 177567 5 0.288 86.236 100.000 169033 6 0.151 88.381 100.000 176679 7 0.090 90.099 100.000 166485 8 0.057 91.000 100.000 171279 9 0.135 92.472 100.000 175907 10 0.028 93.237 100.000 171838 11 0.023 94.147 100.000 175174 12 0.022 94.729 100.000 155840 13 0.021 95.248 100.000 161975 14 0.021 95.485 100.000 168029 15 0.019 95.980 100.000 161440 16 0.018 96.226 100.000 167550 17 0.019 96.713 100.000 172607 18 0.017 96.849 100.000 169682 19 0.017 97.026 100.000 167330 20 0.015 97.299 100.000 173145 21 0.016 97.405 100.000 173020 22 0.015 97.526 100.000 165310 23 0.014 97.704 100.000 165994 24 0.013 97.865 100.000 176089 25 0.013 98.106 100.000 172153 26 0.013 98.201 100.000 172878 27 0.013 98.241 100.000 175909 28 0.012 98.320 100.000 170099 29 0.013 98.400 100.000 175274 30 0.012 98.481 100.000 170135 31 0.012 98.521 100.000 164726 32 0.011 98.536 100.000 171204 33 0.011 98.536 100.000 163467 34 0.011 98.576 100.000 150728 35 0.011 98.696 100.000 172780 36 0.010 98.735 100.000 163459 37 0.010 98.695 100.000 162075 38 0.010 98.750 100.000 165827 39 0.010 98.790 100.000 165852 40 0.010 98.830 100.000 174490 41 0.009 98.870 100.000 165485 42 0.010 98.990 100.000 164896 43 0.009 99.045 100.000 172563 44 0.008 99.045 100.000 169908 45 0.009 99.005 100.000 152600 46 0.008 99.084 100.000 166329 47 0.009 99.084 100.000 173841 48 0.008 99.164 100.000 163433 49 0.008 99.203 100.000 162648 50 0.008 99.258 100.000 177108 51 0.009 99.298 100.000 173468 52 0.008 99.298 100.000 169904 53 0.008 99.298 100.000 171979 54 0.008 99.298 100.000 166437 55 0.008 99.298 100.000 170520 56 0.007 99.337 100.000 172712 57 0.007 99.337 100.000 174966 58 0.007 99.392 100.000 173173 59 0.008 99.392 100.000 173910 60 0.007 99.392 100.000 169447 61 0.007 99.431 100.000 161931 62 0.007 99.471 100.000 106123 63 0.007 99.471 100.000 177625 64 0.007 99.511 100.000 172946 65 0.007 99.511 100.000 173579 66 0.007 99.511 100.000 172204 67 0.007 99.550 100.000 172994 68 0.006 99.550 100.000 174403 69 0.007 99.590 100.000 173900 70 0.006 99.630 100.000 169824 71 0.007 99.630 100.000 171172 72 0.006 99.669 100.000 172633 73 0.006 99.669 100.000 159052 74 0.007 99.669 100.000 174377 75 0.007 99.669 100.000 163376 76 0.006 99.669 100.000 174366 77 0.007 99.669 100.000 175517 78 0.007 99.669 100.000 175583 79 0.006 99.669 100.000 174024 80 0.006 99.669 100.000 174381 81 0.006 99.669 100.000 177120 82 0.006 99.708 100.000 175032 83 0.006 99.708 100.000 173298 84 0.007 99.708 100.000 171622 85 0.006 99.709 100.000 163705 86 0.006 99.709 100.000 175330 87 0.006 99.709 100.000 178355 88 0.006 99.709 100.000 170868 89 0.006 99.709 100.000 164401 90 0.005 99.709 100.000 173884 91 0.006 99.709 100.000 159754 92 0.006 99.709 100.000 177335 93 0.006 99.709 100.000 169868 94 0.006 99.709 100.000 168164 95 0.005 99.709 100.000 151894 96 0.006 99.709 100.000 171580 97 0.005 99.709 100.000 169471 98 0.005 99.724 100.000 156458 99 0.005 99.724 100.000 168167 100 0.006 99.724 100.000 172201 ✔ Saved model to output directory /content/drive/My Drive/Spacy/Classifier/model-final ✔ Created best model /content/drive/My Drive/Spacy/Classifier/model-best . . I also tried to train without the pretrained model (ie)en_core_web_sm, The logs for that are here below. (Uncollapse to view), the results are not very different, the evaluation metrics are off the roof. We need to predict the test data and try to submit to the competition for a better picture of the model. . %%bash ## Arguement info # Language of text in which the Model is going to be trained # Path to store model # Training data json path # Evaluation data json path # Pipeline components that we are going to train # Number of iterations in total # Nummber of iterations to wait before improvement in eval accuracy # Pretrained model to start with # version # Augmentation for data(2 params) # Model Architecture for text classifier (cnn + bow) (python -m spacy train en /content/drive/My Drive/Spacy/Classifier_without_using_websm /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/train_texts.json -p &quot;textcat&quot; -n 100 -ne 10 -t2v /content/drive/My Drive/Spacy/Pretrained/fifty_iter/model49.bin -V 0.1 -nl 0.1 -ovl 0.1) . . ✔ Created output directory: /content/drive/My Drive/Spacy/Classifier_without_using_websm Training pipeline: [&#39;textcat&#39;] Starting with blank model &#39;en&#39; Counting training words (limit=0) Loaded pretrained tok2vec for: [] Textcat evaluation score: F1-score macro-averaged across the labels &#39;POSITIVE, NEGATIVE, NEUTRAL&#39; Itn Textcat Loss Textcat Token % CPU WPS - - - 1 26.755 40.980 100.000 166278 2 5.293 65.846 100.000 172083 3 1.506 76.992 100.000 175595 4 0.695 83.314 100.000 173543 5 0.293 86.284 100.000 172609 6 0.156 88.784 100.000 171486 7 0.091 90.136 100.000 161118 8 0.056 91.761 100.000 156752 9 0.112 92.442 100.000 167948 10 0.028 93.329 100.000 162446 11 0.024 94.144 100.000 165753 12 0.022 95.206 100.000 168336 13 0.021 95.769 100.000 161408 14 0.020 96.150 100.000 162562 15 0.019 96.474 100.000 163309 16 0.018 96.775 100.000 168399 17 0.018 97.140 100.000 169412 18 0.017 97.357 100.000 171364 19 0.017 97.503 100.000 172552 20 0.016 97.584 100.000 167923 21 0.016 97.678 100.000 168228 22 0.015 97.934 100.000 158830 23 0.014 98.055 100.000 170587 24 0.013 98.216 100.000 161772 25 0.014 98.256 100.000 160948 26 0.013 98.296 100.000 163401 27 0.013 98.391 100.000 168392 28 0.012 98.351 100.000 162147 29 0.012 98.391 100.000 171460 30 0.012 98.511 100.000 171279 31 0.012 98.511 100.000 161304 32 0.011 98.511 100.000 171576 33 0.011 98.511 100.000 171248 34 0.011 98.591 100.000 166902 35 0.010 98.710 100.000 164750 36 0.011 98.830 100.000 164097 37 0.011 98.790 100.000 170317 38 0.010 98.790 100.000 163521 39 0.010 98.830 100.000 162378 40 0.009 98.964 100.000 164281 41 0.009 98.964 100.000 173645 42 0.011 99.004 100.000 165681 43 0.009 99.044 100.000 165916 44 0.009 99.044 100.000 168503 45 0.008 99.044 100.000 166608 46 0.008 99.123 100.000 170394 47 0.009 99.084 100.000 171932 48 0.008 99.124 100.000 172888 49 0.009 99.084 100.000 169469 50 0.009 99.084 100.000 167170 51 0.008 99.084 100.000 169762 52 0.008 99.124 100.000 166178 53 0.008 99.124 100.000 161415 54 0.008 99.164 100.000 164241 55 0.008 99.164 100.000 172629 56 0.008 99.164 100.000 164923 57 0.008 99.243 100.000 160153 58 0.007 99.243 100.000 171699 59 0.007 99.283 100.000 165604 60 0.008 99.323 100.000 161672 61 0.007 99.362 100.000 157016 62 0.007 99.417 100.000 171005 63 0.007 99.417 100.000 168709 64 0.007 99.417 100.000 170886 65 0.007 99.417 100.000 164144 66 0.007 99.417 100.000 154789 67 0.007 99.417 100.000 162214 68 0.006 99.457 100.000 164467 69 0.006 99.457 100.000 169052 70 0.006 99.496 100.000 168125 71 0.007 99.496 100.000 164085 72 0.006 99.575 100.000 163078 73 0.006 99.575 100.000 162955 74 0.006 99.575 100.000 166206 75 0.007 99.575 100.000 164477 76 0.006 99.575 100.000 169814 77 0.006 99.575 100.000 162547 78 0.006 99.575 100.000 168980 79 0.007 99.575 100.000 172534 80 0.006 99.575 100.000 161797 81 0.007 99.575 100.000 162510 82 0.006 99.575 100.000 172787 83 0.005 99.535 100.000 159187 84 0.006 99.535 100.000 168200 85 0.005 99.614 100.000 167757 86 0.006 99.614 100.000 158842 87 0.006 99.654 100.000 166849 88 0.005 99.654 100.000 162507 89 0.006 99.654 100.000 167156 90 0.005 99.654 100.000 97872 91 0.006 99.654 100.000 162397 92 0.006 99.708 100.000 168693 93 0.005 99.708 100.000 167645 94 0.005 99.708 100.000 163485 95 0.006 99.708 100.000 171732 96 0.005 99.708 100.000 165686 97 0.005 99.708 100.000 167604 98 0.005 99.708 100.000 166435 99 0.005 99.708 100.000 161645 100 0.005 99.708 100.000 171467 ✔ Saved model to output directory /content/drive/My Drive/Spacy/Classifier_without_using_websm/model-final ✔ Created best model /content/drive/My Drive/Spacy/Classifier_without_using_websm/model-best . . Prediction on test data . test_data = pd.read_csv(DATA_PATH/&#39;test.csv&#39;, index_col=0) test_data.head() . original_text lang retweet_count original_author . id . 1.246628e+18 3. Yeah, I once cooked potatoes when I was 3 y... | en | 0 | LToddWood | . 1.245898e+18 Happy Mother&#39;s Day to all the mums, step-mums,... | en | 0 | iiarushii | . 1.244717e+18 I love the people from the UK, however, when I... | en | 0 | andreaanderegg | . 1.245730e+18 Happy 81st Birthday Happy Mother’s Day to my m... | en | 1 | TheBookTweeters | . 1.244636e+18 Happy Mothers day to all those wonderful mothe... | en | 0 | andreaanderegg | . Clean test data . We will clean the test data of links with regex as well. . test_data[&#39;original_text&#39;].replace( # Regex pattern to match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . test_data.shape . (1387, 4) . list_of_test_texts = test_data[&#39;original_text&#39;].tolist() . Let&#39;s load the Spacy model from our training . import spacy textcat_mod = spacy.load(DRIVE_PATH.parent/&#39;Classifier/model-best&#39;) . I will try to fasten the prediction by using multithreading as mentioned here . d = textcat_mod(list_of_test_texts[0]) d.cats . {&#39;NEGATIVE&#39;: 0.020245620980858803, &#39;NEUTRAL&#39;: 0.9727445840835571, &#39;POSITIVE&#39;: 0.007009787950664759} . max(d.cats, key=lambda x: d.cats[x]) . &#39;NEUTRAL&#39; . label_map = {&#39;POSITIVE&#39;:1, &#39;NEGATIVE&#39;:-1, &#39;NEUTRAL&#39;:0} . preds = [] for doc in textcat_mod.pipe(list_of_test_texts, n_threads=4, batch_size=100): pred_cls = max(doc.cats, key=lambda x: doc.cats[x]) preds.append(label_map[pred_cls]) . len(preds), len(list_of_test_texts) . (1387, 1387) . Let&#39;s form the submission . sub_df = pd.DataFrame( preds, index=test_data.index, columns=[&#39;sentiment_class&#39;] ) . sub_df.shape . (1387, 1) . sub_df.head() . sentiment_class . id . 1.246628e+18 0 | . 1.245898e+18 0 | . 1.244717e+18 -1 | . 1.245730e+18 -1 | . 1.244636e+18 0 | . sub_df.to_csv(DRIVE_PATH.parent/&#39;submission.csv&#39;) . The submitted predictions scored a mere 39/100 in weighted f1-score, that&#39;s disappointing. -_- . Let&#39;s analyze the predictions . Prediction distribution . sub_df[&#39;sentiment_class&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fed20ed1b70&gt; . sub_df[&#39;sentiment_class&#39;].value_counts() . 0 847 1 277 -1 263 Name: sentiment_class, dtype: int64 . This looks very similar to the train data . train_data[&#39;sentiment_class&#39;].value_counts() . 0 1701 -1 769 1 765 Name: sentiment_class, dtype: int64 . What would have gone wrong?, I guess what I can do is try another method(traditional). Coming up in another post .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/eda/sentiment/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "relUrl": "/nlp/eda/sentiment/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Mother's day Sentiment analysis",
            "content": "This notebook explores the data from HackerEarth Machine learning challenge for Mother&#39;s day, The following is the Problem description. . You work in an event management company. On Mother&#39;s Day, your company has organized an event where they want to cast positive Mother&#39;s Day related tweets in a presentation. Data engineers have already collected the data related to Mother&#39;s Day that must be categorized into positive, negative, and neutral tweets. You are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral. . Download the data . import requests zip_file = requests.get(&#39;https://he-s3.s3.amazonaws.com/media/hackathon/hackerearth-test-draft-1-102/predicting-tweet-sentiments-231101b4/fa62f5d69a9f11ea.zip?Signature=v92IcNfljnopA9xQoCPCftwg1g0%3D&amp;Expires=1590318817&amp;AWSAccessKeyId=AKIA6I2ISGOYH7WWS3G5&#39;) with open(&#39;data.zip&#39;, &#39;wb&#39;) as f: f.write(zip_file.content) . !unzip data.zip . Archive: data.zip creating: dataset/ inflating: dataset/train.csv inflating: dataset/test.csv . %load_ext google.colab.data_table . Peek at the data . from pathlib import Path import pandas as pd DATA_PATH = Path(&#39;dataset/&#39;) train_data = pd.read_csv(DATA_PATH/&#39;train.csv&#39;, index_col=0) train_data.head(100) . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . ... ... | ... | ... | ... | ... | . 1.244092e+18 Happy Mothers Day from all of us at Kellyzola ... | en | 1 | design_pia | 0 | . 1.246529e+18 Happy Mothers Day katiebrooks_88 . I do my lev... | en | 1 | missny99 | 0 | . 1.244747e+18 RESPECT, GRATITUDE and ADORATION to all MOTHER... | en | 0 | EgbertsTreasure | 0 | . 1.245141e+18 It takes someone really brave to be a mother, ... | en | 0 | momaferd | -1 | . 1.245903e+18 On Mother&#39;s Day, I&#39;m sharing this video again ... | en | 0 | GotMommyBrain | -1 | . 100 rows × 5 columns . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Float64Index: 3235 entries, 1.24502457848689e+18 to 1.24540908968687e+18 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 original_text 3235 non-null object 1 lang 3231 non-null object 2 retweet_count 3231 non-null object 3 original_author 3235 non-null object 4 sentiment_class 3235 non-null int64 dtypes: int64(1), object(4) memory usage: 151.6+ KB . So there are four columns . 1. The tweet text 2. Language of the tweet 3. Number of Retweets 4. Sentiment group (+ve, -ve, neu) . There are also some missing values in lang and retweet_count columns. . Let&#39;s look at the number of languages of tweets this dataset has . train_data[&#39;lang&#39;].value_counts() . en 2994 pink Peruvian opal! via 4 Find More 2 &amp;gt 2 WORLDS OKAYEST MOTHER! &amp;lt 2 ... 0.4754834129 1 -0.0064143617 1 -0.3850425633 1 0.7885519508 1 -0.2758448854 1 Name: lang, Length: 232, dtype: int64 . I see that most tweets are in English, but it seems that some entries in the data are not actually indicating any language. . print(&#39;Total Other language tweets: &#39;, train_data.shape[0].item()-2994) . Total Other language tweets: 241 . Let&#39;s see if they are actually in different languages. . train_data.loc[train_data[&#39;lang&#39;] != &#39;en&#39;, :] . original_text lang retweet_count original_author sentiment_class . id . 1.244590e+18 Happy mothersday to all those celebrating toda... | -0.0138325017 | en | 11 | 0 | . 1.244823e+18 Exactly what my late mum aka hype mama would d... | -0.9677309496 | en | 0 | 0 | . 1.246515e+18 It&#39;s the world&#39;s most difficult job No sick le... | -0.3876905537 | en | 1 | 0 | . 1.244226e+18 Happy Mother’s Day! To all the amazing Mums ou... | 0.5309553602 | en | 0 | 0 | . 1.244419e+18 Happy Mothers Day , Mummy! Nearly 90 and still... | -0.045423609 | en | 2 | 0 | . ... ... | ... | ... | ... | ... | . 1.246356e+18 Happy Mothers Day All My Nigerian Massive Fami... | 0.2117897904 | en | 0 | 0 | . 1.245821e+18 HAPPY MOTHERS DAY ! HAPPY MOTHERS DAY !!Now th... | -0.8739088126 | en | 0 | 0 | . 1.246719e+18 Isan Elba celebrates Happy Mothers’ Day with h... | 0.4945825935 | en | 0 | 0 | . 1.245565e+18 Still miss my mom she passed 18th of March 201... | 0.6927740873 | 0 | 0 | 0 | . 1.245871e+18 I’m so thankful for my 5, healthy happy joy br... | 0.2522315249 | en | 1 | 0 | . 241 rows × 5 columns . I think all the tweets are in English, you can see that the en value representing English is misplaced in other columns retweet_count and original_author. They are either filled with random float numbers or some other tweet text, maybe the data was not scraped properly? . prHowever from what I know, I think we can ignore the columns other than the original_text column which has the tweet text, which is the most important for analysis for sentiment of text. You can see that almost all of the texts have some link embedded to them, They are not likely to help in getting to know the sentiment of the tweeter. . The pattern here is that most of the links either are images which start with pic.twitter.* or links referring to other sites like http://www.instagram.*, We should be able to identify the pattern with a Regular expression. Let&#39;s try to test the assumption. . Cleaning links . import re sample_tweet = &quot;&quot;&quot; Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... https://www. facebook.com/36426361058377 0/posts/1130869587256498/ … #happymothersday2020 pic.twitter.com/ZCZOF1xb6K wo&quot;&quot;&quot; . print(re.sub(&#39;(https?: / /.*|pic.*)[ r n]*&#39;, &#39;&#39;, sample_tweet)) . Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... . This will remove most of the links, but it will also remove the text between the links like in the case above the #happymothersday2020 hashtag is removed. . Let&#39;s apply the regex to the texts . (train_data[&#39;original_text&#39;].replace({&#39;(https?: / /.*|pic.*)[ r n]*&#39;:&#39;&#39;}, regex=True).to_frame()).head() . original_text . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | . Visualize the words . Prepare the mask . import numpy as np from io import BytesIO from PIL import Image from PIL.ImageOps import invert img_file = requests.get(&#39;http://www.agirlandagluegun.com/wp-content/uploads/blogger/-ox_bazyTgmQ/TcNwpMfduLI/AAAAAAAAOX4/hcxXcz0A8-A/s1600/scan0001.jpg&#39;) img = BytesIO(img_file.content) img_mask = np.array(Image.open(img)) # Check if the image was downloaded properly img_mask.shape . (718, 1600, 3) . # From https://www.datacamp.com/community/tutorials/wordcloud-python def transform_format(val): # Just trying to invert pixels if any(v == 255 for v in val): return 0 else: return 255 # Transform the mask into a new one that will work with the function: transformed_mask = np.ndarray((img_mask.shape[0], img_mask.shape[1]), np.int32) for i in range(len(img_mask)): transformed_mask[i] = list(map(transform_format, img_mask[i])) . from wordcloud import WordCloud, STOPWORDS import matplotlib.pyplot as plt wc = WordCloud(background_color=&quot;white&quot;, max_words=2000, mask=transformed_mask, stopwords=set(STOPWORDS), contour_width=1, contour_color=&#39;steelblue&#39;) # generate word cloud wc.generate(&#39; n&#39;.join(train_data[&#39;original_text&#39;].values.tolist())) . &lt;wordcloud.wordcloud.WordCloud at 0x7f0482536358&gt; . %matplotlib inline plt.figure(figsize=(18,18)) plt.axis(&quot;off&quot;) plt.imshow(wc) . &lt;matplotlib.image.AxesImage at 0x7f04825f85f8&gt; . Now we can try to model this text with any method that we would like! and that is coming up next. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "relUrl": "/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Building தமிழ் language model",
            "content": "Introduction . In this post, I will try to model தமிழ் (Tamil), I have already prepared the data for the language model in the kaggle notebook here, A language model will be useful for many tasks such as text classification, information retrieval etc. . What is a language model? . From what I know, Language model is a machine&#39;s way of understanding a language, technically it is defined as a probability distribution over a sequence of words[^1], by helping the machine to understand language, we can use them text-based classifiers, chatbots and for other NLP tasks in that language. . How do we train a language model? . I recall the times when you was going for school, I was given language lessons for English and தமிழ், The languages were different in grammar, dialects, sounds etc., after some lessons about the words and letters present in the language, both of them were taught to all in the same manner. . We would have lessons in textbooks, of which most of them are stories, biographies and history. Most of the exercises at the end of each lesson are . Fill in the blanks like The ____ rises in the east. or சூரியன் உதிக்கும் திசை _____ | Write short descriptive answers for question based on the lesson. | Maybe longer essays on general What-if scenarios from the lesson. | . We know that to answer them, it required a decent understanding of the language&#39;s grammar, which in turn, is taught to the children by making them read and write the questions and answers. . Now how can we teach a machine to understand and learn the language?, We had textbooks to read and learn about the language, So the machine also needs data, like our textbooks (but not necessarily the same ones with which we learn) to learn the language. . So how can we ensure that the machine is learning properly?, We test it with Fill in the blanks kind of exercises and let it guess the next possible word for the sequence of words we give it. It will not be easy for the machine but a little bit of learning is enough to use the model for other purposes. . We cannot directly give the raw text data to the model, We need to convert them to sequence of words to help it learn the flow of the words . Things we need for a language model . A decent amount of raw text data, more about that here . | A language tokenizer, more about that here and here . | . This notebook is executed on kaggle, so the paths mentioned here will be needed to change if you run in your own environment. Setup libraries and paths . Set seed for reproducibility . Load text data from csv . LANG_DATA = pd.read_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, index_col=[0]) LANG_DATA.head() . We have the url, article_id and title as additional information about the text, Let&#39;s check the average length of the article text. . Exploration . The total articles we have are 131162 . Remove empty articles from the dataframe . LANG_DATA.dropna(axis=&#39;rows&#39;, inplace=True) LANG_DATA.info() . Length of articles . The average length of each article is 1370 words . We had one empty article, I suppose. . Prepare Text data for Language model . processor = SPProcessor(lang=&#39;ta&#39;, sp_model=DATA_PATH/&#39;tamil_tok.model&#39;, sp_vocab=DATA_PATH/&#39;tamil_tok.vocab&#39;) . Set batch size . bs = 16 . data_lm = (TextList.from_df(LANG_DATA, path=DATA_PATH, cols=&#39;text&#39;, processor=processor) # Split out some data in a random fashion for testing .split_by_rand_pct(0.1) # This is where we convert the raw text data to # sequences of words .label_for_lm() # We want to do a language model so we label accordingly .databunch(bs=bs)) . data_lm.sanity_check() . Let&#39;s save the language model data to skip the processing above next time. . data_lm.save(DATA_PATH/&#39;data_lm.pkl&#39;) . Let&#39;s have a look at the tokenized data from the sentencepiece tokenizer. . data_lm.show_batch() . bos means beginning of the sentence. | eos means end of the sentence. | xx maj used to indicate the next word begins with a capital in the original text. more about this can be found here and here | . Train the language model . Initialize model . config = awd_lstm_lm_config.copy() config[&#39;qrnn&#39;] = True config[&#39;n_hid&#39;] = 1550 config[&#39;n_layers&#39;] = 4 # This is a classification metric, # determines how well can the model # narrow the choice of words from it&#39;s # vocabulary for the next prediction. perplexity = Perplexity() learn = language_model_learner(data_lm, arch=AWD_LSTM, config=config, pretrained=False, metrics=[accuracy, perplexity], ).to_fp16() # gradient clipping learn.clip = 0.1 learn.model_dir=DATA_PATH . Find proper learning rate . learn.lr_find() . learn.recorder.plot(suggestion=True) . Get suggested learning rate . min_grad_lr = learn.recorder.min_grad_lr min_grad_lr . Start training . Stage - 1 . learn.fit_one_cycle(10, min_grad_lr, # Momentums, just a try! div_factor=10.0, pct_start=0.8, moms=(0.75,0.65), callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;perplexity&#39;, name=&#39;best_st1&#39;), CSVLogger(learn, filename=DATA_PATH/&#39;history&#39;, append=True)]) . Save the intermediate results . learn.load(&#39;best_st1&#39;); learn.save(&#39;ta-wiki-stage1&#39;) learn.save_encoder(&#39;ta-wiki-enc-stage1&#39;) . You can chop and change the parameters, to get a better model, find the latest run of the notebook on kaggle, please upvote there if you liked this. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Testing out தமிழ் language tokenizer",
            "content": "You can find the notebook in which I built the tokenizers here . Introduction . This notebook is intended to experiment with different tokenizers built previously, with varying vocab_size values like 8000, 16000, 20000, 30000, So how do we exactly test a tokenizer?, this brings us to why do we need a tokenizer in the first place. . In English language we can easily find meaningful units of a sentence, by whitespaces. But it is not that easy in other languages, Like in தமிழ், Consider the following sentence, . இந்த தொழிற்சாலை பணப்பற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது., this loosely translates to something like The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. . We need to focus on the compound word பணப்பற்றாக்குறை which means lack of cash, that compound word is actually a combination of two words பணம் and பற்றாக்குறை representing Cash and Deficiency/lack of A tokenizer for தமிழ், should split the words into two as mentioned above. . Since Tokenization is the process of demarcating and possibly classifying sections of a string of input characters[1](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) or in simple words identifying linguistically meaningful units [2](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language) for further processing. We need it for a language model which essentially built to understand the language that it is built for. . Since we built the tokenizer in an unsupervised[3](https://github.com/google/sentencepiece#sentencepiece) way, There are no numerical ways of gauging the efficiency of the tokenization (AFAIK), so we are left with to try and tokenize some random sentences with the model, and check the tokenizer ourselves. . Sentences for testing tokenization . I have chosen some sentences at random, Check the comments above each for their translation in English. . sentences = [ # Sita is a mischievous girl &#39;சீதா ஒரு குறும்பு பெண்&#39;, # I remember my childhood &#39;எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது&#39;, # India has successfully tested the Agni-5 missile for the fourth time from Abdul Kalam Island (Wheeler Island) in Odisha. &#39;இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.&#39;, # The European Union&#39;s Galileo satellite system is in operation. It is believed to be the world&#39;s most accurate high-precision positioning system. &#39;ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.&#39;, # The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. &#39;இந்த தொழிற்சாலை பணபற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.&#39;, # Citizens, witnesses and warriors mourn the death of their king. It is up to the department to regret any loss. &#39;தம் மன்னன் இறந்ததற்கு குடிமக்களும் சான்றோரும் வீரர்களும் வருந்திப் பாடுவது கையறுநிலை என்னும் துறையாகும். எந்த இழப்பையும் எண்ணி வருந்துவது கையறுநிலைத் துறைக்குரியது.&#39;, # The Poems from Sangam Tamil Literature portrays the trading feats of early Tamilian,Tamilians traded across seas and other countries &#39;சங்கத்தமிழ்க் கவிதைகள் பழந்தமிழர்தம் வணிகச்சிறப்பைப் பறைசாற்றி நிற்கின்றன. தமிழர் கடல்கடந்து, அயல்நாடுகளிலும் வணிகம் செய்தனர் என்ற செய்திகளைச் சங்கப்பாடல்கள்வழி அறிகின்றோம்.&#39;, # Everyone stood up to call for a national flag at a school event. &#39;பள்ளி நிகழ்ச்சி ஒன்றில் தேசியக் கொடி ஏற்றுமாறு அழைக்க அவரும் எழுந்தார் அனைவரும் எழுந்து நின்றனர்&#39;, ] . Let&#39;s try to tokenize each one of them. . Begin Experimentation . Initial setup . import sentencepiece as spm from pathlib import Path from IPython.core.display import display, HTML from string import Template sp = spm.SentencePieceProcessor() TOK_PATH = &#39;/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer&#39; MODEL_PATHS = [p for p in Path(TOK_PATH).glob(&#39;*.model&#39;)] . Sentence Tokenization . I will try to comment on the tokenization with a limited knowledge of தமிழ் grammar, I will refer to the model by the vocab size they are built with (ie) 8000, 16000, 20000, 30000 . First sentence சீதா ஒரு குறும்பு பெண் . tokenize_and_display_results(sentences[0]) . tok_30000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_20000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_8000_size | ▁சீதா | ▁ஒரு | ▁குறு | ம்பு | ▁பெண் | . tok_16000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . குறும்பு is actually not a compound word, so I think apart from the model with 8000 size all other models got it right. . Next sentence எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது . tokenize_and_display_results(sentences[1]) . tok_30000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_20000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_8000_size | ▁என | க்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவ | ம் | ▁நினைவ | ிருக்கிறது | . tok_16000_size | ▁எனக்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . In this sentence I remember my childhood the model with 20000 and 30000 got the tokenization right. 16000 was just close, because the letter ப் does not have a meaning on it&#39;s own. . Next sentence இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது. . tokenize_and_display_results(sentences[2]) . tok_30000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசாவில | ுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_20000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_8000_size | ▁இந்தியா | ▁அக் | னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | வ | ிலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோ | தி | த்தது | . | . tok_16000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . This is interesting, We can probably say that 8000 vocab model is not doing so well. Looking at the others, both the 16000 vocab model and 20000 split ஒடிசாவிலுள்ள meaning to refer something in Odisha into ஒடிசா and விலுள்ள, I think the right split is ஒடிசாவில் and உள்ள, which the 30000 got right. . Next Sentence . ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது. . tokenize_and_display_results(sentences[3]) . tok_30000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_20000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_8000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லி | யோ | ▁செயற்கை | கோள | ் | ▁அமைப்பு | ▁செயல்பாட்ட | ுக்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லிய | மான | ▁செய் | மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_16000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . All of them may have got it wrong with Galileo which is split into கலி(Gali) and லியோ(leo), (Maybe I am wrong on this part) and in GPS இடஞ்சுட்டலாக may be called as இடம்சுட்டல்(can be loosely translated to Location Pointer) should have been split like இடம் and சுட்டல் and ஆக, Maybe I am expecting too much. . Conclusion . I will stop here for this blogpost, You can try the other sentences if you want by forking the notebook here . Please comment if you think something is wrong somewhere, share it if you have found this interesting. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Wiki data extraction",
            "content": "This post explains how I downloaded and extracted wiki dump archive using wikiextractor. . This code was used on a kaggle environment, which can be found here. You can fork and change as per your needs. . Setup . Import required libraries . # For JSON data extraction import json # For path manipulations from pathlib import Path # For preprocessing import string # For deleting files and folders import shutil # To clone necessary files import git # To download the dump import requests as req # To use wikiextractor import subprocess # To clean and process data import pandas as pd . Setup output paths . DATA_PATH = Path(&#39;/kaggle/working/&#39;) EXTRACTED_PATH = DATA_PATH/&#39;extracted&#39; EXTRACTED_PATH.mkdir() . Data Extraction . Request file from wikipedia. . You can use a different link here. . bzip_file = req.get(&#39;https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2&#39;) . Save request content to a file . with open(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;, &#39;wb&#39;) as f: f.write(bzip_file.content) . Clone wiki extractor from github . Thanks to attardi and GitPython . git.Git(str(DATA_PATH)).clone(&quot;https://github.com/attardi/wikiextractor.git&quot;) . Use wikiextractor to get data from the dump . This runs the wikiextractor cloned from github. . run_stat = subprocess.run( [&#39;python&#39;, # File to run str(DATA_PATH/&#39;wikiextractor/WikiExtractor.py&#39;), # Processing parameters to get as json &#39;-s&#39;, &#39;--json&#39;, # Directory to store Extracted text &#39;-o&#39;, str(DATA_PATH/&#39;extracted&#39;), # Archive file to extract from str(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;)] ) . Get list of files extracted from the extraction folder . files_extracted = [str(f) for f in EXTRACTED_PATH.rglob(&quot;*/*&quot;)] . Load json data from the files, since all files are stored as json we can load them like below, This gives us a list of dictionaries . lang_text = [json.loads(line) for _file in files_extracted for line in open(_file)] . or this . lang_text = [] for _file in files_extracted: with open(_file, &#39;r&#39;) as f: file_lines = f.readlines() for line in file_lines: lang_text.append(json.loads(line)) . Preprocessing . You can use any of the following, or skip the preprocessing altogether if you wish so. . Filter English words from text . Check each word after removing their punctuations, if it is an english word . filter_english = lambda text: &#39; &#39;.join([ word for word in text.split() if word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)).isalpha() is False ]) . or . def filter_english(text): words = [] # Spltting words for word in text.split(): # Replace symbols trans_table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) word = word.translate(trans_table) if not word.isalpha(): words.append(word) return &#39; &#39;.join(words) . Form dataframe and apply preprocessing . # Since we have a list of dictionaries. lang_df = pd.DataFrame(lang_text) lang_df[&#39;text&#39;] = lang_df[&#39;text&#39;].apply(filter_english) . Store the output in compressed format . lang_df.to_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, header=True) . The above saved file can be loaded with pd.read_csv. . You can find the full code for this in Github gist or with the output in kaggle. . Clean up the downloaded files, (if required) . shutil.rmtree(str(EXTRACTED_PATH)) shutil.rmtree(str(DATA_PATH/&#39;wikiextractor&#39;)) Path(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;).unlink() .",
            "url": "https://mani2106.github.io/Blog-Posts/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "relUrl": "/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "Building தமிழ் language tokenizer",
            "content": "You can find the blog post regarding extraction here and kaggle notebook with output here . Import required libraries . from pathlib import Path import sentencepiece as spm import pandas as pd . Read data from csv . lang_data = pd.read_csv(&#39;../input/tamil-wiki-data-extraction/filtered_data.csv.tar.gz&#39;, index_col=[0]) lang_data.head() . id url title text . 0 48482 | https://ta.wikipedia.org/wiki?curid=48482 | தென் துருவம் | தென் துருவம் தென் முனை தென் துருவம் என்பது புவ... | . 1 48485 | https://ta.wikipedia.org/wiki?curid=48485 | ஆர்க்டிக் வட்டம் | ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து... | . 2 48486 | https://ta.wikipedia.org/wiki?curid=48486 | நாஞ்சில் நாடன் | நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்... | . 3 48492 | https://ta.wikipedia.org/wiki?curid=48492 | டிக்கோயா | டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின... | . 4 48493 | https://ta.wikipedia.org/wiki?curid=48493 | நள்ளிரவுச் சூரியன் | நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த... | . lang_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 133412 entries, 0 to 133411 Data columns (total 4 columns): id 133412 non-null int64 url 133412 non-null object title 133412 non-null object text 133412 non-null object dtypes: int64(1), object(3) memory usage: 5.1+ MB . Setup paths . OUTPUT_DIR = Path(&#39;/kaggle/working&#39;) TEXTS_DIR = OUTPUT_DIR/&#39;texts&#39; TOK_DIR = OUTPUT_DIR/&#39;tokenizer&#39; # Create directories TOK_DIR.mkdir() TEXTS_DIR.mkdir() . Prepare texts . We can pass a list of files as a comma seperated string according to documentation, So we can store each article in a text file and pass the names in a comma seperated string. . for t in lang_data.itertuples(): file_name = Path(TEXTS_DIR/f&#39;text_{t.Index}.txt&#39;) file_name.touch() with file_name.open(&#39;w&#39;) as f: f.write(t.text) . len([t for t in TEXTS_DIR.iterdir()]), lang_data.shape[0] . (133412, 133412) . All the files have been converted to texts . Train sentencepiece model . Let&#39;s make a comma seperated string of filenames . files = &#39;,&#39;.join([str(t) for t in TEXTS_DIR.iterdir()]) files[:100] . &#39;/kaggle/working/texts/text_40902.txt,/kaggle/working/texts/text_44212.txt,/kaggle/working/texts/text&#39; . We must find the right vocab_size for the tokenizer, that can be done only by testing the tokenizer after building onw . for v in 8000, 16000, 20000, 30000: api_str = f&quot;&quot;&quot;--input={files} --vocab_size={v} --model_type=unigram --character_coverage=0.9995 --model_prefix={str(TOK_DIR)}/tok_{v}_size --max_sentence_length=20000&quot;&quot;&quot; print(&quot;Training with vocab set as:&quot;, v) spm.SentencePieceTrainer.train(api_str) . Training with vocab set as: 8000 Training with vocab set as: 16000 Training with vocab set as: 20000 Training with vocab set as: 30000 . Cleanup . !rm -rf /kaggle/working/texts/ . Let&#39;s test the models in another notebook, you can find the outputs in this kaggle notebook .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "Building a Pokemon Classifier",
            "content": ". In this notebook, I used the pokemon images dataset from here but unfortuantely it is not available now. . Get data from Kaggle . !kaggle datasets download -d mrgravelord/complete-pokemon-image-dataset . Downloading complete-pokemon-image-dataset.zip to /content 90% 121M/134M [00:01&lt;00:00, 77.3MB/s] 100% 134M/134M [00:01&lt;00:00, 95.2MB/s] . !unzip /content/complete-pokemon-image-dataset.zip . !rm /content/complete-pokemon-image-dataset.zip . Load required libraries . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import ReduceLROnPlateauCallback, SaveModelCallback from fastai.callbacks import CSVLogger . Prepare Data for training . path = Path(&quot;.&quot;) . Form data bunch object from the folders. . data = ImageDataBunch.from_folder(path, train=&quot;.&quot;, ds_tfms=get_transforms(), size=128, bs=64, valid_pct=0.2).normalize(imagenet_stats) . Check the number of different pokemon images that we have. . len(data.classes) . 928 . Creating a CNN model from architecture of resnet18. I could use a bigger model but I would not be able to serve them from Google or OneDrive because of the size. . Error Rate is 1-accuracy. | Using mixup for better regularization. | Converting the operations to be performed in a lower precision, more | . learn = cnn_learner(data, models.resnet18, metrics=error_rate).mixup().to_fp16() . Adding callbacks to monitor the training process and . Reduce the learning_rate by using the ReduceLROnPlateauCallback. | Saving the model on every improvement in error_rate | Log the training stats in a csv file. | . callbacks_list = [ ReduceLROnPlateauCallback(learn=learn, monitor=&#39;error_rate&#39;, factor=1e-6, patience=5, min_delta=1e-5), SaveModelCallback(learn, mode=&quot;min&quot;, every=&#39;improvement&#39;, monitor=&#39;error_rate&#39;, name=&#39;best&#39;), CSVLogger(learn=learn, append=True) ] . Start Training . Now, All the setup has been made, Let&#39;s train the model with default parameters, for 15 epochs. . learn.fit_one_cycle(15, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 7.173859 | 6.631011 | 0.985798 | 01:09 | . 1 | 6.246106 | 5.397111 | 0.870562 | 01:08 | . 2 | 5.001963 | 3.665833 | 0.672144 | 01:07 | . 3 | 4.327330 | 2.772682 | 0.540881 | 01:06 | . 4 | 3.941842 | 2.320177 | 0.469669 | 01:06 | . 5 | 3.648211 | 2.069086 | 0.420978 | 01:06 | . 6 | 3.423512 | 1.901359 | 0.372895 | 01:06 | . 7 | 3.328791 | 1.758360 | 0.343883 | 01:06 | . 8 | 3.140401 | 1.657776 | 0.326841 | 01:06 | . 9 | 3.044241 | 1.591135 | 0.313857 | 01:07 | . 10 | 2.940413 | 1.538893 | 0.300670 | 01:06 | . 11 | 2.759924 | 1.502491 | 0.290931 | 01:07 | . 12 | 2.781063 | 1.474272 | 0.283628 | 01:06 | . 13 | 2.761597 | 1.457427 | 0.282816 | 01:06 | . 14 | 2.700450 | 1.459171 | 0.280179 | 01:07 | . Better model found at epoch 0 with error_rate value: 0.9857983589172363. Better model found at epoch 1 with error_rate value: 0.870561957359314. Better model found at epoch 2 with error_rate value: 0.6721444725990295. Better model found at epoch 3 with error_rate value: 0.5408805012702942. Better model found at epoch 4 with error_rate value: 0.46966931223869324. Better model found at epoch 5 with error_rate value: 0.4209778904914856. Epoch 6: reducing lr to 2.599579409433508e-09 Better model found at epoch 6 with error_rate value: 0.37289512157440186. Better model found at epoch 7 with error_rate value: 0.34388312697410583. Better model found at epoch 8 with error_rate value: 0.3268411457538605. Better model found at epoch 9 with error_rate value: 0.3138567805290222. Better model found at epoch 10 with error_rate value: 0.30066952109336853. Better model found at epoch 11 with error_rate value: 0.29093122482299805. Epoch 12: reducing lr to 2.606527959586539e-10 Better model found at epoch 12 with error_rate value: 0.2836275100708008. Better model found at epoch 13 with error_rate value: 0.28281599283218384. Better model found at epoch 14 with error_rate value: 0.2801785469055176. . Now that we have got some decent accuracy let us try to save the model and interpret from it. . In the following cell, I . Load the best weights saved by the callbacks during training. | Convert the model back to use 32 bit precision. | Export the model as a whole. | Export the weights alone. | . learn.load(&quot;best&quot;); learn.to_fp32() learn.export(&quot;pokemon_resnet18_st1.pkl&quot;) learn.save(&quot;pokemon_resnet18_st1_wgts&quot;) . Model Interpretation . It is very important that we get to know what the model has learnt from the training process. We can do that with the help of ClassificationInterpretation class from the fastai library. . interp = ClassificationInterpretation.from_learner(learn) # Get the instances where the model has made the most error (by loss value) in the validation set. losses,idxs = interp.top_losses() # Check whether the values are all of same length as the validation set len(data.valid_ds)==len(losses)==len(idxs) . True . Interpret the images where the model made errors during the validation. . The cell below shows . the image. | the model&#39;s prediction of that image. | the actual label of that image. | the loss and probability(the extent to which the model is sure about it&#39;s prediction). | . . You can notice that the image has some of it&#39;s regions blighted, as far I know these are the regions that the model looked at to make the prediction for the corresponding image. . interp.plot_top_losses(9, figsize=(15,11)) . Let us also see which pokemon have confused the model the most. . interp.most_confused(min_val=3) . [(&#39;Sharpedo(Mega)&#39;, &#39;Sharpedo&#39;, 7), (&#39;Moltres&#39;, &#39;Rapidash&#39;, 4), (&#39;Thundurus(Incarnate)&#39;, &#39;Thundurus(Therian)&#39;, 4), (&#39;Charizard(Mega Y)&#39;, &#39;Charizard&#39;, 3), (&#39;Greninja&#39;, &#39;Greninja(Ash)&#39;, 3), (&#39;Groudon(Primal)&#39;, &#39;Incineroar&#39;, 3), (&#39;Latias(Mega)&#39;, &#39;Latios(Mega)&#39;, 3), (&#39;Nidoran(Female)&#39;, &#39;Nidorina&#39;, 3)] . Apart from the 2nd one in this list, You can see why the model was confused generally, most of it&#39;s confusion stem from the evolved species of the same pokemon. . . Let&#39;s try to train the model a little bit differently this time. . learn.load(&#39;best&#39;); . Till now we have been training only the tail region of the model (i.e.) only the last two/ three layers of our model, so essentially this model is almost same as the model which was pretrained on 1000 categories of the ImageNet dataset with some minor tweaks for our problem here. We have some options to improve the model, which are . Train all the layers so that the model can adapt to the current classification problem. We do that by unfreeze(). | Train with a very low learning rate so that it does&#39;nt forget the learnings from the pretrained weights. | . . Let&#39;s see how well we can improve the model. . learn.to_fp16() learn.unfreeze() . Before we start training again, We need to figure out at what speed the neural network should learn, this is controlled by the learning rate parameter and finding a value for is crucial to the training process. . Luckily the fastai&#39;s lr_find method will help us do just the same. . learn.lr_find(start_lr=1e-20) # Plot the learning rates and the corresponding losses. learn.recorder.plot(suggestion=True) # Get the suggested learning rate min_grad_lr = learn.recorder.min_grad_lr . Min numerical gradient: 9.77E-17 Min loss divided by 10: 6.46E-09 . Use the same callbacks as before and train for 30 epochs. . learn.fit_one_cycle(30, min_grad_lr, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 2.648827 | 1.461440 | 0.280179 | 01:08 | . 1 | 2.687755 | 1.460599 | 0.282004 | 01:08 | . 2 | 2.646746 | 1.471151 | 0.281802 | 01:07 | . 3 | 2.647440 | 1.466154 | 0.284033 | 01:07 | . 4 | 2.687051 | 1.459437 | 0.280179 | 01:07 | . 5 | 2.656536 | 1.468453 | 0.284236 | 01:07 | . 6 | 2.646480 | 1.469294 | 0.280787 | 01:08 | . 7 | 2.707206 | 1.462577 | 0.281802 | 01:08 | . 8 | 2.650942 | 1.462410 | 0.283222 | 01:07 | . 9 | 2.657768 | 1.457848 | 0.279976 | 01:07 | . 10 | 2.689249 | 1.459695 | 0.281193 | 01:07 | . 11 | 2.656215 | 1.463556 | 0.282613 | 01:07 | . 12 | 2.715505 | 1.461581 | 0.282410 | 01:09 | . 13 | 2.689469 | 1.462295 | 0.282410 | 01:08 | . 14 | 2.685328 | 1.460551 | 0.283222 | 01:08 | . 15 | 2.624705 | 1.458205 | 0.283222 | 01:10 | . 16 | 2.675736 | 1.468264 | 0.283628 | 01:11 | . 17 | 2.641450 | 1.461090 | 0.281193 | 01:10 | . 18 | 2.662758 | 1.455160 | 0.283425 | 01:12 | . 19 | 2.662972 | 1.459052 | 0.283019 | 01:13 | . 20 | 2.711507 | 1.464223 | 0.282207 | 01:13 | . 21 | 2.697404 | 1.463553 | 0.283425 | 01:13 | . 22 | 2.643310 | 1.462558 | 0.280584 | 01:12 | . 23 | 2.657411 | 1.463225 | 0.285048 | 01:12 | . 24 | 2.679297 | 1.467203 | 0.283425 | 01:13 | . 25 | 2.654091 | 1.464559 | 0.281599 | 01:12 | . 26 | 2.619208 | 1.465727 | 0.283222 | 01:12 | . 27 | 2.622938 | 1.466129 | 0.280990 | 01:12 | . 28 | 2.646025 | 1.465645 | 0.284236 | 01:13 | . 29 | 2.679323 | 1.458704 | 0.284033 | 01:13 | . Better model found at epoch 0 with error_rate value: 0.2801785469055176. Better model found at epoch 9 with error_rate value: 0.27997565269470215. Epoch 11: reducing lr to 9.288489603500534e-23 Epoch 17: reducing lr to 5.97347999592849e-23 Epoch 29: reducing lr to 3.9089488838232423e-28 . We can see that the model has improved slightly but not much, other ways that we can try are . Try using a different architecture rather than resnet18. | Add more Image augmentation methods (even though fastai has some reasonable defaults). | . Persist the environment so that we would be able to deploy the model without any problems . !pip freeze &gt; resnet18.txt . You can skip the following section, where I just save the model to my drive. . Save model to Google Drive . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&amp;redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&amp;scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&amp;response_type=code Enter your authorization code: ·········· Mounted at /content/drive . Try the model . Curious to try out the model, I have built a small Flask web app which is hosted here. You can find the code for the same in my github repo. . . The website may take some time to load since it was hosted on a free tier heroku site. . That&#39;s it for this post, Please share it if you have found it useful. Don&#39;t hesitate to leave a comment if you find that any of my explanation needs some clarification. .",
            "url": "https://mani2106.github.io/Blog-Posts/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "relUrl": "/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "date": " • Jun 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A passionate Software Engineer and Data Science Practitioner, interested in Machine Learning, Deep Learning and Data Science. I always like to do things hands on. . More of my work can be found here. . You can also contact me via email @ manimaran_p@outlook.com .",
          "url": "https://mani2106.github.io/Blog-Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mani2106.github.io/Blog-Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}