{
  
    
        "post0": {
            "title": "Building a Sticky Table of Contents: From Spec to Implementation",
            "content": "Transforming Blog Navigation: The Sticky TOC Journey . Reading long-form technical content can be challenging without proper navigation aids. When I noticed the empty right sidebar space on my Jekyll blog posts, I saw an opportunity to dramatically improve the reading experience by implementing a sticky table of contents (TOC) component. This post documents the complete journey from initial concept to production implementation using spec-driven development methodology. . The Problem: Lost in Long Content . My blog posts often contain detailed technical discussions with multiple sections and subsections. Readers would frequently lose their place or struggle to navigate to specific sections without scrolling back to find headers. The existing TOC implementation was basic and not optimally positioned, failing to provide the navigation assistance readers needed. . User Experience Pain Points . No persistent navigation: Readers had to scroll to find section headers | Wasted screen real estate: Empty right sidebar on desktop screens | Poor mobile experience: No responsive behavior for smaller screens | Accessibility gaps: Limited screen reader support and keyboard navigation | Inconsistent styling: TOC didn’t match the blog’s design system | . The Spec-Driven Approach . Rather than diving straight into code, I used Kiro’s spec-driven development methodology to systematically transform this idea into a comprehensive feature. This approach proved invaluable for managing complexity and ensuring nothing was overlooked. . Why Spec-Driven Development? . The systematic approach provided several key benefits: . Clear requirements: Defined exactly what needed to be built and why | Comprehensive design: Identified technical challenges and solutions upfront | Actionable tasks: Broke down complex implementation into manageable steps | Quality assurance: Built-in validation and testing strategies | Future maintenance: Documented decisions and procedures for ongoing updates | . Phase 1: Requirements Gathering . The first phase involved creating detailed requirements using the EARS format (Easy Approach to Requirements Syntax). This systematic approach helped identify all the necessary functionality and edge cases. . Core User Stories . Navigation Enhancement . As a blog reader, I want to see a sticky table of contents on the right side of blog posts, so that I can quickly navigate to different sections without scrolling back to the top. . Automatic Generation . As a blog reader, I want the table of contents to automatically generate from the post headers, so that I can see the hierarchical structure of the content. . Interactive Navigation . As a blog reader, I want to click on TOC entries to jump to specific sections, so that I can quickly navigate to content that interests me. . Acceptance Criteria Highlights . The requirements phase established critical acceptance criteria: . Responsive Behavior: TOC displays on desktop (&gt;1024px) but hides on mobile/tablet | Content Threshold: TOC only appears when posts have 2+ headers | Sticky Positioning: TOC remains visible during scroll with proper offset | Active Highlighting: Current section highlighted based on scroll position | Accessibility: Full keyboard navigation and screen reader support | Performance: Smooth scrolling and 60fps scroll performance | Phase 2: Technical Design . The design phase involved researching the existing Jekyll setup and creating a comprehensive technical architecture that would integrate seamlessly with the current blog infrastructure. . Architecture Overview . The solution consists of three main components working together: . ┌─────────────────────────────────────────────────────────┐ │ Post Header │ ├─────────────────┬───────────────────────────────────────┤ │ │ │ │ Sticky TOC │ Main Content │ │ (Left Side) │ (Blog Post) │ │ │ │ │ - Auto-gen │ - Headers (H1-H6) │ │ - Clickable │ - Paragraphs │ │ - Highlighted │ - Images, etc. │ │ │ │ ├─────────────────┴───────────────────────────────────────┤ │ Comments Section │ └─────────────────────────────────────────────────────────┘ . Key Design Decisions . CSS Grid Layout: Chose CSS Grid for the main layout with graceful degradation to flexbox for older browsers. . Intersection Observer API: Selected for active section highlighting due to superior performance compared to scroll event handlers. . Progressive Enhancement: Built with fallbacks at every level - from jQuery TOC plugin to vanilla JavaScript, from CSS Grid to float-based layout. . Responsive Strategy: Hide TOC completely on screens &lt;1024px rather than trying to fit it in limited space. . Phase 3: Implementation Planning . The design was broken down into 8 major implementation phases with 15 specific coding tasks. Each task included clear acceptance criteria and requirement references. . Task Breakdown Strategy . Foundation First: CSS layout and responsive behavior | Core Functionality: TOC generation and basic navigation | Enhanced Features: Active highlighting and smooth scrolling | Polish &amp; Performance: Styling, optimization, and cross-browser testing | Documentation: Comprehensive testing and user documentation | Implementation Deep Dive . CSS Grid Layout Foundation . The layout system uses CSS Grid with comprehensive fallbacks: . .post-container { display: grid; grid-template-columns: 280px 1fr; // TOC width + Main content grid-gap: 2rem; max-width: 1200px; margin: 0 auto; // Cross-browser grid support -ms-grid-columns: 280px 2rem 1fr; -ms-grid-rows: auto; // Graceful degradation for browsers without CSS Grid &amp;.no-css-grid { display: block; .toc-sidebar { float: left; width: 280px; margin-right: 2rem; } .post-main { margin-left: 312px; // 280px + 2rem gap } } // Responsive behavior - hide TOC on smaller screens @media (max-width: 1024px) { grid-template-columns: 1fr; // Single column layout } } . Sticky Positioning with Fallbacks . The sticky positioning includes comprehensive browser support: . .toc-sidebar { position: -webkit-sticky; // Safari support position: sticky; top: 2rem; height: fit-content; max-height: calc(100vh - 4rem); overflow-y: auto; // Performance optimization: Hardware acceleration transform: translateZ(0); backface-visibility: hidden; // Graceful degradation for browsers without sticky positioning &amp;.no-sticky-position { position: static; max-height: none; } } . JavaScript Enhancement Architecture . The JavaScript implementation prioritizes performance and accessibility: . // Feature detection for progressive enhancement var features = { jquery: typeof jQuery !== &#39;undefined&#39;, cssGrid: CSS.supports(&#39;display&#39;, &#39;grid&#39;), stickyPosition: CSS.supports(&#39;position&#39;, &#39;sticky&#39;), intersectionObserver: &#39;IntersectionObserver&#39; in window, smoothScroll: &#39;scrollBehavior&#39; in document.documentElement.style }; // Intersection Observer for active section highlighting var observerOptions = { rootMargin: &#39;-20% 0px -35% 0px&#39;, // Trigger when section is in middle of viewport threshold: 0 }; window.tocObserver = new IntersectionObserver(function(entries) { // Update active section highlighting based on visible headers }, observerOptions); . Fallback TOC Generation . When the jQuery TOC plugin isn’t available, the system falls back to vanilla JavaScript: . function generateFallbackTOC() { var headers = document.querySelectorAll(&#39;h2, h3, h4, h5, h6&#39;); // Hide TOC if fewer than 2 headers if (headers.length &lt; 2) { hideTOCContainer(); return false; } // Generate hierarchical TOC structure var tocList = document.createElement(&#39;ul&#39;); var currentLevel = 2; var levelStack = [tocList]; headers.forEach(function(header, index) { // Create TOC entry with proper nesting var level = parseInt(header.tagName.charAt(1)); var listItem = document.createElement(&#39;li&#39;); var link = document.createElement(&#39;a&#39;); // Handle nested levels and indentation // ... implementation details }); } . Accessibility Implementation . Accessibility was a core requirement throughout the implementation: . Semantic HTML Structure . &lt;div class=&quot;toc-container&quot; role=&quot;complementary&quot; aria-labelledby=&quot;toc-heading&quot;&gt; &lt;div class=&quot;toc-header&quot;&gt; &lt;h4 id=&quot;toc-heading&quot; class=&quot;toc-title&quot;&gt;Table of Contents&lt;/h4&gt; &lt;/div&gt; &lt;nav class=&quot;toc-nav&quot; aria-label=&quot;Table of Contents&quot;&gt; &lt;div id=&quot;toc&quot; role=&quot;navigation&quot;&gt;&lt;/div&gt; &lt;/nav&gt; &lt;/div&gt; . Keyboard Navigation Support . .toc-nav #toc ul li a { // Focus indicators for accessibility &amp;:focus { outline: 2px solid #0366d6; outline-offset: 2px; background-color: #f1f8ff; color: #0366d6; } } . Screen Reader Compatibility . Proper ARIA labels and landmarks | Semantic navigation structure | Clear focus indicators | Descriptive link text | . Performance Optimizations . Performance was critical for maintaining a smooth reading experience: . Debounced Scroll Handlers . // Performance optimization: Throttled scroll handler for 60fps var optimizedScrollHandler = throttle(function() { // Handle scroll events efficiently }, 16); // ~60fps . Hardware Acceleration . .toc-container .toc-nav #toc ul li a { // Enable hardware acceleration for smooth transitions transform: translateZ(0); backface-visibility: hidden; will-change: background-color, color, border-left-color; } . Intersection Observer Benefits . Using Intersection Observer instead of scroll events provided: . Better Performance: No need to calculate element positions on every scroll | Battery Efficiency: Reduced CPU usage on mobile devices | Smoother Animation: Native browser optimization for intersection detection | . Responsive Design Strategy . The responsive approach prioritizes usability across all device sizes: . Desktop Experience (&gt;1024px) . Full TOC visible in left sidebar | Sticky positioning active | Active section highlighting | Smooth scrolling navigation | . Tablet Experience (768px-1024px) . TOC hidden to preserve reading space | Single-column layout | Full content width utilization | . Mobile Experience (&lt;768px) . TOC completely hidden | Optimized typography and spacing | Touch-friendly navigation | . Cross-Browser Compatibility . The implementation includes comprehensive browser support: . Modern Browsers (Chrome 60+, Firefox 60+, Safari 12+) . Full functionality with all enhancements | CSS Grid layout | Intersection Observer API | Sticky positioning | . Older Browsers (IE 11, older Safari) . Graceful degradation with float-based layout | Fallback scroll-based highlighting | Static positioning with visual indicators | . Feature Detection and Polyfills . // Cross-browser compatibility: Array.from polyfill for IE11 if (!Array.from) { Array.from = function(arrayLike) { var result = []; for (var i = 0; i &lt; arrayLike.length; i++) { result.push(arrayLike[i]); } return result; }; } . Testing and Validation . Comprehensive testing ensured the feature met all requirements: . Automated Validation . The implementation includes built-in requirement validation: . function validateTOCImplementation() { var validationResults = { requirement1_1: false, // TOC displays in right sidebar requirement1_2: false, // TOC remains sticky during scroll requirement1_3: false, // TOC hidden on mobile/tablet requirement1_4: false, // TOC hidden when fewer than 2 headers // ... additional requirements }; // Validate each requirement programmatically // Log results for debugging and monitoring } . Manual Testing Scenarios . Content Variations: Tested with posts having different header structures | Device Testing: Verified responsive behavior across multiple screen sizes | Accessibility Testing: Validated keyboard navigation and screen reader compatibility | Performance Testing: Monitored scroll performance and memory usage | . Cross-Browser Testing . Tested across: . Chrome (latest and previous versions) | Firefox (latest and ESR) | Safari (macOS and iOS) | Edge (Chromium-based) | Internet Explorer 11 (graceful degradation) | . Results and Impact . The implementation successfully transformed the blog reading experience: . User Experience Improvements . Navigation Efficiency . Readers can now jump to any section instantly | Clear visual hierarchy shows content structure | Active highlighting shows current reading position | . Responsive Design . Optimal experience across all device sizes | No horizontal scrolling or layout issues | Appropriate content density for each screen size | . Accessibility Enhancement . Full keyboard navigation support | Screen reader compatibility with proper ARIA labels | High contrast mode support | Reduced motion support for accessibility preferences | . Technical Achievements . Performance Metrics . TOC generation: &lt;50ms for typical blog posts | Scroll performance: Maintains 60fps during navigation | Memory usage: &lt;1MB additional overhead | First paint: No impact on initial page load | . Browser Support . 100% functionality in modern browsers | Graceful degradation in older browsers | Zero JavaScript errors across all tested browsers | . Code Quality . Comprehensive error handling and fallbacks | Extensive documentation and comments | Modular, maintainable code structure | . Lessons Learned . Spec-Driven Development Benefits . Comprehensive Planning: Requirements phase identified edge cases that would have been missed in ad-hoc development | Risk Mitigation: Design phase revealed potential browser compatibility issues before implementation | Quality Assurance: Built-in validation ensured all requirements were met | Maintainable Code: Systematic approach resulted in well-documented, modular implementation | Technical Insights . Progressive Enhancement Works: Building with fallbacks at every level ensures broad compatibility | Performance Matters: Intersection Observer API provides significantly better performance than scroll events | Accessibility First: Considering accessibility from the start is easier than retrofitting | Responsive Strategy: Sometimes hiding features on mobile is better than trying to fit everything | Implementation Challenges . Cross-Browser Compatibility: CSS Grid and sticky positioning required extensive fallbacks | Performance Optimization: Balancing smooth animations with 60fps scroll performance | Accessibility Requirements: Ensuring full keyboard navigation while maintaining visual design | Content Variability: Handling posts with different header structures and edge cases | Future Enhancements . The spec-driven approach established a foundation for future improvements: . Planned Features . Collapsible Sections: Allow readers to collapse/expand TOC sections | Reading Progress: Visual indicator showing reading progress through the post | Bookmark Integration: Save reading position and favorite sections | Print Optimization: Enhanced print styles for TOC inclusion | . Technical Improvements . Service Worker Caching: Cache TOC generation for faster subsequent loads | WebP Image Support: Optimize any TOC-related images for better performance | Advanced Analytics: Track TOC usage patterns to optimize placement and features | . Implementation Guide . For developers wanting to implement similar functionality: . Key Dependencies . CSS Grid: For modern layout with float fallbacks | Intersection Observer API: For performance-optimized active highlighting | jQuery (optional): Can use existing TOC plugins or vanilla JavaScript fallback | . Critical Considerations . Content Threshold: Only show TOC when there’s enough content to justify it | Responsive Strategy: Consider hiding TOC on smaller screens rather than cramming it in | Performance: Use Intersection Observer instead of scroll events for better performance | Accessibility: Include proper ARIA labels and keyboard navigation from the start | Fallbacks: Plan for graceful degradation in older browsers | Code Structure . ├── _includes/toc.html # TOC HTML template and JavaScript ├── _sass/minima/custom-styles.scss # TOC styling and responsive behavior └── _layouts/post.html # Post layout integration . Conclusion . The sticky TOC implementation demonstrates the power of spec-driven development for creating comprehensive, accessible, and performant web features. By taking time to properly define requirements, create a detailed design, and plan implementation tasks, the project delivered a feature that enhances the reading experience while maintaining broad browser compatibility and accessibility standards. . The systematic approach prevented common pitfalls like: . Missing edge cases (posts with few headers) | Performance issues (scroll event handlers) | Accessibility gaps (missing ARIA labels) | Browser compatibility problems (lack of fallbacks) | . Most importantly, the spec-driven methodology created comprehensive documentation that will make future maintenance and enhancements straightforward. The requirements, design decisions, and implementation details are all documented, providing a clear roadmap for ongoing development. . This project reinforces why I’m excited about spec-driven development: it transforms complex features from overwhelming challenges into manageable, systematic projects that deliver high-quality results. . The TOC feature is now live on all blog posts, providing readers with the navigation assistance they need to efficiently consume long-form technical content. The next phase will focus on gathering user feedback and implementing the planned enhancements to further improve the reading experience. . . The complete specification documentation for this TOC implementation, including requirements, design documents, task breakdowns, and validation procedures, is available in the blog repository under .kiro/specs/sticky-toc-component/. .",
            "url": "https://mani2106.github.io/Blog-Posts/web-development/jekyll/ux/accessibility/spec-driven-development/css/javascript/2025/10/04/sticky-toc-component-implementation.html",
            "relUrl": "/web-development/jekyll/ux/accessibility/spec-driven-development/css/javascript/2025/10/04/sticky-toc-component-implementation.html",
            "date": " • Oct 4, 2025"
        }
        
    
  
    
        ,"post1": {
            "title": "Back from Hiatus: Jekyll Blog Security Hardening with Spec-Driven Development",
            "content": "The Comeback: From Hiatus to Security Hardening . After a long hiatus from blogging, I decided it was time to dust off my Jekyll blog and get back to writing. But when I opened my repository, I was greeted by a concerning sight: 12 GitHub Dependabot security alerts ranging from critical to low severity. These weren’t just minor updates—some were critical vulnerabilities with CVSS scores of 9.1 that could lead to memory corruption and use-after-free attacks. . This seemed like the perfect opportunity to not only get my blog back online securely, but also to try out a new approach I’d been exploring: spec-driven development using Kiro. Rather than diving straight into code changes, I decided to treat this security hardening as a proper software project with requirements, design, and implementation planning. . Why Spec-Driven Development? . Coming back to a project after months away, I realized I needed more than just quick fixes. I needed: . Clear documentation of what needed to be done and why | Systematic approach to prevent breaking changes | Maintainable procedures for future updates | Comprehensive testing strategy to ensure nothing broke | . This is where Kiro’s spec-driven approach became invaluable. . The Security Landscape . The vulnerabilities spanned across multiple critical components: . Nokogiri (XML/HTML parser): 5 alerts including critical memory corruption issues | REXML (XML parser): 6 alerts mostly related to Denial of Service attacks | ActiveSupport (Rails component): 1 alert for potential file disclosure | . Here’s what I was facing: . Critical Priority 🚨 . CVE-2025-49794, CVE-2025-49796: Use-after-free and memory corruption in Nokogiri (CVSS 9.1) | . High Priority ⚠️ . CVE-2025-24855, CVE-2024-55549: Use-after-free in libxslt | CVE-2024-43398: REXML DoS vulnerability with deep XML elements | . Medium Priority 📋 . Multiple REXML DoS vulnerabilities | ActiveSupport file disclosure vulnerability | . The Spec-Driven Approach with Kiro . Instead of randomly updating gems, I used Kiro to develop a structured methodology through proper specification: . 1. Requirements Gathering . Using Kiro, I started by creating a comprehensive requirements document that captured: . User stories for each type of security concern | Acceptance criteria in EARS format (Easy Approach to Requirements Syntax) | Priority matrix based on vulnerability severity | Success metrics for the security hardening | . **User Story:** As a blog maintainer, I want to analyze the current GitHub security alerts for my gem dependencies, so that I can prioritize which vulnerabilities need immediate attention. #### Acceptance Criteria 1. WHEN analyzing GitHub alerts THEN the system SHALL categorize vulnerabilities by severity (critical, high, medium, low) 2. WHEN vulnerabilities are categorized THEN the system SHALL identify which gems require updates . 2. Design Document . Kiro helped me create a detailed design that outlined: . Vulnerability analysis framework with three-phase approach | Gem upgrade mapping with version constraints | Dependency resolution strategy to prevent conflicts | Error handling scenarios and rollback procedures | . 3. Implementation Planning . The spec process generated a detailed task breakdown: . 8 major phases with 15 sub-tasks | Each task linked to specific requirements | Clear validation steps for each phase | Rollback procedures documented upfront | . 4. Baseline Documentation . Through the spec process, I systematically documented the current state: . Ruby 2.7.1 (approaching EOL) | Jekyll 4.1.1 (outdated) | Bundler 2.1.4 (incompatible with modern Ruby) | Multiple outdated gems with restrictive version constraints | . 5. Priority-Based Upgrade Strategy . The design phase helped me categorize updates by security impact and dependency relationships: . # Phase 1: Ruby runtime upgrade Ruby 2.7.1 → Ruby 3.3.9 # Phase 2: Critical security gems nokogiri: &quot;&lt; 1.18.9&quot; → &quot;~&gt; 1.18.0&quot; rexml: &quot;&lt; 3.3.9&quot; → &quot;~&gt; 3.4.0&quot; # Phase 3: Major framework updates activesupport: &quot;&lt; 6.1.7.5&quot; → &quot;~&gt; 7.2.0&quot; # Phase 4: Supporting infrastructure faraday: &quot;&lt; 1.0&quot; → &quot;~&gt; 2.12.0&quot; . 3. Constraint Optimization . One major issue was overly restrictive version constraints that prevented security updates: . # Before: Blocking security updates gem &quot;faraday&quot;, &quot;&lt; 1.0&quot; # Stuck on 0.17.5 from 2019! # After: Security-friendly constraints gem &quot;faraday&quot;, &quot;~&gt; 2.12&quot; # Allows patch updates . The Implementation Journey: Spec to Code . With Kiro’s comprehensive spec in hand, the implementation became straightforward. Each task was clearly defined with acceptance criteria, making it easy to know exactly what needed to be done and how to validate success. . Task-by-Task Execution . The beauty of the spec-driven approach was that I could work through each task systematically: . - [x] 1. Establish baseline and prepare for upgrades - [x] 2. Upgrade Ruby version to latest stable - [x] 3. Update critical security gems (Nokogiri and REXML) - [x] 4. Update ActiveSupport to latest Rails 7.x series - [x] 5. Update supporting gems to latest versions - [x] 6. Optimize Gemfile version constraints for security - [x] 7. Comprehensive testing and validation - [x] 8. Document changes and create maintenance procedures . Ruby Version Upgrade . Following the spec’s Phase 1, the foundation needed to be solid. Ruby 2.7.1 was approaching end-of-life and causing bundler compatibility issues: . # Before FROM jekyll/jekyll:4.1.0 # Ruby 2.7.1 # After FROM ruby:3.3-alpine # Ruby 3.3.9 . This single change resolved multiple compatibility issues and enabled modern gem versions. . Critical Security Updates . Nokogiri: The Big One Nokogiri had the most severe vulnerabilities, including critical memory corruption issues: . # Updated from 1.16.3 to 1.18.10 gem &quot;nokogiri&quot;, &quot;~&gt; 1.18&quot; . This update resolved 5 security alerts, including the critical CVE-2025-49794 and CVE-2025-49796. . REXML: DoS Protection REXML had 6 different Denial of Service vulnerabilities: . # Updated from 3.2.5 to 3.4.4 gem &quot;rexml&quot;, &quot;~&gt; 3.4&quot; . ActiveSupport: Major Version Jump The most challenging update was ActiveSupport, requiring a major version upgrade: . # 6.0.6.1 → 7.2.2.2 (major version jump) gem &quot;activesupport&quot;, &quot;~&gt; 7.2&quot; . Surprisingly, this major version upgrade had zero breaking changes for Jekyll usage—a testament to Rails’ commitment to backward compatibility. . Testing and Validation . Each phase included comprehensive testing: . Automated Checks . # Build verification bundle install jekyll build # Functionality testing docker-compose up -d curl -I http://localhost:4000 . Manual Verification . ✅ All blog posts render correctly | ✅ Math equations display properly (KaTeX) | ✅ Code syntax highlighting works | ✅ RSS feed generates correctly | ✅ SEO tags are present | ✅ Site navigation functions | . The Results . Security Impact . 12 vulnerabilities resolved: From critical to low severity | Zero known vulnerabilities: Clean security audit | Future-proof constraints: Automatic security patches enabled | . Performance Improvements . Ruby 3.3.x: Significant performance gains over 2.7.x | Modern HTTP/2: Faraday 2.x includes modern features | Updated plugins: Performance optimizations included | . Version Summary . | Component | Before | After | Security Impact | |———–|——–|——-|—————–| | Ruby | 2.7.1 | 3.3.9 | Latest security patches | | Nokogiri | 1.16.3 | 1.18.10 | 5 CVEs resolved | | REXML | 3.2.5 | 3.4.4 | 6 DoS vulnerabilities fixed | | ActiveSupport | 6.0.6.1 | 7.2.2.2 | File disclosure vulnerability fixed | | Faraday | 0.17.5 | 2.12.3 | 5+ years of security updates | . The Power of Spec-Driven Development . This project reinforced why I’m excited about spec-driven development with Kiro: . Benefits I Experienced . Clear roadmap: Never wondered “what should I do next?” | Risk mitigation: Potential issues identified upfront in design phase | Comprehensive testing: Validation criteria defined before implementation | Documentation by design: Requirements and procedures created as part of the process | Maintainable outcomes: Future changes have a clear framework to follow | The Kiro Advantage . Iterative refinement: Requirements → Design → Tasks → Implementation | Built-in validation: Each phase reviewed before proceeding | Comprehensive coverage: Nothing falls through the cracks | Future-ready: Maintenance procedures established from day one | . Lessons Learned . 1. Spec-Driven Beats Ad-Hoc . Taking time to create proper requirements and design prevented dependency conflicts and reduced risk significantly. . 2. Version Constraints Matter . Overly restrictive constraints (gem &quot;name&quot;, &quot;&lt; 1.0&quot;) can block critical security updates for years. . 3. Major Version Upgrades Aren’t Always Breaking . ActiveSupport 6.x → 7.x had zero breaking changes for our Jekyll usage. . 4. Documentation Is Critical . The spec process automatically generated detailed records of changes, rationale, and rollback procedures. . 5. Kiro Makes Complex Projects Manageable . What could have been an overwhelming security crisis became a systematic, well-documented project. . What’s Next for This Blog . Now that the security foundation is solid, I’m excited about the future improvements planned: . Upcoming UX Enhancements . GitHub Discussions integration: Moving away from Utterances to GitHub’s native discussion feature for better community engagement and moderation | Redesigned reading experience: Complete UX overhaul focusing on readability, accessibility, and modern design principles | Enhanced layout architecture: Left sidebar: Table of Contents for easy navigation within posts | Right sidebar: Comments section for contextual discussions | Responsive design: Seamless experience across desktop, tablet, and mobile | Progressive enhancement: Modern features that degrade gracefully | . | . Technical Roadmap . Performance optimization: Leveraging Ruby 3.3.x performance gains and modern Jekyll features | Enhanced SEO: Building on Jekyll-seo-tag 2.8.0 improvements with structured data | Better accessibility: WCAG 2.1 compliance and screen reader optimization | Modern web standards: HTTP/3, WebP images, and progressive loading | . Content Strategy Evolution . More technical deep-dives: Like this security hardening journey | Spec-driven development series: Sharing the Kiro methodology and real-world applications | Open source contributions: Documenting contributions and community learnings | Interactive tutorials: Hands-on guides with embedded examples | . Ongoing Maintenance Strategy . The spec process established comprehensive ongoing procedures: . Monthly Security Reviews . Monitor GitHub Dependabot alerts | Apply critical security patches following established procedures | Update security documentation | . Quarterly Dependency Updates . Update all gems to latest stable versions | Execute comprehensive testing protocols | Review and optimize version constraints | . Annual Major Updates . Evaluate Ruby version upgrades | Consider Jekyll major version updates | Comprehensive security audit and spec review | . The Maintenance Procedures . Kiro helped create comprehensive maintenance procedures covering: . Emergency Response: Critical vulnerability response within 24 hours | Regular Updates: Monthly and quarterly update schedules | Testing Protocols: Comprehensive validation checklists | Rollback Procedures: Quick recovery from failed updates | Documentation Standards: Keeping procedures current | . Key Takeaways . For Jekyll Users . Don’t ignore Dependabot alerts: They represent real security risks | Use semantic versioning wisely: ~&gt; x.y allows security patches | Keep Ruby current: EOL versions create cascading problems | Test systematically: Automated + manual validation prevents issues | Document everything: Future you will thank present you | For Developers . Spec-driven development works: Taking time to plan saves time in execution | Requirements matter: Clear acceptance criteria prevent scope creep and missed edge cases | Design upfront: Identifying issues before coding saves debugging time | Maintenance is part of the spec: Don’t just build it, plan to maintain it | Tools like Kiro are game-changers: Structured approaches scale better than ad-hoc methods | The Bottom Line . What started as a comeback blog post became a demonstration of modern development practices. The combination of returning from hiatus with 12 security alerts created the perfect opportunity to try spec-driven development with Kiro. . The systematic approach took more time upfront but resulted in: . Zero security vulnerabilities | Improved performance and modern dependencies | Comprehensive documentation and procedures | Clear roadmap for future improvements | Confidence in ongoing maintenance | . The blog is now running on Ruby 3.3.9 with the latest secure versions of all dependencies. More importantly, I have a proven methodology for future updates—both security and feature enhancements. . This is just the beginning of my return to regular blogging. With a solid, secure foundation and a clear development methodology, I’m excited to continue improving this blog and sharing more technical insights. . The next posts will dive deeper into spec-driven development, the upcoming UX redesign process, and how GitHub Discussions will transform the commenting experience. . I will try to do regular blogging after a multi-year dormancy! 🚀 . .",
            "url": "https://mani2106.github.io/Blog-Posts/security/jekyll/ruby/devops/maintenance/kiro/spec-driven-development/2025/09/30/jekyll-blog-security-hardening-journey.html",
            "relUrl": "/security/jekyll/ruby/devops/maintenance/kiro/spec-driven-development/2025/09/30/jekyll-blog-security-hardening-journey.html",
            "date": " • Sep 30, 2025"
        }
        
    
  
    
        ,"post2": {
            "title": "Testing the Tweet Generator Action",
            "content": "Testing the Tweet Generator . This is a test post to verify that our new tweet generator action is working correctly. . What This Tests . Content Detection: The action should detect this new post | Style Analysis: It will analyze the writing style | Thread Generation: Create an engaging tweet thread | PR Creation: Open a pull request with the generated content | Key Features . The tweet generator includes: . AI-powered content analysis | Style-aware thread creation | Engagement optimization | Safety filtering | . Next Steps . If this works correctly, you should see: . A new pull request with generated tweet threads | Updated style profile in .generated/ | Thread preview in the PR description | . Let’s see how it performs! .",
            "url": "https://mani2106.github.io/Blog-Posts/test/automation/2025/01/17/test-tweet-generator.html",
            "relUrl": "/test/automation/2025/01/17/test-tweet-generator.html",
            "date": " • Jan 17, 2025"
        }
        
    
  
    
        ,"post3": {
            "title": "Explaining prediction models and individual predictions",
            "content": "The following script tries to reproduce one algorithm from the paper titled Explaining prediction models and individual predictions . You can have a look at the resultant json here . The json would have record level information of each feature and its impact on the prediction. . import os import pandas as pd from sklearn.pipeline import Pipeline import numpy as np from typing import List import json, joblib # The feature importances for each data instance # are calculated with four random samples drawn from the data # I also tried to use numba to compile to machine code but could # not achieve it file_path = &#39;.&#39; # LOAD your model model = joblib.load(os.path.join(file_path, &#39;model_pipeline.joblib&#39;)) # Bring in the data training and testing data X_train = pd.read_csv(os.path.join(file_path, &#39;train.csv&#39;), index_col=0) X_test = pd.read_csv(os.path.join(file_path, &#39;test.csv&#39;), index_col=0) # TODO try to complete numba usage # @numba.jit(nopython=True) def shuffle_feature_records( full_data: np.array, feature_data: np.array, feature_index: int, m: int) -&gt; List[np.array]: &quot;&quot;&quot; Randomly selects `m` no of records to compare and calculate feature importances &quot;&quot;&quot; feat_len = len(feature_data) feature_indices = np.arange(0, feat_len, dtype=np.uint8) # take random indices rand_indices = np.random.choice(full_data.shape[0], m, replace=False) # take random samples w = full_data[rand_indices] b1_vec: np.array = np.empty((m, feat_len), dtype=np.object) b2_vec: np.array = np.empty((m, feat_len), dtype=np.object) for i in np.arange(0, m): del_index = np.argwhere(feature_indices==feature_index) indices = np.delete(feature_indices, del_index) # remove current feature index to add later np.random.shuffle(indices) split_pt = indices.shape[0]//2 # split indices for 2 different vectors ind_x, ind_w = indices[:split_pt], indices[split_pt:] # form the indices for b1 and b2 vectors ind_x_1 = np.append(ind_x, feature_index) ind_w_1 = np.append(ind_w, feature_index) # to ensure the features are assigned in the order of training data b1_sort_indices = np.argsort(np.append(ind_x, ind_w_1)) b2_sort_indices = np.argsort(np.append(ind_x_1, ind_w)) b1_vec[i] = np.concatenate((feature_data[ind_x], w[i, ind_w_1]), axis=0)[b1_sort_indices] b2_vec[i] = np.concatenate((feature_data[ind_x_1], w[i, ind_w]), axis=0)[b2_sort_indices] arrs = [b1_vec, b2_vec] return arrs def get_feat_imp_data( model: Pipeline, X_train: pd.DataFrame, X_test: pd.DataFrame) -&gt; dict: &quot;&quot;&quot; Loops over the available data and sends them for calculation &quot;&quot;&quot; # Get features for 100 records tr_data = X_train[X_train[&#39;id&#39;].isin(range(0, 101))].copy() te_data = X_test[X_test[&#39;id&#39;].isin(range(0, 101))].copy() # join all the data data = pd.concat([tr_data, te_data], axis=&#39;rows&#39;) cols = data.columns.tolist() feat_imp = {} # send them one by one to calculate for d in data.values: col_imp = {} for feat_index in range(data.shape[1]): b1, b2 = shuffle_feature_records(X_train.values, d, feature_index=feat_index, m=4) b1 = pd.DataFrame(b1, columns=cols) b2 = pd.DataFrame(b2, columns=cols) b1_pred = model.predict(b1) b2_pred = model.predict(b2) col_imp[cols[feat_index]] = np.sum(b1_pred - b2_pred) / 4 feat_imp[d[0]] = col_imp return feat_imp feats = get_feat_imp_data(model, X_train, X_test) with open(&#39;feat_imps_1.json&#39;, &#39;w&#39;) as f: json.dump(feats, f) .",
            "url": "https://mani2106.github.io/Blog-Posts/take-home/implementation/2021/05/29/model-agnostic-featimp.html",
            "relUrl": "/take-home/implementation/2021/05/29/model-agnostic-featimp.html",
            "date": " • May 29, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Probability - d2l.ai Exercises - Part 6",
            "content": "Prepare libraries, variables and functions for plotting . %matplotlib inline import numpy as np import tensorflow as tf import tensorflow_probability as tfp from d2l import tensorflow as d2l from matplotlib_venn import venn2 . . def plot_estimates(estimates, title_str): d2l.set_figsize((6, 4.5)) for i in range(6): d2l.plt.plot(estimates[:, i].numpy(), label=(&quot;P(die=&quot; + str(i + 1) + &quot;)&quot;)) d2l.plt.axhline(y=0.167, color=&#39;black&#39;, linestyle=&#39;dashed&#39;) d2l.plt.gca().set_xlabel(&#39;Groups of experiments&#39;) d2l.plt.gca().set_ylabel(&#39;Estimated probability&#39;) d2l.plt.legend(); d2l.plt.title(title_str) . . Vary the number of samples and number of groups for the dice experiment. Observe and analyze the experimental results. . fair_probs = tf.ones(6) / 6 . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; From this we can see that the more number of samples/experiment groups that we perform, the probability that a number will fall on the die getting close to 1/6. . Given two events with probability $P(A)$ and $P(B)$ , compute upper and lower bounds on $P(A&#8746;B)$ and $P(A&#8745;B)$ . Let&#39;s sample 1000 from two events with $P(A) =0.5$ and $P(B) =0.7$ then represent the results in a venn diagram as hinted. . counts = tfp.distributions.Binomial(1, probs=[.5, .7]).sample(1000) event_samples = list(counts.numpy().astype(int).tolist()) . event_10 = event_samples.count([1,0]) event_01 = event_samples.count([0,1]) event_11 = event_samples.count([1,1]) . venn2((event_10, event_01, event_11), (&#39;Event 1&#39;, &#39;event 2&#39;)) d2l.plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; From the diagram we can see that $$ P(A) = 468/1000 = 0.468 ( approx 0.5) P(B) = 692/1000 = 0.692 ( approx 0.7)$$ . Upper and lower bounds for $P(A&#8746;B)$ and $P(A&#8745;B)$ . Assuming Upper bound for $P(A∪B) = 1$, so that $$P(A∩B) &gt;= P(A) + P(B) - 1 P(A∩B) &gt;= 0.16$$ so the lower bound of $P(A∩B)$ is 0.16. . The upper bound of $P(A∩B)$ would be $min(P(A), P(B))$, since the $P(A∩B)$ is event of both A and B ocurring together. Consequently lower bound of $P(A∪B)$ would be $max(P(A), P(B))$ . so, $$P(A∩B) &lt;= 0.468$$ and $$P(A∪B) &gt;= 0.692$$ . At last $$0.692 &lt;= P(A∪B) &lt;= 1$$ and $$0.16&lt;=P(A∩B)&lt;=0.468$$ . Assume that we have a sequence of random variables, say A , B , and C , where B only depends on A , and C only depends on B , can you simplify the joint probability P(A,B,C) ? . Since this is a sequential occurence, A occurred first, B occurred second and at last C occurred. Let&#39;s assume $A, B$ as one event $Z$ . $$P(Z, C) = P(C|Z) * P(Z) $$ . substituting value for $Z$ $$P(A, B, C) = P(C|A, B) * P(A, B) $$ $$P(A, B, C) = P(C|A, B) * P(A|B) * P(B) $$ . In Section 2.6.2.6, the first test is more accurate. Why not run the first test twice rather than run both the first and second tests? . We did not conduct the same test because simply put running the same test on the same person, will not give a different result. .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/05/23/probability.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/05/23/probability.html",
            "date": " • May 23, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Automatic differentiation - d2l.ai Exercises - Part 5",
            "content": "import tensorflow as tf . . x = tf.range(4, dtype=tf.float32) # allocate memory x = tf.Variable(x) . . Why is the second derivative much more expensive to compute than the first derivative? . As we have learnt in our previous lessons about the chain rule and its usage to differentiate composite functions, applying the chain rule for a second time quickly adds up the number of times that we need to differentiate. Let&#39;s take an example, . Suppose that functions $y=f(u)$ and $u=g(x)$ are both differentiable, then the chain rule states that, . $$ frac{dy}{dx} = frac {dy}{du} * frac {du}{dx}$$ . let&#39;s assume $a= frac {dy}{du}$ and $b = frac {du}{dx}$ . so a second derivative of that would be (using product rule here), . $$ frac{d^2y}{dx^2} = a* frac{db}{dx} + b * frac {da}{dx} $$ . We can see that the number of differentiations have doubled for this simple composite function. This is a simple reason for why the calculation is expensive for the second derivative. . After running the function for backpropagation, immediately run it again and see what happens. . Let&#39;s check value of x . x . &lt;tf.Variable &#39;Variable:0&#39; shape=(4,) dtype=float32, numpy=array([0., 1., 2., 3.], dtype=float32)&gt; . 1st time . with tf.GradientTape() as t: y = x * x t.gradient(y, x) . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 2., 4., 6.], dtype=float32)&gt; . 2nd time . t.gradient(y, x) . RuntimeError Traceback (most recent call last) &lt;ipython-input-4-87ae3eb74e70&gt; in &lt;module&gt;() -&gt; 1 t.gradient(y, x) /usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py in gradient(self, target, sources, output_gradients, unconnected_gradients) 1025 &#34;&#34;&#34; 1026 if self._tape is None: -&gt; 1027 raise RuntimeError(&#34;A non-persistent GradientTape can only be used to&#34; 1028 &#34;compute one set of gradients (or jacobians)&#34;) 1029 if self._recording: RuntimeError: A non-persistent GradientTape can only be used tocompute one set of gradients (or jacobians) . I get the above error when I try to run the backward propagation two times, looking at the documentation of tf.GradientTape it is specified that . By default, the resources held by a GradientTape are released as soon as GradientTape.gradient() method is called. To compute multiple gradients over the same computation, create a persistent gradient tape. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. . Question 3 . TLDR: Try using a vector/matrix in the function used to demonstrate gradient calculation on a control flow. . Let&#39;s bring that function used in the lesson . def f(a): b = a * 2 while tf.norm(b) &lt; 1000: b = b * 2 if tf.reduce_sum(b) &gt; 0: c = b else: c = 100 * b return c . In the lesson they have used single value random variable, the question asks us to use a vector or matrix in its place. . Let&#39;s use the following random vector with shape (1,2) . a = tf.Variable(tf.random.normal(shape=(1, 2))) a . . &lt;tf.Variable &#39;Variable:0&#39; shape=(1, 2) dtype=float32, numpy=array([[0.02355854, 0.7092207 ]], dtype=float32)&gt; . The differentiation . with tf.GradientTape() as t: d = f(a) d_grad = t.gradient(d, a) d_grad . . &lt;tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[2048., 2048.]], dtype=float32)&gt; . We can try using a bigger matrix now. . a = tf.Variable(tf.random.normal(shape=(5, 3, 2))) a . . &lt;tf.Variable &#39;Variable:0&#39; shape=(5, 3, 2) dtype=float32, numpy= array([[[ 1.075387 , 0.3135932 ], [-0.9573184 , 0.06560528], [-0.18362595, -0.53352755]], [[ 0.48579827, 0.7452166 ], [ 0.08363861, -1.1838139 ], [-0.23465118, -0.7641606 ]], [[-1.6045334 , -0.50408304], [-0.82722497, -0.26551095], [ 0.5110664 , 0.68872666]], [[ 0.9107071 , 1.2259225 ], [ 0.43528086, -0.36280373], [-1.7287321 , -0.54161024]], [[ 0.01597405, -0.01879804], [ 1.75937 , 1.2245483 ], [ 0.04912755, 0.9361492 ]]], dtype=float32)&gt; . with tf.GradientTape() as t: d = f(a) d_grad = t.gradient(d, a) d_grad . . &lt;tf.Tensor: shape=(5, 3, 2), dtype=float32, numpy= array([[[256., 256.], [256., 256.], [256., 256.]], [[256., 256.], [256., 256.], [256., 256.]], [[256., 256.], [256., 256.], [256., 256.]], [[256., 256.], [256., 256.], [256., 256.]], [[256., 256.], [256., 256.], [256., 256.]]], dtype=float32)&gt; . So when we differentiate a single value, the resultant gradient vector is similar in shape, as in the cases of the higher dimensional vectors/matrices. . I did not get what the question was trying to test the reader on, I read the discussion, A person said to use tf.hessians to a related question. Let&#39;s see why . Let&#39;s take a look at the matrix we have . a = tf.Variable(tf.random.normal(shape=()), trainable=True) a . . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=-1.1861123&gt; . The following is the suggested way to calculate a hessian based vector product . with tf.GradientTape() as t2: with tf.GradientTape() as t1: d1 = f(a) diff_1 = t1.gradient(d1, a) diff_2 = t2.gradient(diff_1, a) diff_2 . diff_1, diff_2 . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=102400.0&gt;, None) . The following is modified based on this to see what happens. . with tf.GradientTape() as t4: t4.watch(a) with tf.GradientTape() as t3: t3.watch(a) d = f(a) dif = t3.gradient(d, a) j_diff = t4.jacobian(dif, a) . WARNING:tensorflow:11 out of the last 11 calls to &lt;function pfor.&lt;locals&gt;.f at 0x7f5d138bd5f0&gt; triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for more details. . dif, j_diff . (&lt;tf.Tensor: shape=(), dtype=float32, numpy=102400.0&gt;, None) . Both the methods return the same result for the small vector, where the second order derivative is turning into None, I am not sure why..., thinking about the question asked, since $f(a)$ is no longer scalar, the resultant gradient is also a vector/matrix. So I am again not sure of the suggestion given. . Redesign an example of finding the gradient of the control flow. Run and analyze the result. . The example given in the lesson was a little complex, I will use a simple control flow, to see if we can calculate the gradient with autodiff. . Control flow . Test with exponentiation . Let&#39;s start with a simple transformation, We all know the gradient of an exponential function is the same. . def f(x): return tf.math.exp(x) . Let&#39;s take a single value . x = tf.Variable(tf.random.normal(shape=()), trainable=True) x . . &lt;tf.Variable &#39;Variable:0&#39; shape=() dtype=float32, numpy=1.5236874&gt; . with tf.GradientTape() as t: y = f(x) dy_dx = t.gradient(y, x) . y == dy_dx . &lt;tf.Tensor: shape=(), dtype=bool, numpy=True&gt; . Test with a random function . So now we can try performing some transformations on the value. . $$y rightarrow f(x) rightarrow left { left( eqalign{ sin{x}, if sum{x}_n/n &gt; 0.5 2 * cos{x}, else } right) right }$$ def f(x): if tf.math.reduce_sum(x) &gt; 0.5: x = tf.math.sin(x) else: x = 2 * tf.math.cos(x) return x . x = tf.Variable(tf.random.normal(shape=(20, 1)), trainable=True) x . . &lt;tf.Variable &#39;Variable:0&#39; shape=(20, 1) dtype=float32, numpy= array([[ 0.7322539 ], [-1.2769327 ], [ 0.85455877], [-0.96424586], [-0.7693011 ], [ 0.33113363], [-1.3973997 ], [-2.1953282 ], [ 1.4873 ], [-0.7533999 ], [-0.58519673], [-0.94799626], [ 0.67873853], [ 0.34689212], [ 0.4139984 ], [ 0.5342669 ], [-0.06651524], [-0.19935435], [-0.3597326 ], [-0.24857213]], dtype=float32)&gt; . The differentiation . with tf.GradientTape() as t: y = f(x) dy_dx = t.gradient(y, x) dy_dx . . &lt;tf.Tensor: shape=(20, 1), dtype=float32, numpy= array([[-1.337095 ], [ 1.9142638 ], [-1.5085627 ], [ 1.6432385 ], [ 1.3912666 ], [-0.6502306 ], [ 1.9700089 ], [ 1.6224738 ], [-1.9930325 ], [ 1.368245 ], [ 1.1047268 ], [ 1.6244967 ], [-1.2556233 ], [-0.6799534 ], [-0.8045463 ], [-1.0184205 ], [ 0.13293241], [ 0.396073 ], [ 0.7040479 ], [ 0.49204046]], dtype=float32)&gt; . x_axis = tf.range(0, 20, 1) plt.plot(x_axis, tf.reshape(f(x), [-1]).numpy(), color=&#39;r&#39;, label=&#39;y&#39;) plt.plot(x_axis, tf.reshape(dy_dx, [-1]).numpy(), color=&#39;g&#39;, label=&#39;dy_dx&#39;) plt.legend(loc=&#39;lower right&#39;) plt.show() . . The graph seems to be validating differentiation, since we can see the green line following the red line . Let $f(x)=sin(x)$ . Plot $f(x)$ and $ frac {df(x)}{dx}$ , where the latter is computed without exploiting that $f&#8242;(x)=cos(x)$. . Let&#39;s setup the required variables . # sine function f = tf.math.sin x = tf.Variable(tf.range(-10, 10, 0.1)) with tf.GradientTape() as t: y = f(x) # the gradient of sin(x) is cos(x) dy_dx = t.gradient(y, x) . . plt.figure(1) x_axis = tf.range(-10, 10, 0.1) plt.plot(x_axis, f(x).numpy(), color=&#39;r&#39;) plt.plot(x_axis, dy_dx.numpy(), color=&#39;g&#39;) plt.show() . .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/05/16/autodiff.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/05/16/autodiff.html",
            "date": " • May 16, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Text mining with chemical text from chemdNER",
            "content": "This blog post has the code and details, of the take home project that I did for a company as part of their interview process, this primarily uses the CHEMDNER data to create a named recognition model using spaCy . The CHEMDNER corpus of chemicals and drugs and its annotation principles . Krallinger, M. et al. The CHEMDNER corpus of chemicals and drugs and its annotation principles. J Cheminform, 2014 | . This code should be directly runnable in colab, so just check and change the paths where I save and get data from my google drive, to run on your colab instance. Prepare data . Get data and make create directories to organise information. . !wget --no-check-certificate https://biocreative.bioinformatics.udel.edu/media/store/files/2014/chemdner_corpus.tar.gz . !mkdir /content/data !tar -xvf /content/drive/MyDrive/chemdNER data/data.tar.gz -C /content/data . chemdner_corpus/ chemdner_corpus/BioC.dtd chemdner_corpus/cdi_ann_test_13-09-13.txt chemdner_corpus/cem_ann_test_13-09-13.txt chemdner_corpus/chemdner_abs_test_pmid_label.txt chemdner_corpus/chemdner_chemical_disciplines.txt chemdner_corpus/chemdner.key chemdner_corpus/development.abstracts.txt chemdner_corpus/development.annotations.txt chemdner_corpus/development.bioc.xml chemdner_corpus/evaluation.abstracts.txt chemdner_corpus/evaluation.annotations.txt chemdner_corpus/evaluation.bioc.xml chemdner_corpus/evaluation.predictions.txt chemdner_corpus/plain2bioc.py chemdner_corpus/Readme.txt chemdner_corpus/silver.abstracts.txt chemdner_corpus/silver.predictions.txt chemdner_corpus/training.abstracts.txt chemdner_corpus/training.annotations.txt chemdner_corpus/training.bioc.xml chemdner_corpus/evaluation.predictions.bioc.xml . . Install required libraries . Look at the libraries currently installed. . !pip list | grep spacy . spacy 2.2.4 . !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.3.0/en_core_sci_lg-0.3.0.tar.gz . Since we are training data with medical text a model with similar vectors will be extremely useful, using the scispacy model vectors. . !pip list| grep en-core-sci-lg . en-core-sci-lg 0.3.0 . Import annotations . import pandas as pd from pathlib import Path annot_path = Path(&#39;/content/data/chemdner_corpus&#39;) export_path = Path(&#39;/content/data/spacy_annotation_data&#39;) export_path.mkdir(exist_ok=True) . train_annot = pd.read_csv(annot_path/&#39;training.annotations.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Text Type&#39;, &#39;Start&#39;, &#39;End&#39;, &#39;Entity Text&#39;, &#39;Entity&#39;]) train_annot.head() . PMID Text Type Start End Entity Text Entity . 0 21826085 | A | 946 | 957 | haloperidol | TRIVIAL | . 1 22080034 | A | 190 | 199 | aflatoxin | FAMILY | . 2 22080034 | A | 594 | 603 | aflatoxin | FAMILY | . 3 22080034 | A | 718 | 727 | aflatoxin | FAMILY | . 4 22080034 | A | 1072 | 1081 | aflatoxin | FAMILY | . Since the training text can be in the title or in the abstract, we should be careful on what text we are giving in an annotation. . Let&#39;s check in the annotation for one value . start, end = train_annot.loc[train_annot[&#39;PMID&#39;] == 21826085, [&#39;Start&#39;, &#39;End&#39;]].values.tolist()[0] . start, end # start, end indices for haloperidol . (946, 957) . train_text = pd.read_csv(annot_path/&#39;training.abstracts.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Title&#39;, &#39;Abstract&#39;]) train_text.head() . PMID Title Abstract . 0 21826085 | DPP6 as a candidate gene for neuroleptic-induc... | We implemented a two-step approach to detect p... | . 1 22080034 | Nanosilver effects on growth parameters in exp... | Aflatoxicosis is a cause of economic losses in... | . 2 22080035 | The influence of the intensity of smoking and ... | The aim of this study was to investigate the e... | . 3 22080037 | Mercury induces the expression of cyclooxygena... | Nuclear factor-κB (NF-κB) is a transcription f... | . 4 22258629 | Toxic effects of chromium on tannery workers a... | Chromium is widely used in the leather industr... | . train_annot[&#39;Text Type&#39;].value_counts() . A 26425 T 3053 Name: Text Type, dtype: int64 . train_text.loc[train_text[&#39;PMID&#39;] == 21826085, &#39;Abstract&#39;].item()[start:end] . &#39;haloperidol&#39; . We can see that the annotation data will seperate for the Title and Abstract pieces of the text. . Make raw text files for pretraining . import srsly . dev_text = pd.read_csv(annot_path/&#39;development.abstracts.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Title&#39;, &#39;Abstract&#39;]) eval_text = pd.read_csv(annot_path/&#39;evaluation.abstracts.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Title&#39;, &#39;Abstract&#39;]) . texts_path = export_path/&#39;texts&#39; texts_path.mkdir(exist_ok=True) def make_jsonl(df:pd.DataFrame): jsonl_txts = [] for data in df.itertuples(index=False): jsonl_txts.append({&#39;text&#39;:data.Title+&#39; n&#39;+data.Abstract}) return jsonl_txts train_txts = make_jsonl(train_text) dev_txts = make_jsonl(dev_text) eval_txts = make_jsonl(eval_text) all_txts = train_txts + eval_txts + dev_txts . srsly.write_jsonl(str(texts_path/&#39;pretrain_txts.jsonl&#39;), train_txts[:1500]) . Make json file for training . # set pmid as the index train_annot.set_index(&#39;PMID&#39;, drop=True, inplace=True) train_text.set_index(&#39;PMID&#39;, drop=True, inplace=True) . train_data = train_text.join(train_annot) . train_data.head() . Title Abstract Text Type Start End Entity Text Entity . PMID . 21826085 DPP6 as a candidate gene for neuroleptic-induc... | We implemented a two-step approach to detect p... | A | 946.0 | 957.0 | haloperidol | TRIVIAL | . 22080034 Nanosilver effects on growth parameters in exp... | Aflatoxicosis is a cause of economic losses in... | A | 190.0 | 199.0 | aflatoxin | FAMILY | . 22080034 Nanosilver effects on growth parameters in exp... | Aflatoxicosis is a cause of economic losses in... | A | 594.0 | 603.0 | aflatoxin | FAMILY | . 22080034 Nanosilver effects on growth parameters in exp... | Aflatoxicosis is a cause of economic losses in... | A | 718.0 | 727.0 | aflatoxin | FAMILY | . 22080034 Nanosilver effects on growth parameters in exp... | Aflatoxicosis is a cause of economic losses in... | A | 1072.0 | 1081.0 | aflatoxin | FAMILY | . Let&#39;s check the number of annotations for each label. . train_data[&#39;Entity&#39;].value_counts().plot(kind=&#39;bar&#39;, title=&#39;Number of annotations per label&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fcc6f26c630&gt; . We have eight entities and the entity named TRIVIAL has the most number of annotations . Now we can start generating the annotation from the dataframe, to accomodate the training format that we want we can split the data frame to Title and Abstract annotations, so that we can run the same logic easily, (ie) grouping the texts and getting all the annotations corresponding to that text in one shot. . abstract_annots = train_data.loc[train_data[&#39;Text Type&#39;] == &#39;A&#39;, :] title_annots = train_data.loc[train_data[&#39;Text Type&#39;] == &#39;T&#39;, :] . import en_core_sci_lg from spacy.tokens import Span from spacy.gold import docs_to_json import json # load the pretrained model and other utility functions # to generate the training data mod = en_core_sci_lg.load() # since we are going to fill this manually # for annotation, disabling some pipeline # modules may speedup the process mod.disable_pipes([&#39;ner&#39;]) . [(&#39;ner&#39;, &lt;spacy.pipeline.pipes.EntityRecognizer at 0x7f8f0be55408&gt;)] . def extract_spacy_docs(annot_df:pd.DataFrame, ext_from:str) -&gt; list: # Extracting the doc objects from a pretrained model # so that the tokenization matches for this domain of # text that we have annot_docs = [] # Same texts have multiple annotations so group and take # one by one for _, annot_data in annot_df.groupby(ext_from): # Text for annotation txt = annot_data.iloc[0][ext_from] doc = mod(txt) ent_spans = [] text_ranges = [] for annot in annot_data.itertuples(): start = int(annot.Start) end = int(annot.End) # Validating overlapping entity annotations if start not in text_ranges and end not in text_ranges: # Make spans with our annotations span = doc.char_span(int(annot.Start), int(annot.End), label=annot.Entity) # It may be None for invalid annotations(wrt spaCy) if span is not None: ent_spans.append(span) text_ranges.extend(list(range(start, end+1,))) # Just for info else: print(txt, txt[start:end]) doc.ents = ent_spans annot_docs.append(doc) return annot_docs . abstract_annots = extract_spacy_docs(abstract_annots, &#39;Abstract&#39;) title_annots = extract_spacy_docs(title_annots, &#39;Title&#39;) train_annot_docs = abstract_annots + title_annots train_json = docs_to_json(train_annot_docs) . spacy_annot_path = export_path/&#39;annots&#39; spacy_annot_path.mkdir(exist_ok=True) srsly.write_json(spacy_annot_path/&#39;train_data.json&#39;, [train_json]) . We must do the same for the development and evaluation data . Make json file for evaluation and development . Let&#39;s make a function out of the process we did for the training data, for the evaluation and development data. . dev_annots = pd.read_csv(annot_path/&#39;development.annotations.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Text Type&#39;, &#39;Start&#39;, &#39;End&#39;, &#39;Entity Text&#39;, &#39;Entity&#39;]) eval_annots = pd.read_csv(annot_path/&#39;evaluation.annotations.txt&#39;, sep=&#39; t&#39;, names=[&#39;PMID&#39;, &#39;Text Type&#39;, &#39;Start&#39;, &#39;End&#39;, &#39;Entity Text&#39;, &#39;Entity&#39;]) . def prepare_df_for_spacy_annots(df1: pd.DataFrame, df2: pd.DataFrame) -&gt; &#39;Abstract and Title dataframes&#39;: # Emulates the processing we did for training data df1.set_index(&#39;PMID&#39;, drop=True, inplace=True) df2.set_index(&#39;PMID&#39;, drop=True, inplace=True) df = df1.join(df2) abstract_annots = df.loc[df[&#39;Text Type&#39;] == &#39;A&#39;, :] title_annots = df.loc[df[&#39;Text Type&#39;] == &#39;T&#39;, :] return abstract_annots, title_annots def prepare_spacy_json_from_df(df1: pd.DataFrame, df2: pd.DataFrame): # transform the docs to spacy json abstract_annots, title_annots = prepare_df_for_spacy_annots(df1, df2) abstract_docs = extract_spacy_docs(abstract_annots, &#39;Abstract&#39;) title_docs = extract_spacy_docs(title_annots, &#39;Title&#39;) annot_docs = abstract_docs + title_docs spacy_json = docs_to_json(train_annot_docs) return spacy_json . dev_json = prepare_spacy_json_from_df(dev_text, dev_annots) eval_json = prepare_spacy_json_from_df(eval_text, eval_annots) srsly.write_json(spacy_annot_path/&#39;eval_data.json&#39;, [eval_json]) srsly.write_json(spacy_annot_path/&#39;dev_data.json&#39;, [dev_json]) . We now have all the files required to make the model, let&#39;s start by pretraining the model with spacy cli . Pretrain spacy with texts . We have imported scispacy&#39;s model vectors(en_core_sci_lg) to give us a headstart . !python -m spacy pretrain /content/data/spacy_annotation_data/texts/pretrain_txts.jsonl /usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0 /content/data/pretrained_model -i 50 -se 10 -s 0 -bs 64 . Debug the training data for spacy NER model . Let&#39;s see if the data is proper for training using spacy&#39;s debug-data command . %%bash python -m spacy debug-data en /content/drive/MyDrive/chemdNER data/annot_data/train_data.json /content/drive/MyDrive/chemdNER data/annot_data/dev_data.json -p &#39;ner&#39; -b /usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0 . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: ner Starting with base model &#39;/usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0&#39; 4884 training docs 4884 evaluation docs ⚠ 4884 training examples also in evaluation data ============================== Vocab &amp; Vectors ============================== ℹ 647737 total words in the data (47733 unique) ℹ 600000 vectors (667864 unique keys, 200 dimensions) ⚠ 21625 words in training data without vectors (0.03%) ========================== Named Entity Recognition ========================== ℹ 8 new labels, 0 existing labels 0 missing values (tokens with &#39;-&#39; label) ⚠ 3 entity span(s) with punctuation ⚠ Low number of examples for new label &#39;NO CLASS&#39; (40) ✔ Examples without occurrences available for all labels ✔ No entities consisting of or starting/ending with whitespace Entity spans consisting of or starting/ending with punctuation can not be trained with a noise level &gt; 0. ================================== Summary ================================== ✔ 3 checks passed ⚠ 4 warnings . We have low annotations for NO CLASS but since we do not have more data, we have to work with what we&#39;ve got. . We only had 4884 pieces of text from the dataset, and it seems to be used in all the places (train, dev, eval), but the annotation in each will differ. . !python -m spacy link /usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0 en_core_sci_lg . Train model . I tried to train with the pretrained model which we made using raw text but it was not working and it seems most of the tutorials use it only text classification problems. . But anyway I start with the vectors of scispacy&#39;s pretrained en_core_sci_lg model for a headstart. . %%bash python -m spacy train en /content/data/NER model /content/drive/MyDrive/chemdNER data/annot_data/train_data.json /content/drive/MyDrive/chemdNER data/annot_data/dev_data.json -v /usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0 -p &quot;ner&quot; -n 30 -g 0 -V 0.1 -cW 3 -cd 4 -cw 4 -cP 3 . Training pipeline: [&#39;ner&#39;] Using GPU: 0 Starting with blank model &#39;en&#39; Loading vector from model &#39;/usr/local/lib/python3.6/dist-packages/en_core_sci_lg/en_core_sci_lg-0.3.0&#39; Counting training words (limit=0) Itn NER Loss NER P NER R NER F Token % CPU WPS GPU WPS - - - 1 48369.133 38.580 23.141 28.929 91.214 79243 91040 2 37874.029 41.895 28.937 34.231 91.214 79243 93572 3 35916.519 45.435 33.101 38.300 91.214 79243 88774 4 34941.559 48.925 37.233 42.286 91.214 79243 90150 5 33485.552 52.235 40.570 45.669 91.214 79243 88586 6 32591.528 56.412 44.786 49.931 91.214 79243 85016 7 31565.729 59.580 48.290 53.344 91.214 79243 87689 8 30361.623 63.674 53.106 57.912 91.214 79243 83318 9 29941.032 66.154 56.482 60.937 91.214 79243 82007 10 28910.156 68.003 58.950 63.154 91.214 79243 81201 11 28704.946 69.475 60.998 64.961 91.214 79243 84349 12 27963.309 70.596 62.859 66.503 91.214 79243 84314 13 27236.334 71.279 64.103 67.501 91.214 79243 85654 14 27401.528 72.042 65.215 68.458 91.214 79243 85735 15 26876.413 72.860 65.895 69.203 91.214 79243 86002 16 26792.615 73.405 66.611 69.843 91.214 79243 83849 17 26456.828 74.399 67.623 70.849 91.214 79243 86532 18 25825.262 74.959 68.207 71.424 91.214 79243 85455 19 25672.156 75.481 68.839 72.007 91.214 79243 84888 20 25490.234 75.762 69.159 72.310 91.214 79243 84073 21 24909.075 76.367 69.799 72.935 91.214 79243 86211 22 24919.627 76.807 70.315 73.418 91.214 79243 85168 23 24733.465 77.390 70.775 73.935 91.214 79243 87323 24 24437.002 77.744 71.139 74.295 91.214 79243 83429 25 24302.424 78.324 71.547 74.782 91.214 79243 88724 26 24329.596 78.659 71.951 75.156 91.214 79243 84788 27 24270.859 79.210 72.179 75.531 91.214 79243 86826 28 23739.026 79.464 72.519 75.833 91.214 79243 85965 29 23988.403 79.575 72.563 75.908 91.214 79243 83390 30 23883.377 79.800 72.583 76.021 91.214 79243 82936 ✔ Saved model to output directory /content/data/NER model/model-final ✔ Created best model /content/data/NER model/model-best . . I did not try to tune the hyper parameters and relied on the defaults from the lib, but it has given decent results . Evaluate model with test data . Let&#39;s evaluate with the evaluation data that we generated earlier . %%bash python -m spacy evaluate /content/data/NER model/model-best /content/drive/MyDrive/chemdNER data/annot_data/eval_data.json -g 0 -dp /content/data/eval results -R . ================================== Results ================================== Time 8.66 s Words 688898 Words/s 79533 TOK 91.21 POS 0.03 UAS 3.88 LAS 0.00 NER P 79.80 NER R 72.58 NER F 76.02 Textcat 0.00 ✔ Generated 25 parses as HTML /content/data/eval results . /usr/local/lib/python3.6/dist-packages/spacy/displacy/__init__.py:189: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary. warnings.warn(Warnings.W006) . The results of some text are persisted and can be viewed in the archive. . Misc . Here I tried to write code for simple annotation format, mostly for archival, can be skipped, if need be . def extract_annot_data(annot_df: pd.DataFrame, ext_from: str) -&gt; list: # groups text and generates the list of entities and # their spans in the text train_data = [] for _, annot_data in annot_df.groupby(ext_from): ttext = annot_data.iloc[0][ext_from] entity_list = [] # print(ttext) text_ranges = [] for annot in annot_data.itertuples(): start = int(annot.Start) end = int(annot.End) if start not in text_ranges and end not in text_ranges: text_ranges.extend(list(range(start, end+1,))) entity_list.append((annot.Start, annot.End, annot.Entity)) train_data.append((ttext, {&#39;entities&#39;:entity_list})) return train_data . abst_annots = extract_annot_data(abstract_annots, &#39;Abstract&#39;) title_annots = extract_annot_data(title_annots, &#39;Title&#39;) . abst_annots[0] . (&#39;(-)-Carvone is a monoterpene ketone found in spearmint (Mentha spicata var. crispa) essential oil that is widely used as an odor and flavor additive. An intestinal antispasmodic effect was recently reported for (-)-carvone, and it has been shown to be more potent than its (+)-antipode. The mechanism of (-)-carvone action in the intestines has not been investigated. To gain a better understanding of the (-)-carvone antispasmodic effect, we investigated its pharmacological effects in the guinea pig ileum. Terminal portions of the ileum were mounted for isotonic contraction recordings. The effect of (-)-carvone was compared with that of the classical calcium channel blocker (CCB) verapamil. In isolated ileal smooth muscle, (-)-carvone did not produce direct contractile or relaxation responses and did not modify electrically elicited contractions or low K(+)-evoked contractions. The submaximal contractions induced by histamine (p&lt;0.001), BaCl2 (p&lt;0.05), and carbachol (p&lt;0.01) were significantly reduced by (-)-carvone. The contractile response elicited by high concentrations of carbachol was reduced but not abolished by (-)-carvone. No additive action was detected with co-incubation of (-)-carvone and verapamil on carbachol-induced contraction. (-)-Carvone reduced the contraction induced by high K(+) and was almost 100 times more potent than verapamil. Thus, (-)-carvone showed a typical and potent CCB-like action. Many effects described for both (-)-carvone and spearmint oil can be explained as a CCB-like mode of action.&#39;, {&#39;entities&#39;: [(1090.0, 1099.0, &#39;TRIVIAL&#39;), (1133.0, 1144.0, &#39;TRIVIAL&#39;), (1200.0, 1211.0, &#39;TRIVIAL&#39;), (1216.0, 1225.0, &#39;TRIVIAL&#39;), (1229.0, 1238.0, &#39;TRIVIAL&#39;), (1359.0, 1368.0, &#39;TRIVIAL&#39;), (1312.0, 1316.0, &#39;FORMULA&#39;), (968.0, 977.0, &#39;TRIVIAL&#39;), (1376.0, 1387.0, &#39;TRIVIAL&#39;), (1465.0, 1476.0, &#39;TRIVIAL&#39;), (1260.0, 1271.0, &#39;TRIVIAL&#39;), (948.0, 953.0, &#39;FORMULA&#39;), (1017.0, 1028.0, &#39;TRIVIAL&#39;), (862.0, 866.0, &#39;FORMULA&#39;), (927.0, 936.0, &#39;TRIVIAL&#39;), (0.0, 11.0, &#39;TRIVIAL&#39;), (17.0, 35.0, &#39;FAMILY&#39;), (211.0, 222.0, &#39;TRIVIAL&#39;), (304.0, 315.0, &#39;TRIVIAL&#39;), (604.0, 615.0, &#39;TRIVIAL&#39;), (656.0, 663.0, &#39;SYSTEMATIC&#39;), (686.0, 695.0, &#39;TRIVIAL&#39;), (730.0, 741.0, &#39;TRIVIAL&#39;), (406.0, 417.0, &#39;TRIVIAL&#39;)]}) . A sample text from the corpus . ttext . &#39;(-)-Carvone is a monoterpene ketone found in spearmint (Mentha spicata var. crispa) essential oil that is widely used as an odor and flavor additive. An intestinal antispasmodic effect was recently reported for (-)-carvone, and it has been shown to be more potent than its (+)-antipode. The mechanism of (-)-carvone action in the intestines has not been investigated. To gain a better understanding of the (-)-carvone antispasmodic effect, we investigated its pharmacological effects in the guinea pig ileum. Terminal portions of the ileum were mounted for isotonic contraction recordings. The effect of (-)-carvone was compared with that of the classical calcium channel blocker (CCB) verapamil. In isolated ileal smooth muscle, (-)-carvone did not produce direct contractile or relaxation responses and did not modify electrically elicited contractions or low K(+)-evoked contractions. The submaximal contractions induced by histamine (p&lt;0.001), BaCl2 (p&lt;0.05), and carbachol (p&lt;0.01) were significantly reduced by (-)-carvone. The contractile response elicited by high concentrations of carbachol was reduced but not abolished by (-)-carvone. No additive action was detected with co-incubation of (-)-carvone and verapamil on carbachol-induced contraction. (-)-Carvone reduced the contraction induced by high K(+) and was almost 100 times more potent than verapamil. Thus, (-)-carvone showed a typical and potent CCB-like action. Many effects described for both (-)-carvone and spearmint oil can be explained as a CCB-like mode of action.&#39; . from spacy.gold import docs_to_json, GoldParse . A sample of the spacy json format for training models . docs_to_json([mod(ttext)]) . {&#39;id&#39;: 0, &#39;paragraphs&#39;: [{&#39;cats&#39;: [], &#39;raw&#39;: &#39;(-)-Carvone is a monoterpene ketone found in spearmint (Mentha spicata var. crispa) essential oil that is widely used as an odor and flavor additive. An intestinal antispasmodic effect was recently reported for (-)-carvone, and it has been shown to be more potent than its (+)-antipode. The mechanism of (-)-carvone action in the intestines has not been investigated. To gain a better understanding of the (-)-carvone antispasmodic effect, we investigated its pharmacological effects in the guinea pig ileum. Terminal portions of the ileum were mounted for isotonic contraction recordings. The effect of (-)-carvone was compared with that of the classical calcium channel blocker (CCB) verapamil. In isolated ileal smooth muscle, (-)-carvone did not produce direct contractile or relaxation responses and did not modify electrically elicited contractions or low K(+)-evoked contractions. The submaximal contractions induced by histamine (p&lt;0.001), BaCl2 (p&lt;0.05), and carbachol (p&lt;0.01) were significantly reduced by (-)-carvone. The contractile response elicited by high concentrations of carbachol was reduced but not abolished by (-)-carvone. No additive action was detected with co-incubation of (-)-carvone and verapamil on carbachol-induced contraction. (-)-Carvone reduced the contraction induced by high K(+) and was almost 100 times more potent than verapamil. Thus, (-)-carvone showed a typical and potent CCB-like action. Many effects described for both (-)-carvone and spearmint oil can be explained as a CCB-like mode of action.&#39;, &#39;sentences&#39;: [{&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;nsubj&#39;, &#39;head&#39;: 4, &#39;id&#39;: 0, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-Carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;cop&#39;, &#39;head&#39;: 3, &#39;id&#39;: 1, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;is&#39;, &#39;tag&#39;: &#39;VBZ&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 2, &#39;id&#39;: 2, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;a&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 3, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;monoterpene&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 4, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;ketone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;acl&#39;, &#39;head&#39;: -1, &#39;id&#39;: 5, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;found&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 6, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;in&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 7, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;spearmint&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 3, &#39;id&#39;: 8, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(&#39;, &#39;tag&#39;: &#39;-LRB-&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 2, &#39;id&#39;: 9, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;Mentha&#39;, &#39;tag&#39;: &#39;FW&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 10, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;spicata&#39;, &#39;tag&#39;: &#39;FW&#39;}, {&#39;dep&#39;: &#39;dep&#39;, &#39;head&#39;: -4, &#39;id&#39;: 11, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;var&#39;, &#39;tag&#39;: &#39;FW&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -8, &#39;id&#39;: 12, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 13, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;crispa&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 14, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 15, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;essential&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;appos&#39;, &#39;head&#39;: -3, &#39;id&#39;: 16, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;oil&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 3, &#39;id&#39;: 17, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;that&#39;, &#39;tag&#39;: &#39;WDT&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 2, &#39;id&#39;: 18, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;is&#39;, &#39;tag&#39;: &#39;VBZ&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 19, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;widely&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;acl:relcl&#39;, &#39;head&#39;: -4, &#39;id&#39;: 20, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;used&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 5, &#39;id&#39;: 21, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;as&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 4, &#39;id&#39;: 22, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;an&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 3, &#39;id&#39;: 23, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;odor&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 24, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 25, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;flavor&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -6, &#39;id&#39;: 26, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;additive&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -14, &#39;id&#39;: 27, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 3, &#39;id&#39;: 28, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;An&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 2, &#39;id&#39;: 29, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;intestinal&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 30, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;antispasmodic&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 3, &#39;id&#39;: 31, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;effect&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 2, &#39;id&#39;: 32, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;was&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 33, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;recently&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 34, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;reported&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 35, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;for&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 36, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -3, &#39;id&#39;: 37, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -4, &#39;id&#39;: 38, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 3, &#39;id&#39;: 39, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;it&#39;, &#39;tag&#39;: &#39;PRP&#39;}, {&#39;dep&#39;: &#39;aux&#39;, &#39;head&#39;: 2, &#39;id&#39;: 40, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;has&#39;, &#39;tag&#39;: &#39;VBZ&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 41, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;been&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -8, &#39;id&#39;: 42, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;shown&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;mark&#39;, &#39;head&#39;: 3, &#39;id&#39;: 43, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;to&#39;, &#39;tag&#39;: &#39;TO&#39;}, {&#39;dep&#39;: &#39;cop&#39;, &#39;head&#39;: 2, &#39;id&#39;: 44, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;be&#39;, &#39;tag&#39;: &#39;VB&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 45, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;more&#39;, &#39;tag&#39;: &#39;RBR&#39;}, {&#39;dep&#39;: &#39;xcomp&#39;, &#39;head&#39;: -4, &#39;id&#39;: 46, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;potent&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 47, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;than&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod:poss&#39;, &#39;head&#39;: 1, &#39;id&#39;: 48, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;its&#39;, &#39;tag&#39;: &#39;PRP$&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 49, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(+)-antipode&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -16, &#39;id&#39;: 50, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 1, &#39;id&#39;: 51, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;The&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 10, &#39;id&#39;: 52, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;mechanism&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 53, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 54, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 55, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;action&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 56, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;in&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 1, &#39;id&#39;: 57, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 58, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;intestines&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;aux&#39;, &#39;head&#39;: 3, &#39;id&#39;: 59, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;has&#39;, &#39;tag&#39;: &#39;VBZ&#39;}, {&#39;dep&#39;: &#39;neg&#39;, &#39;head&#39;: 2, &#39;id&#39;: 60, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;not&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 61, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;been&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 62, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;investigated&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 63, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;mark&#39;, &#39;head&#39;: 1, &#39;id&#39;: 64, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;To&#39;, &#39;tag&#39;: &#39;TO&#39;}, {&#39;dep&#39;: &#39;advcl&#39;, &#39;head&#39;: 11, &#39;id&#39;: 65, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;gain&#39;, &#39;tag&#39;: &#39;VB&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 2, &#39;id&#39;: 66, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;a&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 67, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;better&#39;, &#39;tag&#39;: &#39;JJR&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -3, &#39;id&#39;: 68, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;understanding&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 4, &#39;id&#39;: 69, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 3, &#39;id&#39;: 70, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 2, &#39;id&#39;: 71, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 72, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;antispasmodic&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -5, &#39;id&#39;: 73, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;effect&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 2, &#39;id&#39;: 74, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;nsubj&#39;, &#39;head&#39;: 1, &#39;id&#39;: 75, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;we&#39;, &#39;tag&#39;: &#39;PRP&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 76, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;investigated&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;nmod:poss&#39;, &#39;head&#39;: 2, &#39;id&#39;: 77, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;its&#39;, &#39;tag&#39;: &#39;PRP$&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 78, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;pharmacological&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -3, &#39;id&#39;: 79, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;effects&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 4, &#39;id&#39;: 80, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;in&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 3, &#39;id&#39;: 81, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 2, &#39;id&#39;: 82, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;guinea&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 83, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;pig&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -5, &#39;id&#39;: 84, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;ileum&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -9, &#39;id&#39;: 85, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 86, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;Terminal&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 5, &#39;id&#39;: 87, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;portions&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 88, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 1, &#39;id&#39;: 89, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 90, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;ileum&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 91, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;were&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 92, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;mounted&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 3, &#39;id&#39;: 93, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;for&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 2, &#39;id&#39;: 94, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;isotonic&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 95, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contraction&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -4, &#39;id&#39;: 96, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;recordings&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -5, &#39;id&#39;: 97, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 1, &#39;id&#39;: 98, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;The&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 4, &#39;id&#39;: 99, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;effect&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 100, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 101, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 102, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;was&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 103, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;compared&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 104, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;with&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 105, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;that&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 9, &#39;id&#39;: 106, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 8, &#39;id&#39;: 107, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 7, &#39;id&#39;: 108, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;classical&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 2, &#39;id&#39;: 109, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;calcium&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 110, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;channel&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 4, &#39;id&#39;: 111, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;blocker&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 1, &#39;id&#39;: 112, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(&#39;, &#39;tag&#39;: &#39;-LRB-&#39;}, {&#39;dep&#39;: &#39;appos&#39;, &#39;head&#39;: -2, &#39;id&#39;: 113, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;CCB&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 114, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -10, &#39;id&#39;: 115, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;verapamil&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -13, &#39;id&#39;: 116, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 4, &#39;id&#39;: 117, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;In&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 3, &#39;id&#39;: 118, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;isolated&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 2, &#39;id&#39;: 119, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;ileal&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 120, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;smooth&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: 5, &#39;id&#39;: 121, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;muscle&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 4, &#39;id&#39;: 122, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;nsubj&#39;, &#39;head&#39;: 3, &#39;id&#39;: 123, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;aux&#39;, &#39;head&#39;: 2, &#39;id&#39;: 124, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;did&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;neg&#39;, &#39;head&#39;: 1, &#39;id&#39;: 125, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;not&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 126, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;produce&#39;, &#39;tag&#39;: &#39;VB&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 127, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;direct&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 128, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contractile&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 129, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;or&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 130, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;relaxation&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;dep&#39;, &#39;head&#39;: -3, &#39;id&#39;: 131, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;responses&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -6, &#39;id&#39;: 132, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;aux&#39;, &#39;head&#39;: 2, &#39;id&#39;: 133, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;did&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;neg&#39;, &#39;head&#39;: 1, &#39;id&#39;: 134, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;not&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -9, &#39;id&#39;: 135, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;modify&#39;, &#39;tag&#39;: &#39;VB&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 136, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;electrically&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 137, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;elicited&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -3, &#39;id&#39;: 138, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contractions&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 139, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;or&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 2, &#39;id&#39;: 140, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;low&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 1, &#39;id&#39;: 141, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;K(+)-evoked&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -4, &#39;id&#39;: 142, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contractions&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -17, &#39;id&#39;: 143, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 2, &#39;id&#39;: 144, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;The&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 145, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;submaximal&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 20, &#39;id&#39;: 146, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contractions&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;acl&#39;, &#39;head&#39;: -1, &#39;id&#39;: 147, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;induced&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 148, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;by&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 149, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;histamine&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 1, &#39;id&#39;: 150, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(&#39;, &#39;tag&#39;: &#39;-LRB-&#39;}, {&#39;dep&#39;: &#39;appos&#39;, &#39;head&#39;: -2, &#39;id&#39;: 151, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;p&lt;0.001&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 152, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -4, &#39;id&#39;: 153, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -5, &#39;id&#39;: 154, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;BaCl2&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 1, &#39;id&#39;: 155, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(&#39;, &#39;tag&#39;: &#39;-LRB-&#39;}, {&#39;dep&#39;: &#39;appos&#39;, &#39;head&#39;: -2, &#39;id&#39;: 156, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;p&lt;0.05&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 157, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -9, &#39;id&#39;: 158, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -10, &#39;id&#39;: 159, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -11, &#39;id&#39;: 160, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;carbachol&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 1, &#39;id&#39;: 161, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(&#39;, &#39;tag&#39;: &#39;-LRB-&#39;}, {&#39;dep&#39;: &#39;appos&#39;, &#39;head&#39;: -2, &#39;id&#39;: 162, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;p&lt;0.01&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -1, &#39;id&#39;: 163, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 2, &#39;id&#39;: 164, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;were&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 165, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;significantly&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 166, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;reduced&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 167, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;by&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 168, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -3, &#39;id&#39;: 169, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 2, &#39;id&#39;: 170, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;The&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 171, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contractile&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 8, &#39;id&#39;: 172, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;response&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;acl&#39;, &#39;head&#39;: -1, &#39;id&#39;: 173, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;elicited&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 174, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;by&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 175, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;high&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 176, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;concentrations&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 177, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 178, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;carbachol&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 179, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;was&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 180, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;reduced&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 181, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;but&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;neg&#39;, &#39;head&#39;: 1, &#39;id&#39;: 182, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;not&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -3, &#39;id&#39;: 183, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;abolished&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 184, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;by&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 185, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -6, &#39;id&#39;: 186, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;neg&#39;, &#39;head&#39;: 2, &#39;id&#39;: 187, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;No&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 188, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;additive&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 2, &#39;id&#39;: 189, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;action&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 190, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;was&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 191, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;detected&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 192, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;with&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 193, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;co-incubation&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 194, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 195, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 196, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 197, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;verapamil&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 198, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;on&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 199, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;carbachol-induced&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -7, &#39;id&#39;: 200, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contraction&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -10, &#39;id&#39;: 201, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;nsubj&#39;, &#39;head&#39;: 1, &#39;id&#39;: 202, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-Carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 203, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;reduced&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 1, &#39;id&#39;: 204, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;the&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 205, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;contraction&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;acl&#39;, &#39;head&#39;: -1, &#39;id&#39;: 206, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;induced&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 2, &#39;id&#39;: 207, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;by&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 208, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;high&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -3, &#39;id&#39;: 209, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;K(+&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -7, &#39;id&#39;: 210, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;)&#39;, &#39;tag&#39;: &#39;-RRB-&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -8, &#39;id&#39;: 211, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;cop&#39;, &#39;head&#39;: 5, &#39;id&#39;: 212, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;was&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 213, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;almost&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;nummod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 214, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;100&#39;, &#39;tag&#39;: &#39;CD&#39;}, {&#39;dep&#39;: &#39;nmod:npmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 215, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;times&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 216, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;more&#39;, &#39;tag&#39;: &#39;RBR&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -14, &#39;id&#39;: 217, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;potent&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 218, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;than&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 219, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;verapamil&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -17, &#39;id&#39;: 220, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;advmod&#39;, &#39;head&#39;: 3, &#39;id&#39;: 221, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;Thus&#39;, &#39;tag&#39;: &#39;RB&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: 2, &#39;id&#39;: 222, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;,&#39;, &#39;tag&#39;: &#39;,&#39;}, {&#39;dep&#39;: &#39;nsubj&#39;, &#39;head&#39;: 1, &#39;id&#39;: 223, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 224, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;showed&#39;, &#39;tag&#39;: &#39;VBD&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 5, &#39;id&#39;: 225, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;a&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 4, &#39;id&#39;: 226, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;typical&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 227, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 228, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;potent&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 229, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;CCB-like&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;dobj&#39;, &#39;head&#39;: -6, &#39;id&#39;: 230, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;action&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -7, &#39;id&#39;: 231, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}, {&#39;brackets&#39;: [], &#39;tokens&#39;: [{&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 232, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;Many&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nsubjpass&#39;, &#39;head&#39;: 10, &#39;id&#39;: 233, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;effects&#39;, &#39;tag&#39;: &#39;NNS&#39;}, {&#39;dep&#39;: &#39;acl&#39;, &#39;head&#39;: -1, &#39;id&#39;: 234, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;described&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 5, &#39;id&#39;: 235, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;for&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;cc:preconj&#39;, &#39;head&#39;: 1, &#39;id&#39;: 236, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;both&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;compound&#39;, &#39;head&#39;: 3, &#39;id&#39;: 237, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;(-)-carvone&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;cc&#39;, &#39;head&#39;: -1, &#39;id&#39;: 238, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;and&#39;, &#39;tag&#39;: &#39;CC&#39;}, {&#39;dep&#39;: &#39;conj&#39;, &#39;head&#39;: -2, &#39;id&#39;: 239, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;spearmint&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -6, &#39;id&#39;: 240, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;oil&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;aux&#39;, &#39;head&#39;: 2, &#39;id&#39;: 241, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;can&#39;, &#39;tag&#39;: &#39;MD&#39;}, {&#39;dep&#39;: &#39;auxpass&#39;, &#39;head&#39;: 1, &#39;id&#39;: 242, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;be&#39;, &#39;tag&#39;: &#39;VB&#39;}, {&#39;dep&#39;: &#39;ROOT&#39;, &#39;head&#39;: 0, &#39;id&#39;: 243, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;explained&#39;, &#39;tag&#39;: &#39;VBN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 3, &#39;id&#39;: 244, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;as&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;det&#39;, &#39;head&#39;: 2, &#39;id&#39;: 245, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;a&#39;, &#39;tag&#39;: &#39;DT&#39;}, {&#39;dep&#39;: &#39;amod&#39;, &#39;head&#39;: 1, &#39;id&#39;: 246, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;CCB-like&#39;, &#39;tag&#39;: &#39;JJ&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -4, &#39;id&#39;: 247, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;mode&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;case&#39;, &#39;head&#39;: 1, &#39;id&#39;: 248, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;of&#39;, &#39;tag&#39;: &#39;IN&#39;}, {&#39;dep&#39;: &#39;nmod&#39;, &#39;head&#39;: -2, &#39;id&#39;: 249, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;action&#39;, &#39;tag&#39;: &#39;NN&#39;}, {&#39;dep&#39;: &#39;punct&#39;, &#39;head&#39;: -7, &#39;id&#39;: 250, &#39;ner&#39;: &#39;O&#39;, &#39;orth&#39;: &#39;.&#39;, &#39;tag&#39;: &#39;.&#39;}]}]}]} . .",
            "url": "https://mani2106.github.io/Blog-Posts/take-home/deep-learning/spacy/nlp/2021/04/26/chemdNER.html",
            "relUrl": "/take-home/deep-learning/spacy/nlp/2021/04/26/chemdNER.html",
            "date": " • Apr 26, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Calculus - d2l.ai Exercises - Part 4",
            "content": ". Required Imports . !pip install d2l . . %matplotlib inline import numpy as np from IPython import display from d2l import tensorflow as d2l . . Setup for plotting . def use_svg_display(): &quot;&quot;&quot;Use the svg format to display a plot in Jupyter.&quot;&quot;&quot; display.set_matplotlib_formats(&#39;svg&#39;) def set_figsize(figsize=(3.5, 2.5)): &quot;&quot;&quot;Set the figure size for matplotlib.&quot;&quot;&quot; use_svg_display() d2l.plt.rcParams[&#39;figure.figsize&#39;] = figsize def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend): &quot;&quot;&quot;Set the axes for matplotlib.&quot;&quot;&quot; axes.set_xlabel(xlabel) axes.set_ylabel(ylabel) axes.set_xscale(xscale) axes.set_yscale(yscale) axes.set_xlim(xlim) axes.set_ylim(ylim) if legend: axes.legend(legend) axes.grid() def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None, ylim=None, xscale=&#39;linear&#39;, yscale=&#39;linear&#39;, fmts=(&#39;-&#39;, &#39;m--&#39;, &#39;g-.&#39;, &#39;r:&#39;), figsize=(3.5, 2.5), axes=None): &quot;&quot;&quot;Plot data points.&quot;&quot;&quot; if legend is None: legend = [] set_figsize(figsize) axes = axes if axes else d2l.plt.gca() # Return True if `X` (tensor or list) has 1 axis def has_one_axis(X): return (hasattr(X, &quot;ndim&quot;) and X.ndim == 1 or isinstance(X, list) and not hasattr(X[0], &quot;__len__&quot;)) if has_one_axis(X): X = [X] if Y is None: X, Y = [[]] * len(X), X elif has_one_axis(Y): Y = [Y] if len(X) != len(Y): X = X * len(Y) axes.cla() for x, y, fmt in zip(X, Y, fmts): if len(x): axes.plot(x, y, fmt) else: axes.plot(y, fmt) set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend) . . Question 1: Plot the function $y=f(x)=x^3&#8722;(1/x)$ and its tangent line when $x=1$. . The derivative of $f(x)$ here is $f&#39;(x) = 3x^2 + 1/x^2$ and by setting $x=1$, we get 4, let&#39;s try to simulate like they did. . def f(x): return x**3 - (x ** -1) def numerical_lim(f, x, h): return (f(x + h) - f(x)) / h . h = 0.1 for i in range(10): print(f&#39;h={h:.10f}, numerical limit={numerical_lim(f, 1, h):.10f}&#39;) h *= 0.1 . h=0.1000000000, numerical limit=4.2190909091 h=0.0100000000, numerical limit=4.0201990099 h=0.0010000000, numerical limit=4.0020019990 h=0.0001000000, numerical limit=4.0002000200 h=0.0000100000, numerical limit=4.0000200002 h=0.0000010000, numerical limit=4.0000019997 h=0.0000001000, numerical limit=4.0000002022 h=0.0000000100, numerical limit=3.9999999868 h=0.0000000010, numerical limit=4.0000003310 h=0.0000000001, numerical limit=4.0000003310 . . We do see that the value approaches to 4. So $y=4$ when $x=1$, to find the equation of the tangent line to $x^3−(1/x)$ at $x=1$ . We already have the slope of the tangent line by substituting $x=1$ on $f&#39;(x)$ which is $4$, since a tangent line shares atleast one point with the original equation($f(x)$), substituting the $x$ in $f(x)$ we get $0$. . So the common(shared) point between $f(x)$ and its tangent at $x=1$ is $(1,0)$, we can use the slope $4$ and the point $(1,0)$ to find the equation of the tangent line at $x=1$ with the formula: . $$y - y_1 = m(x-x_1)$$ . where the $m$ is the slope and $x_1$ and $y_1$ are coordinates from the point which we found. Plugging in the values which we have into the equation we get the equation . $$ y = 4x - 4 $$ . This is a good reference for another example of this method. . Let&#39;s see if it is the tangent line by visualising it . x = np.arange(0.1, 3, 0.1) plot(x, [f(x), 4 * x - 4], &#39;x&#39;, &#39;f(x)&#39;, legend=[&#39;f(x)&#39;, &#39;Tangent line (x=1)&#39;]) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Question 2: Find the gradient of the function $f(x) = 3x_1^2 + 5e^{x_2}$ . For gradient we need to calculate the partial derivatives of the function given with respect to its variables. (ie) $ frac{ partial y}{ partial x_1}$ and $ frac{ partial y}{ partial x_2}$ . $$ frac{ partial y}{ partial x_1} = 6x_1 $$ . $$ frac{ partial y}{ partial x_2} = 5e^{x_2}$$ . so the gradient will be $$ nabla_xf(x) = [6x_1, 5e^{x_2}]$$ . Question 3: Find the gradient of $f(x) = &#8741;x&#8741;_2$. . For this function, it can be split into $ sqrt{x^Tx}$. Let&#39;s consider $u = x^Tx$ then our function would become $f(x) = sqrt{u} $ or $u^{1/2}$ . So now we have two variables $u$ and $x$, the gradient of the function would be $$ frac{ partial y} { partial u} . frac{ partial y} { partial x}$$ $$ frac{ partial y} { partial u} = (1/2)*(u)^{-1/2} = frac{1}{2 sqrt u} = frac{1}{2 ||x||_2}$$ $$ frac{ partial y} { partial x} = frac { partial x^Tx} { partial x} = 2x$$ $$ nabla_xf(x) = frac{ partial y} { partial u} . frac{ partial y} { partial x}$$ . $$ = frac{1}{2 ||x||_2} . 2x = frac {x}{||x||_2}$$ . Question 4: Can you write out the chain rule for the case where $u=f(x,y,z)$ and $x=x(a,b) , y=y(a,b)$ and $z=z(a,b) $? . We need to treat $u$ as a function with variables $x$, $y$ and $z$ and each of $x,y,z$ as functions with variables $a$ and $b$ . so by applying chain rule we get . $$ frac{ partial u}{ partial a} = frac{ partial u}{ partial x} frac{ partial x}{ partial a}+ frac{ partial u}{ partial y} frac{ partial y}{ partial a}+ frac{ partial u}{ partial z} frac{ partial z}{ partial a}$$ $$ frac{ partial u}{ partial a} = frac{ partial u}{ partial x} frac{ partial x}{ partial b} + frac{ partial u}{ partial y} frac{ partial y}{ partial b}+ frac{ partial u}{ partial z} frac{ partial z}{ partial b}$$ .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/04/25/calculus_nb.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/04/25/calculus_nb.html",
            "date": " • Apr 25, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Recommending a product price for a seller",
            "content": "This blog post has the code and details, of the take home project that I did for a company as part of their interview process, they gave me a dataset with sales details of a clothes/accessories from their website, the task was to make a price prediction/suggestion for a piece of wardrobe that the customer would like to sell on their company&#39;s platform. . Prepare data and libraries . !unzip /content/ds-take-home-dataset.zip . Archive: /content/ds-take-home-dataset.zip inflating: ds-take-home-dataset.csv . !pip install -U scikit-learn . Collecting scikit-learn Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB) |████████████████████████████████| 6.8MB 5.3MB/s Collecting threadpoolctl&gt;=2.0.0 Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl Requirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1) Requirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.18.5) Requirement already satisfied, skipping upgrade: joblib&gt;=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.16.0) Installing collected packages: threadpoolctl, scikit-learn Found existing installation: scikit-learn 0.22.2.post1 Uninstalling scikit-learn-0.22.2.post1: Successfully uninstalled scikit-learn-0.22.2.post1 Successfully installed scikit-learn-0.23.2 threadpoolctl-2.1.0 . Import Required Libraries . import json import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import RobustScaler import matplotlib.pyplot as plt from joblib import dump import os import json . . file_path = r&#39;/content/drive/My Drive/poshmark_files&#39; . . np.random.seed(0) . . Explore the dataset . id attr1 attr2 attr3 attr4 attr5 attr6 title sold_price . 0 742122 | 4 | 27 | 149.0 | 3808.0 | 1.0 | 99.0 | one teaspoon bandit distressed denim shorts (23) | 65.0 | . 1 652751 | 4 | 3 | 89.0 | 1996.0 | NaN | 1500.0 | gucci emily mini guccissima mini red leather bag | 600.0 | . 2 228229 | 4 | 26 | 301.0 | 5194.0 | NaN | 89.0 | steve madden polka dot wedges 8.5 - wi06 | 12.0 | . 3 645810 | 4 | 27 | NaN | 6335.0 | NaN | 0.0 | crown &amp; ivy navy blue floral print shorts | 12.0 | . 4 854374 | 4 | 22 | 302.0 | 3606.0 | 2.0 | 45.0 | grey wide leg dress pants | 22.0 | . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 1029850 entries, 0 to 1029849 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 id 1029850 non-null int64 1 attr1 1029850 non-null int64 2 attr2 1029850 non-null int64 3 attr3 843080 non-null float64 4 attr4 864417 non-null float64 5 attr5 495664 non-null float64 6 attr6 1029850 non-null float64 7 title 1029761 non-null object 8 sold_price 1029850 non-null float64 dtypes: float64(5), int64(3), object(1) memory usage: 70.7+ MB . Rows: 1029850 and cols: 9 . Number of Listing Id labels: 999850 Number of Listings in the Dataset: 1029850 . id attr1 attr2 attr3 attr4 attr5 attr6 sold_price . count 1.029850e+06 | 1.029850e+06 | 1.029850e+06 | 843080.000000 | 864417.000000 | 495664.000000 | 1.029850e+06 | 1.029850e+06 | . mean 4.999636e+05 | 3.792792e+00 | 1.611883e+01 | 170.677981 | 3026.511901 | 1.452339 | 4.572187e+09 | 1.959587e+02 | . std 2.886351e+05 | 5.353355e-01 | 1.087543e+01 | 91.907382 | 1842.932124 | 0.497728 | 4.594750e+12 | 1.440187e+03 | . min 1.000000e+00 | 1.000000e+00 | 1.000000e+00 | 1.000000 | 1.000000 | 1.000000 | 0.000000e+00 | 0.000000e+00 | . 25% 2.500282e+05 | 4.000000e+00 | 3.000000e+00 | 89.000000 | 1453.000000 | 1.000000 | 2.700000e+01 | 1.500000e+01 | . 50% 4.999745e+05 | 4.000000e+00 | 1.400000e+01 | 180.000000 | 2896.000000 | 1.000000 | 1.190000e+02 | 5.000000e+01 | . 75% 7.499088e+05 | 4.000000e+00 | 2.600000e+01 | 252.000000 | 4425.000000 | 2.000000 | 6.000000e+02 | 2.790000e+02 | . max 9.998500e+05 | 4.000000e+00 | 3.500000e+01 | 306.000000 | 6640.000000 | 3.000000 | 4.662595e+15 | 8.979010e+05 | . There are missing values in some columns, so let&#39;s start by analysing them first. . Target Distribution . data[data[&#39;sold_price&#39;] &gt; 0].shape[0] . . 1009850 . data[data[&#39;sold_price&#39;] == 0].shape[0] . . 20000 . There are 20000 listings with no price details, we might be better off removing them . Missing value analysis . vars_with_na = [var for var in data.columns if data[var].isnull().sum() &gt; 0] # get percentage of missing values data[vars_with_na].isnull().mean() . attr3 0.181357 attr4 0.160638 attr5 0.518703 title 0.000086 dtype: float64 . There is a sizeable chunk of missing data in feature attrs5 Let&#39;s try to see whether they influence the price of a listing. . def analyse_na_value(df, var): df = df.copy() # let&#39;s make a variable that indicates 1 if the observation was missing or zero otherwise df[var] = np.where(df[var].isnull(), 1, 0) # let&#39;s compare the median sold_price in the observations where data is missing # vs the observations where a value is available df.groupby(var)[&#39;sold_price&#39;].median().plot.bar() plt.title(var) plt.show() for var in vars_with_na: analyse_na_value(data, var) . . There is a change in the value of the sold_price when some attributes are empty, we should try to input this relationship when performing feature engineering . Almost all the features we have are categorical according to the instuctions given, let us try to analyze the only numeric variable attr6 . Continous variable analysis . data[&#39;attr6&#39;].describe() . . count 1.029850e+06 mean 4.572187e+09 std 4.594750e+12 min 0.000000e+00 25% 2.700000e+01 50% 1.190000e+02 75% 6.000000e+02 max 4.662595e+15 Name: attr6, dtype: float64 . var = &#39;attr6&#39; data[var].hist(bins=range(1, 10)) plt.ylabel(&#39;Number of listings&#39;) plt.xlabel(var) plt.title(var) plt.show() . . We can see that the variable is not normally distributed, We should remember to deal with this. . Unique value analysis . The feature attr6 is an interesting feature (Which can be clarified if it had a name) because the 20% values of that feature is Zero. This causes a confusion as to whether it is a categorical or numerical variable. A pie chart to view the counts. . data[&#39;attr6&#39;].value_counts().plot(kind=&#39;pie&#39;) . . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f49d1fe1be0&gt; . Percentage of zeros in attr6 . 0.2049288731368646 . Percentage of non-zeros in attr6 . 0.7950711268631354 . Categorical variable analysis . As per the instructions the categorical variables are . cat_vars =&#39;attr1 attr2 attr3 attr4 attr5&#39;.split(&#39; &#39;) . Let&#39;s check the cardinality . attr1 4 attr2 35 attr3 306 attr4 6640 attr5 3 dtype: int64 . We do have huge number of categories in attr3 and attr4, let&#39;s see about the rare labels with respect to the sold prices of the listing. . def analyse_rare_labels(df, var, rare_perc): df = df.copy() # determine the % of observations per category tmp = df.groupby(var)[&#39;sold_price&#39;].count() / len(df) # return categories that are rare return tmp[tmp &lt; rare_perc] # print categories that are present in less than # .1 % of the observations for var in cat_vars: print(analyse_rare_labels(data, var, 0.001)) print() . . Series([], Name: sold_price, dtype: float64) attr2 4 0.000229 5 0.000979 7 0.000657 8 0.000653 10 0.000738 18 0.000559 23 0.000122 29 0.000257 34 0.000841 35 0.000574 Name: sold_price, dtype: float64 attr3 1.0 0.000804 2.0 0.000156 5.0 0.000304 6.0 0.000084 7.0 0.000766 ... 297.0 0.000006 299.0 0.000102 303.0 0.000664 305.0 0.000013 306.0 0.000770 Name: sold_price, Length: 197, dtype: float64 attr4 1.0 5.826091e-06 2.0 1.340001e-04 3.0 2.233335e-05 4.0 4.757974e-05 5.0 9.710152e-07 ... 6636.0 2.126523e-04 6637.0 2.524640e-05 6638.0 5.826091e-06 6639.0 9.710152e-07 6640.0 2.913046e-06 Name: sold_price, Length: 6510, dtype: float64 attr5 3.0 9.710152e-07 Name: sold_price, dtype: float64 . attr4 has a lot of rare labels 6510 (98% of total unique labels) . We should try relating each category with the sold_price . def analyse_discrete(df, var): df = df.copy() df.groupby(var)[&#39;sold_price&#39;].median().plot.bar() plt.title(var) plt.ylabel(&#39;Median sold_price&#39;) plt.show() for var in cat_vars: analyse_discrete(data, var) . . Outlier Analysis . # let&#39;s make boxplots to visualise outliers in the continuous variables def find_outliers(df, var): df = df.copy() df[var] = np.log1p(df[var]) df.boxplot(column=var) plt.title(var) plt.ylabel(var) plt.show() for var in [&#39;attr6&#39;]: find_outliers(data, var) . . Feature Engineering . Import libraries for Feature engineering . from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.preprocessing import OneHotEncoder . Before we do the actual feature engineering, let&#39;s split the data to train and test sets. . Since we need to be able to evaluate the model based on the bins of sold_price like 0-50, 50-100, 100-500, 500-1000 and 1000+., Let&#39;s stratify the splitting by creating bins of the target variable . Avoid zero price listings . Train test split . zero_filter = data[&#39;sold_price&#39;] &gt; 0 . bins = np.linspace(0, 1000, 10) y_binned = np.digitize(data.loc[zero_filter, &#39;sold_price&#39;], bins) . X_train, X_test, y_train, y_test = train_test_split(data[zero_filter].copy(), data.loc[zero_filter, &#39;sold_price&#39;].copy(), test_size=0.3, stratify=y_binned) . bins . array([ 0. , 111.11111111, 222.22222222, 333.33333333, 444.44444444, 555.55555556, 666.66666667, 777.77777778, 888.88888889, 1000. ]) . pd.Series(y_binned).value_counts() . 1 570573 3 144418 2 104505 4 71537 5 44670 10 24617 6 19932 7 13618 8 9224 9 6756 dtype: int64 . Missing values . Let&#39;s check the percentage of missing values in the data . # make a list of the variables that contain missing values vars_with_na = [var for var in X_train.columns if data[var].isnull().sum() &gt; 0] # get percentage of missing values X_train[vars_with_na].isnull().mean() . . attr3 0.181816 attr4 0.160420 attr5 0.518667 title 0.000089 dtype: float64 . # make a list of the variables that contain missing values vars_with_na = [var for var in X_test.columns if data[var].isnull().sum() &gt; 0] # get percentage of missing values X_test[vars_with_na].isnull().mean() . . attr3 0.180113 attr4 0.161057 attr5 0.518562 title 0.000083 dtype: float64 . All the features with missing variables are categorical/string variables, so let&#39;s just assign some arbitrary value missing to them. . # and with an arbitrary number for others miss_cat = [&#39;attr3&#39;, &#39;attr4&#39;, &#39;attr5&#39;] X_train[miss_cat] = X_train[miss_cat].fillna(-9999) X_test[miss_cat] = X_test[miss_cat].fillna(-9999) X_train[&#39;title&#39;] = X_train[&#39;title&#39;].fillna(&#39;No Value Present&#39;) X_test[&#39;title&#39;] = X_test[&#39;title&#39;].fillna(&#39;No Value Present&#39;) . As per the instruction the variable Id is kind of a temporal variable indicating the time in which the item was sold. So we can see if it helps with the prediction . Our only numeric variable as per the instruction is attr6 and it has no missing values so we do not need to touch that for now. . Handle infrequent/rare labels . def find_frequent_labels(df, var, rare_perc): # function finds the labels that are shared by more than # a certain % of the listings in the dataset df = df.copy() tmp = df.groupby(var)[&#39;sold_price&#39;].count() / len(df) return tmp[tmp &gt; rare_perc].index for var in cat_vars: # find the frequent categories frequent_ls = find_frequent_labels(X_train, var, 0.001) # replace rare categories by an arbitrary number X_train[var] = np.where(X_train[var].isin( frequent_ls), X_train[var], -999) X_test[var] = np.where(X_test[var].isin( frequent_ls), X_test[var], -999) . . # so that the smaller value corresponds to the category that shows the smaller # mean sold_price def replace_categories(train, test, var, target): # order the categories in a variable from that with the lowest # house sale price, to that with the highest ordered_labels = train.groupby([var])[target].mean().sort_values().index # create a dictionary of ordered categories to integer values ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)} with open(f&#39;{var}.json&#39;, &#39;w&#39;) as f: json.dump(ordinal_label, f) # use the dictionary to replace the categorical strings by integers train[var] = train[var].map(ordinal_label) test[var] = test[var].map(ordinal_label) . cat_vars . [&#39;attr1&#39;, &#39;attr2&#39;, &#39;attr3&#39;, &#39;attr4&#39;, &#39;attr5&#39;] . for var in cat_vars: replace_categories(X_train, X_test, var, &#39;sold_price&#39;) . # between labels and target def analyse_vars(df, var): # function plots median sold price per encoded category df = df.copy() df.groupby(var)[&#39;sold_price&#39;].median().plot.bar() plt.title(var) plt.ylabel(&#39;sold_price&#39;) plt.show() for var in cat_vars: analyse_vars(X_train, var) . Feature scaling . num_vars = [&#39;attr6&#39;, &#39;id&#39;] numeric_transformer = Pipeline(steps=[ (&#39;scaler&#39;, RobustScaler())] ) . Text Feature Extraction . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import TruncatedSVD . text_transformer = Pipeline( steps=[ (&#39;tfidf&#39;, TfidfVectorizer()), (&#39;best&#39;, TruncatedSVD(n_components=5)) ] ) preprocessor = ColumnTransformer( transformers=[ (&#39;num&#39;, numeric_transformer, num_vars), (&#39;text&#39;, text_transformer, &#39;title&#39;) ], remainder=&#39;passthrough&#39; ) . Model Building . Let&#39;s look at the target data in a scatter plot . plt.scatter(x=range(len(y_train)), y=y_train) . . &lt;matplotlib.collections.PathCollection at 0x7f3b299107b8&gt; . The target has outliers, So transforming the target might help the regression model to fit the data . Import required things for modeling . from sklearn.compose import TransformedTargetRegressor from sklearn.svm import SVR from sklearn.linear_model import Lasso, LassoLarsIC, RidgeCV, LinearRegression, RANSACRegressor, SGDRegressor from sklearn.preprocessing import QuantileTransformer, quantile_transform # to evaluate the model from sklearn.metrics import mean_squared_error, r2_score, median_absolute_error from math import sqrt . Target transformation with Quantiles . The snippet is more or less copied from scikit-learn&#39;s example gallery, this graph shows the effect of transforming the target with quantiles. . density_param = {&#39;density&#39;:True} y = X_train.loc[:, &#39;sold_price&#39;].squeeze() y_trans = quantile_transform(X_train.loc[:, &#39;sold_price&#39;].values.reshape(-1, 1), n_quantiles=300, output_distribution=&#39;normal&#39;, copy=True).squeeze() f, (ax0, ax1) = plt.subplots(1, 2) ax0.hist(y, bins=100, **density_param) ax0.set_ylabel(&#39;Probability&#39;) ax0.set_xlabel(&#39;Target&#39;) ax0.set_title(&#39;Target distribution&#39;) ax1.hist(y_trans, bins=100, **density_param) ax1.set_ylabel(&#39;Probability&#39;) ax1.set_xlabel(&#39;Target&#39;) ax1.set_title(&#39;Transformed target distribution&#39;) f.suptitle(&quot;Listing data: distance to price centers&quot;, y=0.035) f.tight_layout(rect=[0.05, 0.05, 0.95, 0.95]) . . Put together the feature engineering pipeline and model . regr_trans = TransformedTargetRegressor(regressor=LassoLarsIC(normalize=False), transformer=QuantileTransformer(output_distribution=&#39;normal&#39;)) . model = Pipeline(steps=[(&#39;preprocessor&#39;, preprocessor), (&#39;regressor&#39;, regr_trans)], memory=&#39;/content&#39; ) . Begin modeling . Model using regularized linear regression . model.fit(X_train, y_train) . Model Structure . Try predicting on train and test data to evaluate fit . Evaluation . Let&#39;s have a look at the mean square and median absolute errors of the train and test sets. . # We will evaluate performance using the mean squared error and # the root of the mean squared error and r2 # make predictions for train set pred = model.predict(X_train) # determine mse and rmse print(&#39;train mse: {}&#39;.format(int( mean_squared_error(y_train, pred)))) print(&#39;train rmse: {}&#39;.format(int( sqrt(mean_squared_error(y_train, pred))))) print(&#39;train r2: {}&#39;.format( r2_score(y_train, pred))) print() # make predictions for test set pred = model.predict(X_test) # determine mse and rmse print(&#39;test mse: {}&#39;.format(int( mean_squared_error(y_test, pred)))) print(&#39;test rmse: {}&#39;.format(int( sqrt(mean_squared_error(y_test, pred))))) print(&#39;test r2: {}&#39;.format( r2_score(y_test, pred))) print() print(&#39;Median listing price: &#39;, int(y_train.median())) . . train mse: 2384016 train rmse: 1544 train r2: 0.012738801579374459 test mse: 1420721 test rmse: 1191 test r2: -0.005001811084259344 Median listing price: 60 . # We will evaluate performance using the median abs error and # the root of the mean squared error and r2 # make predictions for train set pred = model.predict(X_train) # determine mae print(&#39;train mae: {}&#39;.format(int( median_absolute_error(y_train, pred)))) print(&#39;train r2: {}&#39;.format( r2_score(y_train, pred))) print() # make predictions for test set pred = model.predict(X_test) # determine mae print(&#39;test mae: {}&#39;.format(int( median_absolute_error(y_test, pred)))) print(&#39;test r2: {}&#39;.format( r2_score(y_test, pred))) print() print(&#39;Median listing price: &#39;, int(y_train.median())) . . train mae: 37 train r2: 0.012738801579374459 test mae: 37 test r2: -0.005001811084259344 Median listing price: 60 . It is kinda evident that the model has not performed well(looking at the $r^2$ values) maybe I should have used another modeling method to get better results. . Let&#39;s evaluate our predictions respect to the sold price, In the picture, The blue dots are predicted values and the red ones are the predicted values. . plt.scatter(range(0, len(y_test)), y_test, color=&#39;red&#39;) plt.scatter(range(0, len(X_test)), model.predict(X_test), color=&#39;blue&#39;) plt.xlabel(&#39;record number&#39;) plt.ylabel(&#39;Actual and Predicted Listing Price&#39;) plt.title(&#39;Evaluation of Lasso Predictions&#39;) . . Text(0.5, 1.0, &#39;Evaluation of Lasso Predictions&#39;) . Error distribution . errors = y_test - model.predict(X_test) errors.hist(bins=30) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f3b29ea7cf8&gt; . The chart is not very informative but the range of numbers in x axis gives an idea of the error range. . I also did another exercise for the same company, which deals with calculating the feature importances in a model agnostic way. You can have look at that here .",
            "url": "https://mani2106.github.io/Blog-Posts/take-home/machine%20learning/tabular/scikit-learn/2021/04/10/price_pred_notebook.html",
            "relUrl": "/take-home/machine%20learning/tabular/scikit-learn/2021/04/10/price_pred_notebook.html",
            "date": " • Apr 10, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "Linear Algebra - d2l.ai Exercises - Part 3",
            "content": "Exercise setup . import tensorflow as tf A = tf.reshape(tf.range(20, dtype=tf.float32), (5,4)) A . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . Problems and answers . Prove that the transpose of a matrix A &#8217;s transpose is A : $(A^{T})^T$=$A$. . This is straightforward, transposing is basically converting rows to columns and vice-versa, so when done twice we would end up what we started with. . At = tf.transpose(A) At . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 0., 4., 8., 12., 16.], [ 1., 5., 9., 13., 17.], [ 2., 6., 10., 14., 18.], [ 3., 7., 11., 15., 19.]], dtype=float32)&gt; . At_t = tf.transpose(At) At_t . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . At_t == A . &lt;tf.Tensor: shape=(5, 4), dtype=bool, numpy= array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])&gt; . Show that the sum of transposes is equal to the transpose of a sum: $A^T+B^T=(A+B)^T$. . Let&#39;s consider a second matrix, $B$ . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 7.975751 , 7.70928 , 8.388272 , 11.001523 ], [ 7.6716766, 11.476339 , 6.2204466, 5.6182394], [ 9.765643 , 6.7869806, 8.873018 , 5.6852665], [ 8.200825 , 4.9842663, 11.172729 , 11.063158 ], [ 8.75681 , 8.760315 , 4.151512 , 5.0749035]], dtype=float32)&gt; . The transpose would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751 , 7.6716766, 9.765643 , 8.200825 , 8.75681 ], [ 7.70928 , 11.476339 , 6.7869806, 4.9842663, 8.760315 ], [ 8.388272 , 6.2204466, 8.873018 , 11.172729 , 4.151512 ], [11.001523 , 5.6182394, 5.6852665, 11.063158 , 5.0749035]], dtype=float32)&gt; . $A^T+B^T$ . The sum of the transposed matrices would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751, 11.671677, 17.765644, 20.200825, 24.75681 ], [ 8.70928 , 16.47634 , 15.786981, 17.984266, 25.760315], [10.388272, 12.220447, 18.873018, 25.17273 , 22.151512], [14.001523, 12.618239, 16.685266, 26.063158, 24.074903]], dtype=float32)&gt; . $(A+B)^T$ . The transposed sum would be . &lt;tf.Tensor: shape=(4, 5), dtype=float32, numpy= array([[ 7.975751, 11.671677, 17.765644, 20.200825, 24.75681 ], [ 8.70928 , 16.47634 , 15.786981, 17.984266, 25.760315], [10.388272, 12.220447, 18.873018, 25.17273 , 22.151512], [14.001523, 12.618239, 16.685266, 26.063158, 24.074903]], dtype=float32)&gt; . $A^T+B^T == (A+B)^T$ ? . &lt;tf.Tensor: shape=(4, 5), dtype=bool, numpy= array([[ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True], [ True, True, True, True, True]])&gt; . All the numbers are equal, we can see that by looking at the results . Given any square matrix $A$ , is $A+A^T$ always symmetric? Why? . Let&#39;s define a square matrix . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.]], dtype=float32)&gt; . The tranpose of the same would be . At = tf.transpose(A) At . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 4., 8., 12.], [ 1., 5., 9., 13.], [ 2., 6., 10., 14.], [ 3., 7., 11., 15.]], dtype=float32)&gt; . The sum of the tensors . &lt;tf.Tensor: shape=(4, 4), dtype=float32, numpy= array([[ 0., 5., 10., 15.], [ 5., 10., 15., 20.], [10., 15., 20., 25.], [15., 20., 25., 30.]], dtype=float32)&gt; . Let&#39;s see if the condition stands . &lt;tf.Tensor: shape=(4, 4), dtype=bool, numpy= array([[ True, True, True, True], [ True, True, True, True], [ True, True, True, True], [ True, True, True, True]])&gt; . I guess since we add the rows and columns of the same matrix and its transpose and also since addition is commutative (ie) $A+B = B+A$, all the numbers we add endup becoming equal in terms of their respective positions, so even tranposing the resultant matrix ends up being equal to the former. . Output of len(X) for tensor $X$ shaped (2, 3, 4) and does len(X) always correspond to the length of a certain axis of $X$? What is that axis? . Let&#39;s consider the following as $X$ . &lt;tf.Tensor: shape=(2, 3, 4), dtype=int32, numpy= array([[[ 0, 1, 2, 3], [ 4, 5, 6, 7], [ 8, 9, 10, 11]], [[12, 13, 14, 15], [16, 17, 18, 19], [20, 21, 22, 23]]], dtype=int32)&gt; . len(X) . 2 . We can see that the length returns the size of the first axis, let us see if it does the same for the other arbitrary tensors . &lt;tf.Tensor: shape=(8, 1, 3), dtype=float32, numpy= array([[[11.439855 , 5.880226 , 5.9797716]], [[11.37106 , 5.619686 , 5.9706793]], [[ 6.4085245, 11.867535 , 4.3086786]], [[ 7.1461754, 8.795105 , 8.864346 ]], [[ 9.952526 , 7.6806755, 7.7797728]], [[10.933958 , 11.748696 , 6.464444 ]], [[ 5.296891 , 6.7806816, 4.316203 ]], [[ 8.316187 , 7.272793 , 9.020613 ]]], dtype=float32)&gt; . len(X) . 8 . &lt;tf.Tensor: shape=(1, 2, 3, 9), dtype=float32, numpy= array([[[[ 5.9411087, 6.242239 , 4.4269447, 7.913884 , 7.8960876, 7.511854 , 6.3407526, 11.290615 , 4.5310717], [ 7.182088 , 5.086608 , 4.0900164, 4.7155457, 8.863187 , 4.1158237, 10.514992 , 9.662274 , 8.8960705], [11.142818 , 6.125886 , 9.6489105, 7.8091097, 9.66531 , 9.282991 , 8.218669 , 11.877634 , 8.727693 ]], [[ 4.3840303, 8.792656 , 9.48595 , 9.231619 , 5.972165 , 11.478173 , 10.220118 , 10.394747 , 4.430291 ], [ 7.198678 , 7.2096577, 5.8975067, 4.6933975, 6.6245346, 11.958464 , 10.320432 , 11.609855 , 7.1605587], [ 6.389407 , 5.9069185, 7.974592 , 5.289855 , 5.713969 , 6.6944523, 4.1094055, 4.077242 , 8.026564 ]]]], dtype=float32)&gt; . len(X) . 1 . No matter what the shape of the tensor len always picks the first/outermost axis. . What happens when we divide $A$ by the sum of it&#39;s second axis? A / A.sum(axis=1) . Let&#39;s define $A$ . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt; . A / tf.reduce_sum(A, axis=1) # This produces an error . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-4-0f11f2009278&gt; in &lt;module&gt; 1 #collapse -&gt; 2 A / tf.reduce_sum(A, axis=1) 3 # This produces an error ~ miniconda3 envs tensor_env lib site-packages tensorflow python ops math_ops.py in binary_op_wrapper(x, y) 1123 with ops.name_scope(None, op_name, [x, y]) as name: 1124 try: -&gt; 1125 return func(x, y, name=name) 1126 except (TypeError, ValueError) as e: 1127 # Even if dispatching the op failed, the RHS may be a tensor aware ~ miniconda3 envs tensor_env lib site-packages tensorflow python util dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a ~ miniconda3 envs tensor_env lib site-packages tensorflow python ops math_ops.py in truediv(x, y, name) 1295 TypeError: If `x` and `y` have different dtypes. 1296 &#34;&#34;&#34; -&gt; 1297 return _truediv_python3(x, y, name) 1298 1299 ~ miniconda3 envs tensor_env lib site-packages tensorflow python ops math_ops.py in _truediv_python3(x, y, name) 1234 x = cast(x, dtype) 1235 y = cast(y, dtype) -&gt; 1236 return gen_math_ops.real_div(x, y, name=name) 1237 1238 ~ miniconda3 envs tensor_env lib site-packages tensorflow python ops gen_math_ops.py in real_div(x, y, name) 7440 return _result 7441 except _core._NotOkStatusException as e: -&gt; 7442 _ops.raise_from_not_ok_status(e, name) 7443 except _core._FallbackException: 7444 pass ~ miniconda3 envs tensor_env lib site-packages tensorflow python framework ops.py in raise_from_not_ok_status(e, name) 6841 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6842 # pylint: disable=protected-access -&gt; 6843 six.raise_from(core._status_to_exception(e.code, message), None) 6844 # pylint: enable=protected-access 6845 ~ miniconda3 envs tensor_env lib site-packages six.py in raise_from(value, from_value) InvalidArgumentError: Incompatible shapes: [5,4] vs. [5] [Op:RealDiv] . . Ok, there seems to be shape inconsistencies to the resultant sum tensor. Let&#39;s see the sum output for the axes in tensor. . tf.reduce_sum(A, axis=1) . &lt;tf.Tensor: shape=(5,), dtype=float32, numpy=array([ 6., 22., 38., 54., 70.], dtype=float32)&gt; . tf.reduce_sum(A, axis=0) . &lt;tf.Tensor: shape=(4,), dtype=float32, numpy=array([40., 45., 50., 55.], dtype=float32)&gt; . So I think, When we sum a tensor on a particular axis, the shape of the resultant tensor will end taking the shape with the other remaining axes, for example a tensor with shape (5, 4 ,3) when summed up along the third axis (2) the resultant tensor would be of shape (5, 4) . The shapes for the tensors summed along the rest of the axes can be understood by the same. . (5, 4, 3) axis = 0 : (4, 3) axis = 1 : (5, 3) axis = 2 : (5, 4) . When we do the division with the other resultant tensor, we can easily divide with it since the shapes follow the broadcasting rules . a - 5 X 4 summed_a - 4 result - 5 X 4 . The following is the result of the division . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[0. , 0.02222222, 0.04 , 0.05454545], [0.1 , 0.11111111, 0.12 , 0.12727273], [0.2 , 0.2 , 0.2 , 0.2 ], [0.3 , 0.2888889 , 0.28 , 0.27272728], [0.4 , 0.37777779, 0.36 , 0.34545454]], dtype=float32)&gt; . A fellow learner suggested to reframe the question here, and said that the following code is what would have been expected by the one who framed the question. . A / tf.reshape(tf.reduce_sum(A,axis=1),(-1,1)) . &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[0. , 0.16666667, 0.33333334, 0.5 ], [0.18181819, 0.22727273, 0.27272728, 0.3181818 ], [0.21052632, 0.23684211, 0.2631579 , 0.28947368], [0.22222222, 0.24074075, 0.25925925, 0.2777778 ], [0.22857143, 0.24285714, 0.25714287, 0.27142859]], dtype=float32)&gt; . Let&#39;s see the shapes and values of the numerator and denominator of the above . Numerator: (5, 4) Denominator: (5, 1) . tf.reshape(tf.reduce_sum(A,axis=1),(-1,1)), A . (&lt;tf.Tensor: shape=(5, 1), dtype=float32, numpy= array([[ 6.], [22.], [38.], [54.], [70.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(5, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.], [12., 13., 14., 15.], [16., 17., 18., 19.]], dtype=float32)&gt;) . So if we compare the values, each value in A has been divided by the summation of each of it&#39;s rows . When traveling between two points in Manhattan, what is the distance that you need to cover in terms of the coordinates, i.e., in terms of avenues and streets? Can you travel diagonally? . A user suggested to look at the streets of manhattan in google maps, to understand the question better,! as suggested the maps picture looked like a big piece of land cut into several small rectangular boxes, very much like 2d coordinate space, where each intersection of street can be treated as an point in that space. . Let&#39;s look at sample screenshot from google maps of manhattan streets. . . The question asks us to find the distance that we need to cover if we need to go from one point to another in terms of streets and avenues. Looks like there is a formula called Manhattan distance for a reason! . The distance between two points measured along axes at right angles. In a plane with p1 at (x1, y1) and p2 at (x2, y2), it is |x1 - x2| + |y1 - y2|. referred from here and there is also a special geometry called taxicab geometry, with manhattan distance as its metric, there are some good images and content to understand better here . if we need to measure the distance in terms of streets and avenues, we need to consider them as dimensions (x(street),y(avenues)), let us consider that we are in the metro point in 72nd street and we need to go to the Jones Wood Foundry in 76th street. . Let&#39;s define them as coordinates, 72nd street, 2nd Avenue -&gt; $(72, 2)$ 76th street, 1st Avenue -&gt; $(76, 1)$ . so according to the formula the answer would be $(4, 1)$ ie 4 streets and 1 avenue, and we don&#39;t travel diagonally unless we are flying. . The summation outputs for tensor with shape (2, 3, 4) along axis 0, 1, and 2. . I think I have answered this in the question about A / A.sum(axis=1), that should apply to the any arbitrary shape, for this one it would turnout to be the following . (2, 3, 4) axis = 0 : (3, 4) axis = 1 : (2, 4) axis = 2 : (2, 3) . Feed a tensor with 3 or more axes to the linalg.norm. . What does this function compute for tensors of arbitrary shape? . Let&#39;s take a 3-d tensor . &lt;tf.Tensor: shape=(2, 2, 5), dtype=float32, numpy= array([[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.]], [[10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]]], dtype=float32)&gt; . tf.norm(X) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=49.699093&gt; . Let&#39;s take an arbitrary shaped tensor . &lt;tf.Tensor: shape=(8, 1, 2, 5), dtype=float32, numpy= array([[[[ 0., 1., 2., 3., 4.], [ 5., 6., 7., 8., 9.]]], [[[10., 11., 12., 13., 14.], [15., 16., 17., 18., 19.]]], [[[20., 21., 22., 23., 24.], [25., 26., 27., 28., 29.]]], [[[30., 31., 32., 33., 34.], [35., 36., 37., 38., 39.]]], [[[40., 41., 42., 43., 44.], [45., 46., 47., 48., 49.]]], [[[50., 51., 52., 53., 54.], [55., 56., 57., 58., 59.]]], [[[60., 61., 62., 63., 64.], [65., 66., 67., 68., 69.]]], [[[70., 71., 72., 73., 74.], [75., 76., 77., 78., 79.]]]], dtype=float32)&gt; . tf.norm(X) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=409.2432&gt; . tf.norm still calculates the square root of squared sum of all numbers in the tensor, equivalent to the following . tf.sqrt( float(sum( [x*x for x in range(80)] )) ) . &lt;tf.Tensor: shape=(), dtype=float32, numpy=409.2432&gt; . I am not sure if there was any change of behaviour expected in this, so I should try using mxnet to see if there is a difference in the above calculation of l2 norm. .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/09/d2lai-exercises-pt3.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/09/d2lai-exercises-pt3.html",
            "date": " • Feb 9, 2021"
        }
        
    
  
    
        ,"post10": {
            "title": "Data Preprocessing - d2l.ai Exercises - Part 2",
            "content": "Exercise setup . Let&#39;s use the sample datasets offered directly in colab . import pandas as pd import numpy as np data = pd.read_csv(&#39;/content/sample_data/california_housing_train.csv&#39;) . . data.head() . . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1283.0 | 1015.0 | 472.0 | 1.4936 | 66900.0 | . 1 -114.47 | 34.40 | 19.0 | 7650.0 | 1901.0 | 1129.0 | 463.0 | 1.8200 | 80100.0 | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 174.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 337.0 | 515.0 | 226.0 | 3.1917 | 73400.0 | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 326.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . data.info() . . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17000 entries, 0 to 16999 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 17000 non-null float64 1 latitude 17000 non-null float64 2 housing_median_age 17000 non-null float64 3 total_rooms 17000 non-null float64 4 total_bedrooms 17000 non-null float64 5 population 17000 non-null float64 6 households 17000 non-null float64 7 median_income 17000 non-null float64 8 median_house_value 17000 non-null float64 dtypes: float64(9) memory usage: 1.2 MB . Since there is no data missing, we will add random missing entries in the data . df = data.mask(np.random.random(data.shape) &lt; .1) df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 17000 entries, 0 to 16999 Data columns (total 9 columns): # Column Non-Null Count Dtype -- -- 0 longitude 15290 non-null float64 1 latitude 15376 non-null float64 2 housing_median_age 15281 non-null float64 3 total_rooms 15325 non-null float64 4 total_bedrooms 15246 non-null float64 5 population 15358 non-null float64 6 households 15351 non-null float64 7 median_income 15298 non-null float64 8 median_house_value 15275 non-null float64 dtypes: float64(9) memory usage: 1.2 MB . Questions . Delete the column with the most missing values. . | Convert the preprocessed dataset to the tensor format. . | . Delete column with most missing values . Peek at the data with na . longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1283.0 | 1015.0 | 472.0 | NaN | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1901.0 | 1129.0 | NaN | 1.8200 | NaN | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 174.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 337.0 | 515.0 | 226.0 | 3.1917 | NaN | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 326.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . Find the column name with most nas . column_with_most_na = max(list(df.columns), key=lambda x: len(df.loc[df[x].isna(), x])) . &#39;total_bedrooms&#39; . Remove the column . longitude latitude housing_median_age total_rooms population households median_income median_house_value . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1015.0 | 472.0 | NaN | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1129.0 | NaN | 1.8200 | NaN | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 333.0 | 117.0 | 1.6509 | 85700.0 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 515.0 | 226.0 | 3.1917 | NaN | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 624.0 | 262.0 | 1.9250 | 65500.0 | . Conversion of the preprocessed dataset to the tensor format. . We can split the dataset to inputs and output, with the median_house_value as the output . inputs = df.iloc[:, :-1].copy() outputs = df.iloc[:, -1].copy() inputs.head() . longitude latitude housing_median_age total_rooms population households median_income . 0 -114.31 | 34.19 | 15.0 | 5612.0 | 1015.0 | 472.0 | NaN | . 1 -114.47 | 34.40 | NaN | 7650.0 | 1129.0 | NaN | 1.8200 | . 2 -114.56 | 33.69 | 17.0 | 720.0 | 333.0 | 117.0 | 1.6509 | . 3 -114.57 | 33.64 | 14.0 | 1501.0 | 515.0 | 226.0 | 3.1917 | . 4 -114.57 | 33.57 | 20.0 | 1454.0 | 624.0 | 262.0 | 1.9250 | . outputs.head() . 0 NaN 1 NaN 2 85700.0 3 NaN 4 65500.0 Name: median_house_value, dtype: float64 . import tensorflow as tf X, y = tf.constant(inputs.values), tf.constant(outputs.values) X, y . (&lt;tf.Tensor: shape=(17000, 7), dtype=float64, numpy= array([[-114.31 , 34.19 , 15. , ..., 1015. , 472. , nan], [-114.47 , 34.4 , nan, ..., 1129. , nan, 1.82 ], [-114.56 , 33.69 , 17. , ..., 333. , 117. , 1.6509], ..., [-124.3 , 41.84 , 17. , ..., 1244. , 456. , 3.0313], [-124.3 , 41.8 , 19. , ..., 1298. , 478. , 1.9797], [-124.35 , 40.54 , 52. , ..., 806. , 270. , 3.0147]])&gt;, &lt;tf.Tensor: shape=(17000,), dtype=float64, numpy=array([ nan, nan, 85700., ..., 103600., 85800., 94600.])&gt;) . This completes the second part of the preliminaries. .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/07/d2lai_exercises_pt2.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/07/d2lai_exercises_pt2.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post11": {
            "title": "Data Manipulation - d2l.ai Exercises - Part 1",
            "content": "This is the first notebook in a series to be posted aiming to solve and understand exercises from d2l.ai curriculum on deep learning, the corresponding lesson reference for this notebook is this link. . This series of practice notebook posts may use the exercises and content provided from d2l.ai, I write these to get a good hands-on practice in deep learning. . import tensorflow as tf . tf.__version__ . &#39;2.4.1&#39; . Setup for exercises . Problem 1 . X = tf.reshape(tf.range(12, dtype=tf.float32), (3, 4)) Y = tf.constant([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]]) X, Y . (&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[ 0., 1., 2., 3.], [ 4., 5., 6., 7.], [ 8., 9., 10., 11.]], dtype=float32)&gt;, &lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy= array([[2., 1., 4., 3.], [1., 2., 3., 4.], [4., 3., 2., 1.]], dtype=float32)&gt;) . Problem 2 . a = tf.reshape(tf.range(3), (3, 1)) b = tf.reshape(tf.range(2), (1, 2)) a, b . (&lt;tf.Tensor: shape=(3, 1), dtype=int32, numpy= array([[0], [1], [2]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(1, 2), dtype=int32, numpy=array([[0, 1]], dtype=int32)&gt;) . Solutions . Problem 1 . X == Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[False, True, False, True], [False, False, False, False], [False, False, False, False]])&gt; . X &lt; Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[ True, False, True, False], [False, False, False, False], [False, False, False, False]])&gt; . X &gt; Y . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[False, False, False, False], [ True, True, True, True], [ True, True, True, True]])&gt; . The operations are as expected of an elementwise comparison. Let&#39;s try to check if the operations are opposites of each other by trying to not one of them. . (X &gt; Y) == tf.math.logical_not(X &lt; Y) . &lt;tf.Tensor: shape=(3, 4), dtype=bool, numpy= array([[ True, False, True, False], [ True, True, True, True], [ True, True, True, True]])&gt; . We can see that apart from two cases where the numbers were equal(1 and 3), all the other values matched . Problem 2 . a = tf.reshape(tf.range(16), (2, 4, -1)) b = tf.reshape(tf.range(16), (4, 2, -1)) a, b . (&lt;tf.Tensor: shape=(2, 4, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3], [ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11], [12, 13], [14, 15]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(4, 2, 2), dtype=int32, numpy= array([[[ 0, 1], [ 2, 3]], [[ 4, 5], [ 6, 7]], [[ 8, 9], [10, 11]], [[12, 13], [14, 15]]], dtype=int32)&gt;) . a + b . InvalidArgumentError Traceback (most recent call last) &lt;ipython-input-12-bd58363a63fc&gt; in &lt;module&gt;() -&gt; 1 a + b /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in binary_op_wrapper(x, y) 1162 with ops.name_scope(None, op_name, [x, y]) as name: 1163 try: -&gt; 1164 return func(x, y, name=name) 1165 except (TypeError, ValueError) as e: 1166 # Even if dispatching the op failed, the RHS may be a tensor aware /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py in wrapper(*args, **kwargs) 199 &#34;&#34;&#34;Call target, and fall back on dispatchers if there is a TypeError.&#34;&#34;&#34; 200 try: --&gt; 201 return target(*args, **kwargs) 202 except (TypeError, ValueError): 203 # Note: convert_to_eager_tensor currently raises a ValueError, not a /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py in _add_dispatch(x, y, name) 1484 return gen_math_ops.add(x, y, name=name) 1485 else: -&gt; 1486 return gen_math_ops.add_v2(x, y, name=name) 1487 1488 /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py in add_v2(x, y, name) 470 return _result 471 except _core._NotOkStatusException as e: --&gt; 472 _ops.raise_from_not_ok_status(e, name) 473 except _core._FallbackException: 474 pass /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py in raise_from_not_ok_status(e, name) 6860 message = e.message + (&#34; name: &#34; + name if name is not None else &#34;&#34;) 6861 # pylint: disable=protected-access -&gt; 6862 six.raise_from(core._status_to_exception(e.code, message), None) 6863 # pylint: enable=protected-access 6864 /usr/local/lib/python3.6/dist-packages/six.py in raise_from(value, from_value) InvalidArgumentError: Incompatible shapes: [2,4,2] vs. [4,2,2] [Op:AddV2] . I tried using tensors of 3d shapes, thinking it might but it did&#39;nt, so I was searching about the rules to determine whether an array can be broadcasted or not and found this documentation, where the conditions are explained, the main points to consider broadcasting are, if the dimensions . are equal, or . | one of them is 1 . | . In the above case we hade shapes: [2,4,2] vs. [4,2,2], Let&#39;s try a different shape . a = tf.reshape(tf.range(12), (6, 2, -1)) b = tf.reshape(tf.range(16), (1, -1)) a, b . (&lt;tf.Tensor: shape=(6, 2, 1), dtype=int32, numpy= array([[[ 0], [ 1]], [[ 2], [ 3]], [[ 4], [ 5]], [[ 6], [ 7]], [[ 8], [ 9]], [[10], [11]]], dtype=int32)&gt;, &lt;tf.Tensor: shape=(1, 16), dtype=int32, numpy= array([[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]], dtype=int32)&gt;) . a + b . &lt;tf.Tensor: shape=(6, 2, 16), dtype=int32, numpy= array([[[ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]], [[ 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], [ 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]], [[ 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19], [ 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]], [[ 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21], [ 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22]], [[ 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [ 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24]], [[10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25], [11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26]]], dtype=int32)&gt; . Similar to the examples in the link, the above example followed the rules and operation(addition) could happen with the help of broadcasting. . a - 6 X 2 X 1 b - 1 X 16 a + b - 6 X 2 X 16 .",
            "url": "https://mani2106.github.io/Blog-Posts/d2l.ai-exercises/deep-learning/tensorflow/2021/02/04/d2lai-exercises-pt1.html",
            "relUrl": "/d2l.ai-exercises/deep-learning/tensorflow/2021/02/04/d2lai-exercises-pt1.html",
            "date": " • Feb 4, 2021"
        }
        
    
  
    
        ,"post12": {
            "title": "Gradient Descent for Linear Regression",
            "content": "Implementing Linear Regression with Gradient Descent . This post is my understanding of Linear Regression, Please feel free to comment and point out any errors if seen. . The Cost Function . The cost function is used to measure the correctness of the current solution(hypothesis), the function can be mean of squares/square roots of the errors. It is represented as the following . J($ theta_0$, $ theta_1$,..$ theta_n$) . where $ theta_0$ is a constant, $ theta_1$…$ theta_n$ are the parameters of the equation we are trying to solve . In simple ML terms(I think) Each one of the parameter represent the weight of each feature in the dataset, that we are using to build the model . This is the formula for the cost function with mean of square differences.$^0$ . 12m∑i=1m(hθ(x)−y)2 frac{1}{2m} sum_{i=1}^{m} (h_ theta(x) - y)^22m1​i=1∑m​(hθ​(x)−y)2 . where $h_0(x)$ is the hypothesis or the predicted value for $y$ and $m$ is the number of training examples . A sample pseudocode for the mean of squares of errors would be . Calculate the sum of square differences / errors between each value $(X* theta)$ vector and y vector For each training example Multiply the feature values $X_1$, $X_2$,..$X_n$ with it’s corresponding weights $ theta_1$, $ theta_2 cdots theta_n$, and add the constant, $ theta_0$. . | Subtract the above value from the $y$ target value of that example and square the difference. . | . | Sum all the differences / errors | . | Take the mean of the differences by the dividing with the number of training examples. | . It can be represented like $h_0(x) = theta_0+ theta_1X_1+ theta_2X_2+ cdots+ theta_nX_n$ where $n$ is the number of features we are using for the problem. . Gradient Descent . We need to update the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ so that the cost function . J(θ0,θ1,⋯θn)=12m∑i=1m(hθ(xi)−yi)2J( theta_0, theta_1, cdots theta_n) = frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2J(θ0​,θ1​,⋯θn​)=2m1​∑i=1m​(hθ​(xi)−yi)2 can be minimized. . So to find the minimum for the parameters $ theta_0, theta_1, theta_2 cdots theta_n$ the update is performed like below . θj:=θj∗α∂∂θj∗J(θ0,θ1,⋯θj) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)θj​:=θj​∗α∂θj​∂​∗J(θ0​,θ1​,⋯θj​) . Where, . The := is assignment operator | The $ theta_j$ is the parameter to update, j is the feature index number | The $ alpha$ is the learning rate | The $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ is the derivative term of the cost function, it is like slope$^2$ of the line tangent$^1$ to the curve touching on where the $ theta$ is present in that curve. | . For each feature in the dataset the update has to be done simultaneously for each parameter $ theta$, until the convergence / error given by cost function at its minimum. . Deriving the derivative term $ frac{ partial}{ partial theta_j} J( theta_0, theta_1, cdots theta_j)$ . $^3$To simplify the problem I am considering that we have 2 parameters $ theta_0$ and $ theta_1$, our hypothesis $h_0(x)$ function becomes $ theta_0+ theta_1X$ . Now let $g( theta_0, theta_1)$ be our derivative term . g(θ0,θ1)=J(θ0,θ1)g( theta_0, theta_1)=J( theta_0, theta_1)g(θ0​,θ1​)=J(θ0​,θ1​) . =(12m∑i=1m(hθ(xi)−yi)2)= ( frac{1}{2m} sum_{i=1}^{m} (h_ theta(x^i) - y^i)^2)=(2m1​i=1∑m​(hθ​(xi)−yi)2) . =(12m∑i=1m(θ0+θ1Xi−yi)2)=( frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2)=(2m1​i=1∑m​(θ0​+θ1​Xi−yi)2) . consider f(θ0,θ1)=hθ(xi)−yif( theta_0, theta_1) = h_ theta(x^i) - y^if(θ0​,θ1​)=hθ​(xi)−yi . now our equation becomes . g(θ0,θ1)=12m∑i=1m(f(θ0,θ1)i)2g( theta_0, theta_1)= frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2g(θ0​,θ1​)=2m1​i=1∑m​(f(θ0​,θ1​)i)2 . subsutituting the value of $f( theta_0, theta_1)$ the equation becomes . g(f(θ0,θ1)i)=12m∑i=1m(θ0+θ1Xi−yi)2(1)g(f( theta_0, theta_1)^i)= frac{1}{2m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)^2 tag{1}g(f(θ0​,θ1​)i)=2m1​i=1∑m​(θ0​+θ1​Xi−yi)2(1) . Now let us derive the partial derivative for $(1)$ . ∂∂θjg(f(θ0,θ1)i)=∂∂θj12m∑i=1m(f(θ0,θ1)i)2 frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_j} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2∂θj​∂​g(f(θ0​,θ1​)i)=∂θj​∂​2m1​i=1∑m​(f(θ0​,θ1​)i)2 . Let j be 0 . ∂∂θ0g(f(θ0,θ1)i)=∂∂θ012m∑i=1m(f(θ0,θ1)i)2 frac{ partial}{ partial theta_0}g(f( theta_0, theta_1)^i) = frac{ partial}{ partial theta_0} frac{1}{2m} sum_{i=1}^{m} (f( theta_0, theta_1)^i)^2∂θ0​∂​g(f(θ0​,θ1​)i)=∂θ0​∂​2m1​i=1∑m​(f(θ0​,θ1​)i)2 . since we are performing the partial derivative with respect to $ theta_0$ other variables are considered constant, the following is similar to $ frac{ partial}{ partial x}$ of $(x^2+y)$ which is $2x$ . =1m∑i=1mf(θ0,θ1)i= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i=m1​i=1∑m​f(θ0​,θ1​)i . =1m∑i=1m(hθ(x)−y)= frac{1}{m} sum_{i=1}^{m} (h_ theta(x) - y)=m1​i=1∑m​(hθ​(x)−y) . This is because of the chain rule, when we take derivative of a function like $(1)$, we need to use this formula below . ∂∂θjg(f(θ0,θ1))=∂∂θjg(θ0,θ1)∗∂∂θjf(θ0,θ1)(2) frac{ partial}{ partial theta_j}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1) tag{2}∂θj​∂​g(f(θ0​,θ1​))=∂θj​∂​g(θ0​,θ1​)∗∂θj​∂​f(θ0​,θ1​)(2) . In case when j = 0 the partial derivative of $g$ becomes . ∂∂θ0g(θ0,θ1)=12m∗2(∑i=1mf(θ0,θ1)i)2−1=1m∑i=1mf(θ0,θ1)i frac{ partial}{ partial theta_0}g( theta_0, theta_1) = frac{1}{ cancel2m}* cancel2( sum_{i=1}^{m} f( theta_0, theta_1)^i)^{2-1}= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i∂θ0​∂​g(θ0​,θ1​)=2 | ​m1​∗2 | ​(i=1∑m​f(θ0​,θ1​)i)2−1=m1​i=1∑m​f(θ0​,θ1​)i . and the partial derivative of $f$ becomes . ∂∂θ0f(θ0,θ1)=∂∂θ0(hθ(xi)−yi)(3) frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}(h_ theta(x^i) - y^i) tag{3}∂θ0​∂​f(θ0​,θ1​)=∂θ0​∂​(hθ​(xi)−yi)(3) . ∂∂θ0f(θ0,θ1)=∂∂θ0(θ0+θ1Xi−yi)=1 frac{ partial}{ partial theta_0}f( theta_0, theta_1) = frac{ partial}{ partial theta_0}( theta_0+ theta_1X^i - y^i) = 1∂θ0​∂​f(θ0​,θ1​)=∂θ0​∂​(θ0​+θ1​Xi−yi)=1 . since other variables are considered constants, that gives us . ∂∂θjg(θ0,θ1)∗∂∂θjf(θ0,θ1)=1m∑i=1mf(θ0,θ1)i∗1=1m∑i=1mf(θ0,θ1)i frac{ partial}{ partial theta_j}g( theta_0, theta_1) * frac{ partial}{ partial theta_j} f( theta_0, theta_1)= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * 1 = frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i∂θj​∂​g(θ0​,θ1​)∗∂θj​∂​f(θ0​,θ1​)=m1​i=1∑m​f(θ0​,θ1​)i∗1=m1​i=1∑m​f(θ0​,θ1​)i . Let’s start from Equation $(1)$ to perform the partial derivative when j = 1 . The partial derivative of $g( theta_0, theta_1)$ with respect to $ theta_1$ is same from the last derivation but the partial derivative of $f( theta_0, theta_1)$ becomes . Consider $(3)$ when j = 1 . ∂∂θ1f(θ1,θ1)=∂∂θ1(hθ(xi)−yi)=∂∂θ1(θ0+θ1Xi−yi) frac{ partial}{ partial theta_1}f( theta_1, theta_1) = frac{ partial}{ partial theta_1}(h_ theta(x^i) - y^i) = frac{ partial}{ partial theta_1}( theta_0+ theta_1X^i - y^i)∂θ1​∂​f(θ1​,θ1​)=∂θ1​∂​(hθ​(xi)−yi)=∂θ1​∂​(θ0​+θ1​Xi−yi) . variables other than $ theta_1$ are considered constants, so they become 0 and $ frac{ partial}{ partial theta_1} theta_1$ = 1, so our equation becomes . ∂∂θ1f(θ0,θ1)=0+1∗Xi−0=Xi frac{ partial}{ partial theta_1}f( theta_0, theta_1)= 0+1* X^i-0 =X^i∂θ1​∂​f(θ0​,θ1​)=0+1∗Xi−0=Xi . and according to the chain rule $(2)$ and replacing the partials . ∂∂θ1g(f(θ0,θ1))=∂∂θ1g(θ0,θ1)∗∂∂θ1f(θ0,θ1) frac{ partial}{ partial theta_1}g(f( theta_0, theta_1)) = frac{ partial}{ partial theta_1}g( theta_0, theta_1) * frac{ partial}{ partial theta_1} f( theta_0, theta_1)∂θ1​∂​g(f(θ0​,θ1​))=∂θ1​∂​g(θ0​,θ1​)∗∂θ1​∂​f(θ0​,θ1​) . =1m∑i=1mf(θ0,θ1)i∗Xi= frac{1}{m} sum_{i=1}^{m} f( theta_0, theta_1)^i * X^i=m1​i=1∑m​f(θ0​,θ1​)i∗Xi . =1m∑i=1m(θ0+θ1Xi−yi)∗Xi= frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i) * X^i=m1​i=1∑m​(θ0​+θ1​Xi−yi)∗Xi . Now we can use the derivatives in the Gradient Descent algorithm . θj:=θj∗α∂∂θj∗J(θ0,θ1,⋯θj) theta_j := theta_j * alpha frac{ partial}{ partial theta_j}*J( theta_0, theta_1, cdots theta_j)θj​:=θj​∗α∂θj​∂​∗J(θ0​,θ1​,⋯θj​) . repeat until convergence { θ0:=θ0∗α1m∑i=1m(θ0+θ1Xi−yi) theta_0 := theta_0 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)θ0​:=θ0​∗αm1​∑i=1m​(θ0​+θ1​Xi−yi) . θ1:=θ1∗α1m∑i=1m(θ0+θ1Xi−yi)∗Xi theta_1 := theta_1 * alpha frac{1}{m} sum_{i=1}^{m} ( theta_0+ theta_1X^i - y^i)* X^iθ1​:=θ1​∗αm1​∑i=1m​(θ0​+θ1​Xi−yi)∗Xi } . One disadvantage in Gradient Descent is that depending on the position it is initialized at the start, but in linear regression the cost function (mean of sqaured errors) is a convex function (ie) it is in shape of a bowl when plotted on the graph. . I tried to implement this in python which can be found here . Resources and references . How to implement a machine learning algorithm | Understanding math in Machine learning . | $^0$ Most of the content and explanation is from Coursera’s - Machine Learning class . | $^1$ Tangent is a line which touches exactly at one point of a curve. . | $^2$ Slope of a line given any two points on the line is the ratio number of points we need to rise/descend and move away/towards the origin to the meet the other point. | . . Image from wikihow . | $^3$ Derivation referred from here . | .",
            "url": "https://mani2106.github.io/Blog-Posts/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "relUrl": "/implementation/machine%20learning/linear%20regression/2020/08/31/linear-regression-grad-desc.html",
            "date": " • Aug 31, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Tabular data modeling - with Tensorflow",
            "content": "In this notebook, I was giving a shot with Tensorflow for tabular data modeling, this particular competition data is quite imbalanced as you will see in this notebook a little later, This example is more or else fully adapted from the example from Tensorflow docs . Import required libraries . For Modeling . import tensorflow as tf from tensorflow import keras . For Getting data from file to variable . import numpy as np import pandas as pd . For Visualization . import seaborn as sns import matplotlib as mpl import matplotlib.pyplot as plt . /usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm . For Evaluation and preprocessing . import sklearn from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.preprocessing import StandardScaler from sklearn.impute import SimpleImputer . Download data . !unzip data.zip . Archive: data.zip creating: Dataset/ inflating: Dataset/Train.csv inflating: Dataset/sample_submission.csv inflating: Dataset/Test.csv . import pandas as pd from pathlib import Path DATA_PATH = Path(&#39;/content/Dataset&#39;) train_data = pd.read_csv(DATA_PATH/&#39;Train.csv&#39;, index_col=0, infer_datetime_format=True, converters={&#39;DATE&#39;:pd.to_datetime}) train_data.head() . DATE X_1 X_2 X_3 X_4 X_5 X_6 X_7 X_8 X_9 X_10 X_11 X_12 X_13 X_14 X_15 MULTIPLE_OFFENSE . INCIDENT_ID . CR_102659 2004-07-04 | 0 | 36 | 34 | 2 | 1 | 5 | 6 | 1 | 6 | 1 | 174 | 1.0 | 92 | 29 | 36 | 0 | . CR_189752 2017-07-18 | 1 | 37 | 37 | 0 | 0 | 11 | 17 | 1 | 6 | 1 | 236 | 1.0 | 103 | 142 | 34 | 1 | . CR_184637 2017-03-15 | 0 | 3 | 2 | 3 | 5 | 1 | 0 | 2 | 3 | 1 | 174 | 1.0 | 110 | 93 | 34 | 1 | . CR_139071 2009-02-13 | 0 | 33 | 32 | 2 | 1 | 7 | 1 | 1 | 6 | 1 | 249 | 1.0 | 72 | 29 | 34 | 1 | . CR_109335 2005-04-13 | 0 | 33 | 32 | 2 | 1 | 8 | 3 | 0 | 5 | 1 | 174 | 0.0 | 112 | 29 | 43 | 1 | . Target Distribution . train_data[&#39;MULTIPLE_OFFENSE&#39;].value_counts().plot.bar() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f34852fcb70&gt; . neg, pos = np.bincount(train_data[&#39;MULTIPLE_OFFENSE&#39;]) total = neg + pos print(&#39;Examples: n Total: {} n Positive: {} ({:.2f}% of total) n&#39;.format( total, pos, 100 * pos / total)) . Examples: Total: 23856 Positive: 22788 (95.52% of total) . We do have an imbalanced dataset, as shown above, 95% of the targets are positive (Class 1). . Data Preprocessing . seed=98 np.random.seed(seed=seed) tf.random.set_seed(seed) . We shall drop the date column and try to model with only the logging parameters . cleaned_df = train_data.drop(&#39;DATE&#39;, axis=&#39;columns&#39;) . We split the training data to three train, test and eval sets . train_df, test_df = train_test_split(cleaned_df, test_size=0.2, random_state=seed, stratify=cleaned_df[&#39;MULTIPLE_OFFENSE&#39;]) . train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=seed, stratify=train_df[&#39;MULTIPLE_OFFENSE&#39;]) . train_labels = np.array(train_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) bool_train_labels = train_labels != 0 val_labels = np.array(val_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) test_labels = np.array(test_df.pop(&#39;MULTIPLE_OFFENSE&#39;)) train_features = np.array(train_df) val_features = np.array(val_df) test_features = np.array(test_df) . Create a preprocessing pipeline with scikit-learn . from sklearn.pipeline import Pipeline preproc_pipe = Pipeline([ (&#39;median_imputer&#39;, SimpleImputer(strategy=&#39;median&#39;)), (&#39;scaler&#39;, StandardScaler()) ]) . train_features = preproc_pipe.fit_transform(train_features) val_features = preproc_pipe.transform(val_features) test_features = preproc_pipe.transform(test_features) # ensure that the values are within a range train_features = np.clip(train_features, -5, 5) val_features = np.clip(val_features, -5, 5) test_features = np.clip(test_features, -5, 5) . Check the dimensions of our data . print(&#39;Training labels shape:&#39;, train_labels.shape) print(&#39;Validation labels shape:&#39;, val_labels.shape) print(&#39;Test labels shape:&#39;, test_labels.shape) print(&#39;Training features shape:&#39;, train_features.shape) print(&#39;Validation features shape:&#39;, val_features.shape) print(&#39;Test features shape:&#39;, test_features.shape) . Training labels shape: (15267,) Validation labels shape: (3817,) Test labels shape: (4772,) Training features shape: (15267, 15) Validation features shape: (3817, 15) Test features shape: (4772, 15) . Let&#39;s check if our preprocessing has shown a distinction between the classes of the dataset . pos_df = pd.DataFrame(train_features[ bool_train_labels], columns = train_df.columns) neg_df = pd.DataFrame(train_features[~bool_train_labels], columns = train_df.columns) sns.jointplot(pos_df[&#39;X_10&#39;], pos_df[&#39;X_15&#39;], kind=&#39;hex&#39;, xlim = (-1,1), ylim = (-1,1)) plt.suptitle(&quot;Positive distribution&quot;) sns.jointplot(neg_df[&#39;X_10&#39;], neg_df[&#39;X_15&#39;], kind=&#39;hex&#39;, xlim = (-1,1), ylim = (-1,1)) _ = plt.suptitle(&quot;Negative distribution&quot;) . There is a slight difference in terms of the value, (ie) for the positive class the distribution is slightly on the negative side of zero, and the negative class is slightly on the positive side of zero . Model setup . Let&#39;s setup the model layers and the evaluation metrics . def plot_loss(history, label, n): # Use a log scale to show the wide range of values. plt.semilogy(history.epoch, history.history[&#39;loss&#39;], color=colors[n], label=&#39;Train &#39;+label) plt.semilogy(history.epoch, history.history[&#39;val_loss&#39;], color=colors[n], label=&#39;Val &#39;+label, linestyle=&quot;--&quot;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(&#39;Loss&#39;) plt.legend() . METRICS = [ keras.metrics.TruePositives(name=&#39;tp&#39;), keras.metrics.FalsePositives(name=&#39;fp&#39;), keras.metrics.TrueNegatives(name=&#39;tn&#39;), keras.metrics.FalseNegatives(name=&#39;fn&#39;), keras.metrics.BinaryAccuracy(name=&#39;accuracy&#39;), keras.metrics.Precision(name=&#39;precision&#39;), keras.metrics.Recall(name=&#39;recall&#39;), keras.metrics.AUC(name=&#39;auc&#39;), ] def make_model(metrics=METRICS, output_bias=None): if output_bias is not None: output_bias = tf.keras.initializers.Constant(output_bias) model = keras.Sequential([ keras.layers.Dense( 16, activation=&#39;relu&#39;, input_shape=(train_features.shape[-1],)), keras.layers.Dropout(0.5), keras.layers.Dense(1, activation=&#39;sigmoid&#39;, bias_initializer=output_bias), ]) model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.BinaryCrossentropy(), metrics=metrics) return model . Set constants and callbacks . EPOCHS = 500 BATCH_SIZE = 4096 early_stopping = tf.keras.callbacks.EarlyStopping( monitor=&#39;val_recall&#39;, verbose=1, patience=10, mode=&#39;max&#39;, restore_best_weights=True) . Calculate initial bias for a smooth training . initial_bias = np.log([pos/neg]) initial_bias . array([3.06044634]) . model = make_model(output_bias=initial_bias) . results = model.evaluate(train_features, train_labels, batch_size=BATCH_SIZE, verbose=0) print(&quot;Loss: {:0.4f}&quot;.format(results[0])) . Loss: 0.2112 . import os, tempfile initial_weights = os.path.join(tempfile.mkdtemp(),&#39;initial_weights&#39;) model.save_weights(initial_weights) . Calculate class weights to set to the model . weight_for_0 = (1 / neg)*(total)/2.0 weight_for_1 = (1 / pos)*(total)/2.0 class_weight = {0: weight_for_0, 1: weight_for_1} print(&#39;Weight for class 0: {:.2f}&#39;.format(weight_for_0)) print(&#39;Weight for class 1: {:.2f}&#39;.format(weight_for_1)) . Weight for class 0: 11.17 Weight for class 1: 0.52 . Train the model . weighted_model = make_model() weighted_model.load_weights(initial_weights) weighted_history = weighted_model.fit( train_features, train_labels, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(val_features, val_labels), # The class weights go here class_weight=class_weight) . Epoch 1/500 4/4 [==============================] - 1s 260ms/step - loss: 2.1039 - tp: 32485.0000 - fp: 1406.0000 - tn: 131.0000 - fn: 329.0000 - accuracy: 0.9495 - precision: 0.9585 - recall: 0.9900 - auc: 0.4448 - val_loss: 0.2044 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4527 Epoch 2/500 4/4 [==============================] - 0s 21ms/step - loss: 2.0170 - tp: 14580.0000 - fp: 683.0000 - tn: 0.0000e+00 - fn: 4.0000 - accuracy: 0.9550 - precision: 0.9553 - recall: 0.9997 - auc: 0.4581 - val_loss: 0.2020 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4656 Epoch 3/500 4/4 [==============================] - 0s 20ms/step - loss: 1.9718 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4609 - val_loss: 0.1997 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4698 Epoch 4/500 4/4 [==============================] - 0s 21ms/step - loss: 1.9492 - tp: 14572.0000 - fp: 681.0000 - tn: 2.0000 - fn: 12.0000 - accuracy: 0.9546 - precision: 0.9554 - recall: 0.9992 - auc: 0.4661 - val_loss: 0.1975 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4828 Epoch 5/500 4/4 [==============================] - 0s 22ms/step - loss: 1.9312 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4605 - val_loss: 0.1955 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4899 Epoch 6/500 4/4 [==============================] - 0s 21ms/step - loss: 1.8575 - tp: 14570.0000 - fp: 682.0000 - tn: 1.0000 - fn: 14.0000 - accuracy: 0.9544 - precision: 0.9553 - recall: 0.9990 - auc: 0.4825 - val_loss: 0.1936 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.4988 Epoch 7/500 4/4 [==============================] - 0s 20ms/step - loss: 1.8257 - tp: 14560.0000 - fp: 679.0000 - tn: 4.0000 - fn: 24.0000 - accuracy: 0.9540 - precision: 0.9554 - recall: 0.9984 - auc: 0.4858 - val_loss: 0.1918 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5161 Epoch 8/500 4/4 [==============================] - 0s 20ms/step - loss: 1.7717 - tp: 14555.0000 - fp: 677.0000 - tn: 6.0000 - fn: 29.0000 - accuracy: 0.9538 - precision: 0.9556 - recall: 0.9980 - auc: 0.5054 - val_loss: 0.1902 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5260 Epoch 9/500 4/4 [==============================] - 0s 22ms/step - loss: 1.7657 - tp: 14551.0000 - fp: 678.0000 - tn: 5.0000 - fn: 33.0000 - accuracy: 0.9534 - precision: 0.9555 - recall: 0.9977 - auc: 0.5002 - val_loss: 0.1886 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5330 Epoch 10/500 4/4 [==============================] - 0s 25ms/step - loss: 1.7285 - tp: 14539.0000 - fp: 678.0000 - tn: 5.0000 - fn: 45.0000 - accuracy: 0.9526 - precision: 0.9554 - recall: 0.9969 - auc: 0.5037 - val_loss: 0.1872 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5431 Epoch 11/500 1/4 [======&gt;.......................] - ETA: 0s - loss: 1.5431 - tp: 3918.0000 - fp: 166.0000 - tn: 1.0000 - fn: 11.0000 - accuracy: 0.9568 - precision: 0.9594 - recall: 0.9972 - auc: 0.5130Restoring model weights from the end of the best epoch. 4/4 [==============================] - 0s 22ms/step - loss: 1.6912 - tp: 14541.0000 - fp: 675.0000 - tn: 8.0000 - fn: 43.0000 - accuracy: 0.9530 - precision: 0.9556 - recall: 0.9971 - auc: 0.5073 - val_loss: 0.1860 - val_tp: 3646.0000 - val_fp: 171.0000 - val_tn: 0.0000e+00 - val_fn: 0.0000e+00 - val_accuracy: 0.9552 - val_precision: 0.9552 - val_recall: 1.0000 - val_auc: 0.5554 Epoch 00011: early stopping . We have got a good validation recall of 0.9026 . mpl.rcParams[&#39;figure.figsize&#39;] = (12, 10) colors = plt.rcParams[&#39;axes.prop_cycle&#39;].by_key()[&#39;color&#39;] . plot_loss(weighted_history, &quot;Weighted&quot;, n=0) . Looks like there still could be some room for improvement . def plot_metrics(history): metrics = [&#39;loss&#39;, &#39;auc&#39;, &#39;precision&#39;, &#39;recall&#39;] for n, metric in enumerate(metrics): name = metric.replace(&quot;_&quot;,&quot; &quot;).capitalize() plt.subplot(2,2,n+1) plt.plot(history.epoch, history.history[metric], color=colors[0], label=&#39;Train&#39;) plt.plot(history.epoch, history.history[&#39;val_&#39;+metric], color=colors[0], linestyle=&quot;--&quot;, label=&#39;Val&#39;) plt.xlabel(&#39;Epoch&#39;) plt.ylabel(name) if metric == &#39;loss&#39;: plt.ylim([0, plt.ylim()[1]]) elif metric == &#39;auc&#39;: plt.ylim([0.8,1]) else: plt.ylim([0,1]) plt.legend() . plot_metrics(weighted_history) . Predict with test data . test_data = pd.read_csv(DATA_PATH/&#39;Test.csv&#39;, index_col=0) test_data.drop(&#39;DATE&#39;, axis=&#39;columns&#39;, inplace=True) . proc_test = preproc_pipe.transform(np.array(test_data)) . preds = np.argmax(weighted_model.predict(proc_test), axis=-1) . sub_df = pd.DataFrame( {&#39;INCIDENT_ID&#39;:test_data.index, &#39;MULTIPLE_OFFENSE&#39;:preds}, ) sub_df.head() . INCIDENT_ID MULTIPLE_OFFENSE . 0 CR_195453 | 0 | . 1 CR_103520 | 0 | . 2 CR_196089 | 0 | . 3 CR_112195 | 0 | . 4 CR_149832 | 0 | . sub_df.to_csv(&#39;submission_df.csv&#39;, index=False) . sub_df[&#39;MULTIPLE_OFFENSE&#39;].value_counts() . 0 15903 Name: MULTIPLE_OFFENSE, dtype: int64 . I tried with various batch sizes and epochs to try to get it predict the classes differently maybe I should also try to change the model structure to achieve a different result because this submission only scored 50 recall on the competition test set. . I will also post other notebooks very soon, which will use machine learning based decision tree and random forest methods which performed way better on this problem. .",
            "url": "https://mani2106.github.io/Blog-Posts/tabular/tensorflow/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "relUrl": "/tabular/tensorflow/2020/06/24/Tabular_data_modeling_with_Tensorflow.html",
            "date": " • Jun 24, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Mother's day Sentiment analysis - with spaCy",
            "content": "Setup paths . We will use the method from my previous post to clean the text. . from pathlib import Path import pandas as pd DATA_PATH = Path(&#39;dataset/&#39;) DRIVE_PATH = Path(r&quot;/content/drive/My Drive/Spacy/Pretrained&quot;) . train_data = pd.read_csv(DATA_PATH/&#39;train.csv&#39;, index_col=0) train_data.head() . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . Let&#39;s check average length of text before cleaning. . print(sum( train_data[&#39;original_text&#39;].apply(len).tolist() )/train_data.shape[0]) . . 227.42102009273572 . Clean links with regex . train_data[&#39;original_text&#39;].replace( # Regex is match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . Let&#39;s check the average length again. . The regex did it&#39;s job I suppose. . train_data.head() . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . In my previous exploratory post, I have seen the data and I think that the features other than the text may not be required, (ie) . lang | retweet_count | original_author | . Class distribution . 0 must mean Neutral | 1 means Positive | -1 means Negative | . train_data[&#39;sentiment_class&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2681b9b438&gt; . Let&#39;s see some sentences with negative examples, I am interested why they should be negative on a happy day(Mother&#39;s day) . list_of_neg_sents = train_data.loc[train_data[&#39;sentiment_class&#39;] == -1, &#39;original_text&#39;].tolist() . pprint(list_of_neg_sents[:5]) . . [&#39;Happy mothers day To all This doing a mothers days work. Today been quiet &#39; &#39;but Had time to reflect. Dog walk, finish a jigsaw do the garden, learn few &#39; &#39;more guitar chords, drunk some strawberry gin and tonic and watch Lee evens &#39; &#39;on DVD. My favourite place to visit. #isolate &#39;, &#39;Remembering the 3 most amazing ladies who made me who I am! My late &#39; &#39;grandmother iris, mum carol and great grandmother Ethel. Missed but never &#39; &#39;forgotten! Happy mothers day to all those great mums out there! Love sent to &#39; &#39;all xxxx &#39;, &#39;Happy Mothers Day to everyone tuning in. This is the 4th Round game between &#39; &#39;me and @CastigersJ Live coverage on @Twitter , maybe one day @SkySportsRL or &#39; &#39;on the OurLeague app&#39;, &#34;Happy Mothers Day ! We hope your mums aren&#39;t planning to do any work around &#34; &#39;the house today! Surely it can wait until next week? #plumbers &#39; &#39;#heatingspecialists #mothersday #mothersday &#39;, &#34;Happy mothers day to all those mums whos children can&#39;t be with them today. &#34; &#39;My son Dylan lives in heaven I wish I could see him for one more hug. I wish &#39; &#39;I could tell him how much I love and miss him. Huge happy mothers day to &#39; &#39;your mum too.&#39;] . Well some tweets actually express their feelings for their deceased mothers. This is understandable. . We can use traditional NLP methods or deep learning methods to model the text. We will try the deep learning in this notebook . . Deep Learning approach with Spacy . It&#39;s recommended here that to improve performance of the classifier, Language model pretraining is one way to do so. . Spacy requires a .jsonl format of input to train text . Get texts from the dataframe and store in jsonl format more about that here. . We can also load the test data to get some more sample for the pretraining, this will not cause Data Leakage because we are not giving any labels to the model. . test_data = pd.read_csv(DATA_PATH/&#39;test.csv&#39;, index_col=0) test_data.head() . original_text lang retweet_count original_author . id . 1.246628e+18 3. Yeah, I once cooked potatoes when I was 3 y... | en | 0 | LToddWood | . 1.245898e+18 Happy Mother&#39;s Day to all the mums, step-mums,... | en | 0 | iiarushii | . 1.244717e+18 I love the people from the UK, however, when I... | en | 0 | andreaanderegg | . 1.245730e+18 Happy 81st Birthday Happy Mother’s Day to my m... | en | 1 | TheBookTweeters | . 1.244636e+18 Happy Mothers day to all those wonderful mothe... | en | 0 | andreaanderegg | . Let&#39;s clean the test set for links as well . test_data[&#39;original_text&#39;].replace( # Regex pattern to match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . test_data.shape . (1387, 4) . texts_series = pd.concat([train_data[&#39;original_text&#39;], test_data[&#39;original_text&#39;]], axis=&#39;rows&#39;) . Let&#39;s check the length . texts_series.shape[0], train_data.shape[0]+test_data.shape[0] . (4622, 4622) . So now we can use this texts_series to create the jsonl file. . list_of_texts = [ # Form dictionary with &#39;text&#39; key {&#39;text&#39;: value} for _, value in texts_series.items() ] . I will use srsly to write this list of dictionaries to a jsonl file . import srsly # saving to my Google drive srsly.write_jsonl(DRIVE_PATH/&#39;pretrain_texts.jsonl&#39;, list_of_texts) . We can see a few lines from the saved file. . from pprint import pprint with Path(DRIVE_PATH/&#39;pretrain_texts.jsonl&#39;).open() as f: lines = [next(f) for x in range(5)] pprint(lines) . . [&#39;{&#34;text&#34;:&#34;Happy #MothersDay to all you amazing mothers out there! I know &#39; &#34;it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to &#34; &#39;do what we can to protect the most vulnerable members of our society. &#39; &#39;#BeatCoronaVirus &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy Mothers Day Mum - I &#39;m sorry I can &#39;t be there to bring you &#39; &#34;Mothers day flowers &amp; a cwtch - honestly at this point I&#39;d walk on hot coals &#34; &#34;to be able to. But I&#39;ll be there with bells on as soon as I can be. Love you &#34; &#39;lots xxx (p.s we need more photos!) &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy mothers day To all This doing a mothers days work. Today been &#39; &#39;quiet but Had time to reflect. Dog walk, finish a jigsaw do the garden, &#39; &#39;learn few more guitar chords, drunk some strawberry gin and tonic and watch &#39; &#39;Lee evens on DVD. My favourite place to visit. #isolate &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Happy mothers day to this beautiful woman...royalty soothes you &#39; &#39;mummy jeremy and emerald and more #PrayForRoksie #UltimateLoveNG &#34;} n&#39;, &#39;{&#34;text&#34;:&#34;Remembering the 3 most amazing ladies who made me who I am! My late &#39; &#39;grandmother iris, mum carol and great grandmother Ethel. Missed but never &#39; &#39;forgotten! Happy mothers day to all those great mums out there! Love sent to &#39; &#39;all xxxx &#34;} n&#39;] . Start Pretraining . We should download a pretrained to model to use, Here I am using _en_core_webmd from Spacy. . This can be confusing (ie) Why should I train a pretrained model, if I can download one, The idea is that the downloaded pretrained model would have been trained with a very different type of dataset, but it already has some knowledge on interpreting words in English sentences. . But here we have dataset of tweets which the downloaded pretrained model may or may not have seen during it&#39;s training, So we use our dataset to fine-tune the downloaded model, so that with minimum training it can start understanding the tweets right away. . !python -m spacy download en_core_web_md . . Training results . %%bash # Command to pretrain a language model # Path to jsonl file with data # Using md model as the base # saving the model on my Drive folder # training for 50 iterations with seed set to 0 python -m spacy pretrain /content/drive/My Drive/Spacy/Pretrained/pretrain_texts.jsonl /usr/local/lib/python3.6/dist-packages/en_core_web_md/en_core_web_md-2.2.5 /content/drive/My Drive/Spacy/Pretrained/ -i 50 -s 0 . . ℹ Not using GPU ⚠ Output directory is not empty It is better to use an empty directory or refer to a new output path, then the new directory will be created for you. ✔ Saved settings to config.json ✔ Loaded input texts ✔ Loaded model &#39;/usr/local/lib/python3.6/dist-packages/en_core_web_md/en_core_web_md-2.2.5&#39; ============== Pre-training tok2vec layer - starting at epoch 0 ============== # # Words Total Loss Loss w/s 0 115177 114619.719 114619 7308 0 177090 174673.695 60053 8123 1 291933 282095.656 107421 8353 1 354180 337893.113 55797 8156 2 468951 432705.457 94812 8398 2 531270 479373.527 46668 8271 3 646206 557380.137 78006 8368 3 708360 595962.348 38582 8151 4 823108 662773.332 66810 8349 4 885450 696823.672 34050 8125 5 1000591 756743.684 59920 8254 5 1062540 787816.266 31072 7943 6 1177552 844198.828 56382 8380 6 1239630 874128.219 29929 7996 7 1354814 928725.262 54597 8291 7 1416720 957604.215 28878 8081 8 1531685 1010607.96 53003 8310 8 1593810 1039022.08 28414 8006 9 1708981 1091185.60 52163 8248 9 1770900 1118857.03 27671 8032 10 1885776 1169906.66 51049 8240 10 1947990 1197361.49 27454 8015 11 2062486 1247384.96 50023 8344 11 2125080 1274605.26 27220 8153 12 2239188 1323843.16 49237 8376 12 2302170 1350702.81 26859 7941 13 2416942 1399298.78 48595 8213 13 2479260 1425267.50 25968 7968 14 2594827 1473118.10 47850 8357 14 2656350 1498307.73 25189 7879 15 2770909 1544661.96 46354 7572 15 2833440 1569846.84 25184 7980 16 2948496 1615382.64 45535 8137 16 3010530 1639715.23 24332 7848 17 3125700 1684541.43 44826 8316 17 3187620 1708564.64 24023 7941 18 3302357 1753053.58 44488 8039 18 3364710 1776999.23 23945 7234 19 3480485 1821211.88 44212 8191 19 3541800 1844436.19 23224 7859 20 3657480 1887923.85 43487 8170 20 3718890 1911031.40 23107 7902 21 3833871 1954109.82 43078 8253 21 3895980 1977017.92 22908 7840 22 4011686 2019201.90 42183 8096 22 4073070 2041539.05 22337 7906 23 4188348 2083159.02 41619 8148 23 4250160 2105332.99 22173 7882 24 4364945 2146261.39 40928 8127 24 4427250 2168323.78 22062 8017 25 4542920 2209202.75 40878 8059 25 4604340 2230725.28 21522 7823 26 4719450 2271131.96 40406 8272 26 4781430 2292648.11 21516 7986 27 4896609 2332466.73 39818 8243 27 4958520 2353768.18 21301 8030 28 5073022 2392817.96 39049 8217 28 5135610 2414267.76 21449 8032 29 5250013 2452981.53 38713 8250 29 5312700 2474212.04 21230 8098 30 5427418 2512822.70 38610 8251 30 5489790 2533517.18 20694 8083 31 5604738 2572000.05 38482 8291 31 5666880 2592500.56 20500 8071 32 5781993 2630529.82 38029 8278 32 5843970 2651013.32 20483 8080 33 5959305 2689165.69 38152 8269 33 6021060 2709293.04 20127 8086 34 6136501 2746749.93 37456 8278 34 6198150 2766661.32 19911 8105 35 6313772 2803964.79 37303 8262 35 6375240 2823781.55 19816 8095 36 6490354 2860641.99 36860 8235 36 6552330 2880476.80 19834 8056 37 6667255 2917062.19 36585 8231 37 6729420 2936732.01 19669 8108 38 6844248 2972992.59 36260 8276 38 6906510 2992718.12 19725 8074 39 7021001 3028854.93 36136 8275 39 7083600 3048416.73 19561 8092 40 7199191 3084584.77 36168 8304 40 7260690 3103795.33 19210 8085 41 7375380 3139675.02 35879 8276 41 7437780 3158848.65 19173 8136 42 7552544 3194214.46 35365 8276 42 7614870 3213532.30 19317 8072 43 7730130 3248964.60 35432 7724 43 7791960 3267930.77 18966 8096 44 7906035 3302653.15 34722 8263 44 7969050 3321900.34 19247 8103 45 8083488 3356590.95 34690 8251 45 8146140 3375484.65 18893 8084 46 8261612 3410429.05 34944 8228 46 8323230 3429043.30 18614 8121 47 8437925 3463440.15 34396 8268 47 8500320 3482173.21 18733 7931 48 8615136 3516342.36 34169 8294 48 8677410 3534982.44 18640 8098 49 8791889 3568641.36 33658 8265 49 8854500 3587280.92 18639 8102 ⚠ Skipped 250 empty values ✔ Successfully finished pretrain . I have chosen to use the default parameters however one might need to change them for their problem. . We can see from the logs that the loss value in the last iteration is 18639, but since the batch_size was 3000 our data must have splitted to 2 batches, (number of texts are 4622) we should also take the previous log entry to account which is loss of 33658, So the average of them would be 26148.5, This number might be intimidating but the only way to check if it actually helps is to try to train a model with it. . If it doesn&#39;t then we can resume the training from the model saved on the last epoch. . We keep only the last model from the pretraining. . Let&#39;s train a text classifier with Spacy . Text classifier with Spacy . Now that we have a pretrained model, We now need to prepare data for training the text classifier. Let&#39;s have a look at the data format that Spacy expects the data to be in. . Data Generation . { &quot;entities&quot;: [(0, 4, &quot;ORG&quot;)], &quot;heads&quot;: [1, 1, 1, 5, 5, 2, 7, 5], &quot;deps&quot;: [&quot;nsubj&quot;, &quot;ROOT&quot;, &quot;prt&quot;, &quot;quantmod&quot;, &quot;compound&quot;, &quot;pobj&quot;, &quot;det&quot;, &quot;npadvmod&quot;], &quot;tags&quot;: [&quot;PROPN&quot;, &quot;VERB&quot;, &quot;ADP&quot;, &quot;SYM&quot;, &quot;NUM&quot;, &quot;NUM&quot;, &quot;DET&quot;, &quot;NOUN&quot;], &quot;cats&quot;: {&quot;BUSINESS&quot;: 1.0}, } . This format works for training via code, as given in the examples above, There is also another format mentioned here . cats is the only part we need to worry about, this must be where they look for categories/classes. . We have three classes in our dataset . 0 for Neutral | 1 for Positive | -1 for Negative | . and they are mutually-exclusive (There can be only one label for a sentence) . We also need to split the training data we have to training and evaluation sets so that we can see how well our model has learnt the problem. . Let&#39;s try to programmatically generate the training data from pandas dataframe . label_map = {1:&#39;POSITIVE&#39;, -1:&#39;NEGATIVE&#39;, 0:&#39;NEUTRAL&#39;} . We need list of tuples of text and the annotation details in a dictionary as mentioned above. . train_json = [ # Get the text from dataframe row (tweet.original_text, {&#39;cats&#39;:{ label_map[tweet.sentiment_class]:1.0 } }) for tweet in train_data[[&#39;original_text&#39;, &#39;sentiment_class&#39;]].itertuples(index=False, name=&#39;Tweet&#39;) ] . train_json[0] . (&#34;Happy #MothersDay to all you amazing mothers out there! I know it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus &#34;, {&#39;cats&#39;: {&#39;NEUTRAL&#39;: 1.0}}) . Now we will split the training data . from sklearn.model_selection import train_test_split # Stratified split with labels train_split, eval_split = train_test_split(train_json, test_size=0.2, stratify=train_data[&#39;sentiment_class&#39;]) . len(train_split), len(eval_split) . (2588, 647) . We should save them as json files to give them as input to the command line train utility in spacy. . import json with Path(DRIVE_PATH/&#39;train_clas.json&#39;).open(&#39;w&#39;) as f: json.dump(train_split, f) with Path(DRIVE_PATH/&#39;eval_clas.json&#39;).open(&#39;w&#39;) as f: json.dump(eval_split, f) . Validate data input for spacy . Now should if we have enough data to train the model with train spacy command in CLI, for that I will use Spacy&#39;s debug-data command in CLI. . !python -m spacy debug-data -h . usage: spacy debug-data [-h] [-tm None] [-b None] [-p tagger,parser,ner] [-IW] [-V] [-NF] lang train_path dev_path Analyze, debug and validate your training and development data, get useful stats, and find problems like invalid entity annotations, cyclic dependencies, low data labels and more. positional arguments: lang model language train_path location of JSON-formatted training data dev_path location of JSON-formatted development data optional arguments: -h, --help show this help message and exit -tm None, --tag-map-path None Location of JSON-formatted tag map -b None, --base-model None name of model to update (optional) -p tagger,parser,ner, --pipeline tagger,parser,ner Comma-separated names of pipeline components to train -IW, --ignore-warnings Ignore warnings, only show stats and errors -V, --verbose Print additional information and explanations -NF, --no-format Don&#39;t pretty-print the results . %%bash (python -m spacy debug-data en /content/drive/My Drive/Spacy/Pretrained/train_clas.json /content/drive/My Drive/Spacy/Pretrained/eval_clas.json -p &#39;textcat&#39; ) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 0 training docs 0 evaluation docs ✘ No evaluation docs ✔ No overlap between training and evaluation data ✘ Low number of examples to train from a blank model (0) ============================== Vocab &amp; Vectors ============================== ℹ 0 total words in the data (0 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 0 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 2 checks passed ✘ 2 errors . Data Generation (again) . There must be something I missed now, I asked a question on stackoverflow regarding this, turns out we need to get .jsonl format(again) and use the script provided in the repo to convert to the required json format for training, now I need to change the data generation a little bit to do that. . train_jsonl = [ # Get the text from dataframe row {&#39;text&#39;: tweet.original_text, &#39;cats&#39;: {v: 1.0 if tweet.sentiment_class == k else 0.0 for k, v in label_map.items()}, &#39;meta&#39;:{&quot;id&quot;: str(tweet.Index)} } for tweet in train_data[[&#39;original_text&#39;, &#39;sentiment_class&#39;]].itertuples(index=True, name=&#39;Tweet&#39;) ] . train_jsonl[0] . {&#39;cats&#39;: {&#39;NEGATIVE&#39;: 0.0, &#39;NEUTRAL&#39;: 1.0, &#39;POSITIVE&#39;: 0.0}, &#39;meta&#39;: {&#39;id&#39;: &#39;1.24502457848689e+18&#39;}, &#39;text&#39;: &#34;Happy #MothersDay to all you amazing mothers out there! I know it&#39;s hard not being able to see your mothers today but it&#39;s on all of us to do what we can to protect the most vulnerable members of our society. #BeatCoronaVirus &#34;} . So instead of a list of tuples now I have a list of dictionaries. We need to split again to have an evaluation set . train_split, eval_split = train_test_split(train_jsonl, test_size=0.2, stratify=train_data[&#39;sentiment_class&#39;]) . len(train_split), len(eval_split) . (2588, 647) . We still need to convert the jsonl to the required json format, now for that I will use the script named textcatjsonl_to_trainjson.py in this repo. Let&#39;s download the script from the repo. . !wget -O script.py https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py . --2020-05-30 07:44:50-- https://raw.githubusercontent.com/explosion/spaCy/master/examples/training/textcat_example_data/textcatjsonl_to_trainjson.py Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ... Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 1542 (1.5K) [text/plain] Saving to: ‘script.py’ script.py 100%[===================&gt;] 1.51K --.-KB/s in 0s 2020-05-30 07:44:51 (17.2 MB/s) - ‘script.py’ saved [1542/1542] . %%bash python script.py -m en /content/drive/My Drive/Spacy/train_texts.jsonl /content/drive/My Drive/Spacy python script.py -m en /content/drive/My Drive/Spacy/eval_texts.jsonl /content/drive/My Drive/Spacy . Let&#39;s try to debug again . Validate (again) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 2584 training docs 647 evaluation docs ⚠ 5 training examples also in evaluation data ============================== Vocab &amp; Vectors ============================== ℹ 98859 total words in the data (10688 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 3 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 1 check passed ⚠ 1 warning . It worked !, Thanks to the answerer of this question, now we know that our data format is correct. Turns out there is another command to convert our files to spacy&#39;s JSON format which is mentioned here. . The output is pointing out that the evaluation set has some data leakage. I will try to remove that now. . new_eval = [annot for annot in eval_split if all([annot[&#39;text&#39;] != t[&#39;text&#39;] for t in train_split])] . len(new_eval), len(eval_split) . (641, 647) . We thought there were 5 samples leaking into the training data, it is six here, anyway let&#39;s try to validate the data again. . %%bash (python -m spacy debug-data en /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/eval_texts.json -p &#39;textcat&#39; ) . =========================== Data format validation =========================== ✔ Corpus is loadable =============================== Training stats =============================== Training pipeline: textcat Starting with blank model &#39;en&#39; 2584 training docs 641 evaluation docs ✔ No overlap between training and evaluation data ============================== Vocab &amp; Vectors ============================== ℹ 98859 total words in the data (10688 unique) ℹ No word vectors present in the model ============================ Text Classification ============================ ℹ Text Classification: 3 new label(s), 0 existing label(s) ℹ The train data contains only instances with mutually-exclusive classes. ================================== Summary ================================== ✔ 2 checks passed . We are all set to start training now! . Classifier Training . I have made the command to train in CLI, Please refer the comments for details in the order of the arguments given here . %%bash ## Arguement info # Language of text in which the Model is going to be trained # Path to store model # Training data json path # Evaluation data json path # Pipeline components that we are going to train # Number of iterations in total # Nummber of iterations to wait before improvement in eval accuracy # Pretrained model to start with # version # Augmentation for data(2 params) # Model Architecture for text classifier (cnn + bow) (python -m spacy train en -b en_core_web_sm /content/drive/My Drive/Spacy/Classifier /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/train_texts.json -p &quot;textcat&quot; -n 100 -ne 10 -t2v /content/drive/My Drive/Spacy/Pretrained/fifty_iter/model49.bin -V 0.1 -nl 0.1 -ovl 0.1) . Training pipeline: [&#39;textcat&#39;] Starting with base model &#39;en_core_web_sm&#39; Adding component to base model &#39;textcat&#39; Counting training words (limit=0) Loaded pretrained tok2vec for: [] Textcat evaluation score: F1-score macro-averaged across the labels &#39;POSITIVE, NEGATIVE, NEUTRAL&#39; Itn Textcat Loss Textcat Token % CPU WPS - - - 1 26.738 39.853 100.000 177034 2 5.179 65.120 100.000 157933 3 1.483 76.615 100.000 178008 4 0.686 83.266 100.000 177567 5 0.288 86.236 100.000 169033 6 0.151 88.381 100.000 176679 7 0.090 90.099 100.000 166485 8 0.057 91.000 100.000 171279 9 0.135 92.472 100.000 175907 10 0.028 93.237 100.000 171838 11 0.023 94.147 100.000 175174 12 0.022 94.729 100.000 155840 13 0.021 95.248 100.000 161975 14 0.021 95.485 100.000 168029 15 0.019 95.980 100.000 161440 16 0.018 96.226 100.000 167550 17 0.019 96.713 100.000 172607 18 0.017 96.849 100.000 169682 19 0.017 97.026 100.000 167330 20 0.015 97.299 100.000 173145 21 0.016 97.405 100.000 173020 22 0.015 97.526 100.000 165310 23 0.014 97.704 100.000 165994 24 0.013 97.865 100.000 176089 25 0.013 98.106 100.000 172153 26 0.013 98.201 100.000 172878 27 0.013 98.241 100.000 175909 28 0.012 98.320 100.000 170099 29 0.013 98.400 100.000 175274 30 0.012 98.481 100.000 170135 31 0.012 98.521 100.000 164726 32 0.011 98.536 100.000 171204 33 0.011 98.536 100.000 163467 34 0.011 98.576 100.000 150728 35 0.011 98.696 100.000 172780 36 0.010 98.735 100.000 163459 37 0.010 98.695 100.000 162075 38 0.010 98.750 100.000 165827 39 0.010 98.790 100.000 165852 40 0.010 98.830 100.000 174490 41 0.009 98.870 100.000 165485 42 0.010 98.990 100.000 164896 43 0.009 99.045 100.000 172563 44 0.008 99.045 100.000 169908 45 0.009 99.005 100.000 152600 46 0.008 99.084 100.000 166329 47 0.009 99.084 100.000 173841 48 0.008 99.164 100.000 163433 49 0.008 99.203 100.000 162648 50 0.008 99.258 100.000 177108 51 0.009 99.298 100.000 173468 52 0.008 99.298 100.000 169904 53 0.008 99.298 100.000 171979 54 0.008 99.298 100.000 166437 55 0.008 99.298 100.000 170520 56 0.007 99.337 100.000 172712 57 0.007 99.337 100.000 174966 58 0.007 99.392 100.000 173173 59 0.008 99.392 100.000 173910 60 0.007 99.392 100.000 169447 61 0.007 99.431 100.000 161931 62 0.007 99.471 100.000 106123 63 0.007 99.471 100.000 177625 64 0.007 99.511 100.000 172946 65 0.007 99.511 100.000 173579 66 0.007 99.511 100.000 172204 67 0.007 99.550 100.000 172994 68 0.006 99.550 100.000 174403 69 0.007 99.590 100.000 173900 70 0.006 99.630 100.000 169824 71 0.007 99.630 100.000 171172 72 0.006 99.669 100.000 172633 73 0.006 99.669 100.000 159052 74 0.007 99.669 100.000 174377 75 0.007 99.669 100.000 163376 76 0.006 99.669 100.000 174366 77 0.007 99.669 100.000 175517 78 0.007 99.669 100.000 175583 79 0.006 99.669 100.000 174024 80 0.006 99.669 100.000 174381 81 0.006 99.669 100.000 177120 82 0.006 99.708 100.000 175032 83 0.006 99.708 100.000 173298 84 0.007 99.708 100.000 171622 85 0.006 99.709 100.000 163705 86 0.006 99.709 100.000 175330 87 0.006 99.709 100.000 178355 88 0.006 99.709 100.000 170868 89 0.006 99.709 100.000 164401 90 0.005 99.709 100.000 173884 91 0.006 99.709 100.000 159754 92 0.006 99.709 100.000 177335 93 0.006 99.709 100.000 169868 94 0.006 99.709 100.000 168164 95 0.005 99.709 100.000 151894 96 0.006 99.709 100.000 171580 97 0.005 99.709 100.000 169471 98 0.005 99.724 100.000 156458 99 0.005 99.724 100.000 168167 100 0.006 99.724 100.000 172201 ✔ Saved model to output directory /content/drive/My Drive/Spacy/Classifier/model-final ✔ Created best model /content/drive/My Drive/Spacy/Classifier/model-best . . I also tried to train without the pretrained model (ie)en_core_web_sm, The logs for that are here below. (Uncollapse to view), the results are not very different, the evaluation metrics are off the roof. We need to predict the test data and try to submit to the competition for a better picture of the model. . %%bash ## Arguement info # Language of text in which the Model is going to be trained # Path to store model # Training data json path # Evaluation data json path # Pipeline components that we are going to train # Number of iterations in total # Nummber of iterations to wait before improvement in eval accuracy # Pretrained model to start with # version # Augmentation for data(2 params) # Model Architecture for text classifier (cnn + bow) (python -m spacy train en /content/drive/My Drive/Spacy/Classifier_without_using_websm /content/drive/My Drive/Spacy/train_texts.json /content/drive/My Drive/Spacy/train_texts.json -p &quot;textcat&quot; -n 100 -ne 10 -t2v /content/drive/My Drive/Spacy/Pretrained/fifty_iter/model49.bin -V 0.1 -nl 0.1 -ovl 0.1) . . ✔ Created output directory: /content/drive/My Drive/Spacy/Classifier_without_using_websm Training pipeline: [&#39;textcat&#39;] Starting with blank model &#39;en&#39; Counting training words (limit=0) Loaded pretrained tok2vec for: [] Textcat evaluation score: F1-score macro-averaged across the labels &#39;POSITIVE, NEGATIVE, NEUTRAL&#39; Itn Textcat Loss Textcat Token % CPU WPS - - - 1 26.755 40.980 100.000 166278 2 5.293 65.846 100.000 172083 3 1.506 76.992 100.000 175595 4 0.695 83.314 100.000 173543 5 0.293 86.284 100.000 172609 6 0.156 88.784 100.000 171486 7 0.091 90.136 100.000 161118 8 0.056 91.761 100.000 156752 9 0.112 92.442 100.000 167948 10 0.028 93.329 100.000 162446 11 0.024 94.144 100.000 165753 12 0.022 95.206 100.000 168336 13 0.021 95.769 100.000 161408 14 0.020 96.150 100.000 162562 15 0.019 96.474 100.000 163309 16 0.018 96.775 100.000 168399 17 0.018 97.140 100.000 169412 18 0.017 97.357 100.000 171364 19 0.017 97.503 100.000 172552 20 0.016 97.584 100.000 167923 21 0.016 97.678 100.000 168228 22 0.015 97.934 100.000 158830 23 0.014 98.055 100.000 170587 24 0.013 98.216 100.000 161772 25 0.014 98.256 100.000 160948 26 0.013 98.296 100.000 163401 27 0.013 98.391 100.000 168392 28 0.012 98.351 100.000 162147 29 0.012 98.391 100.000 171460 30 0.012 98.511 100.000 171279 31 0.012 98.511 100.000 161304 32 0.011 98.511 100.000 171576 33 0.011 98.511 100.000 171248 34 0.011 98.591 100.000 166902 35 0.010 98.710 100.000 164750 36 0.011 98.830 100.000 164097 37 0.011 98.790 100.000 170317 38 0.010 98.790 100.000 163521 39 0.010 98.830 100.000 162378 40 0.009 98.964 100.000 164281 41 0.009 98.964 100.000 173645 42 0.011 99.004 100.000 165681 43 0.009 99.044 100.000 165916 44 0.009 99.044 100.000 168503 45 0.008 99.044 100.000 166608 46 0.008 99.123 100.000 170394 47 0.009 99.084 100.000 171932 48 0.008 99.124 100.000 172888 49 0.009 99.084 100.000 169469 50 0.009 99.084 100.000 167170 51 0.008 99.084 100.000 169762 52 0.008 99.124 100.000 166178 53 0.008 99.124 100.000 161415 54 0.008 99.164 100.000 164241 55 0.008 99.164 100.000 172629 56 0.008 99.164 100.000 164923 57 0.008 99.243 100.000 160153 58 0.007 99.243 100.000 171699 59 0.007 99.283 100.000 165604 60 0.008 99.323 100.000 161672 61 0.007 99.362 100.000 157016 62 0.007 99.417 100.000 171005 63 0.007 99.417 100.000 168709 64 0.007 99.417 100.000 170886 65 0.007 99.417 100.000 164144 66 0.007 99.417 100.000 154789 67 0.007 99.417 100.000 162214 68 0.006 99.457 100.000 164467 69 0.006 99.457 100.000 169052 70 0.006 99.496 100.000 168125 71 0.007 99.496 100.000 164085 72 0.006 99.575 100.000 163078 73 0.006 99.575 100.000 162955 74 0.006 99.575 100.000 166206 75 0.007 99.575 100.000 164477 76 0.006 99.575 100.000 169814 77 0.006 99.575 100.000 162547 78 0.006 99.575 100.000 168980 79 0.007 99.575 100.000 172534 80 0.006 99.575 100.000 161797 81 0.007 99.575 100.000 162510 82 0.006 99.575 100.000 172787 83 0.005 99.535 100.000 159187 84 0.006 99.535 100.000 168200 85 0.005 99.614 100.000 167757 86 0.006 99.614 100.000 158842 87 0.006 99.654 100.000 166849 88 0.005 99.654 100.000 162507 89 0.006 99.654 100.000 167156 90 0.005 99.654 100.000 97872 91 0.006 99.654 100.000 162397 92 0.006 99.708 100.000 168693 93 0.005 99.708 100.000 167645 94 0.005 99.708 100.000 163485 95 0.006 99.708 100.000 171732 96 0.005 99.708 100.000 165686 97 0.005 99.708 100.000 167604 98 0.005 99.708 100.000 166435 99 0.005 99.708 100.000 161645 100 0.005 99.708 100.000 171467 ✔ Saved model to output directory /content/drive/My Drive/Spacy/Classifier_without_using_websm/model-final ✔ Created best model /content/drive/My Drive/Spacy/Classifier_without_using_websm/model-best . . Prediction on test data . test_data = pd.read_csv(DATA_PATH/&#39;test.csv&#39;, index_col=0) test_data.head() . original_text lang retweet_count original_author . id . 1.246628e+18 3. Yeah, I once cooked potatoes when I was 3 y... | en | 0 | LToddWood | . 1.245898e+18 Happy Mother&#39;s Day to all the mums, step-mums,... | en | 0 | iiarushii | . 1.244717e+18 I love the people from the UK, however, when I... | en | 0 | andreaanderegg | . 1.245730e+18 Happy 81st Birthday Happy Mother’s Day to my m... | en | 1 | TheBookTweeters | . 1.244636e+18 Happy Mothers day to all those wonderful mothe... | en | 0 | andreaanderegg | . Clean test data . We will clean the test data of links with regex as well. . test_data[&#39;original_text&#39;].replace( # Regex pattern to match : the text to replace with {&#39;(https?: / /.*|pic.*)[ r n]*&#39; : &#39;&#39;}, regex=True, inplace=True) . test_data.shape . (1387, 4) . list_of_test_texts = test_data[&#39;original_text&#39;].tolist() . Let&#39;s load the Spacy model from our training . import spacy textcat_mod = spacy.load(DRIVE_PATH.parent/&#39;Classifier/model-best&#39;) . I will try to fasten the prediction by using multithreading as mentioned here . d = textcat_mod(list_of_test_texts[0]) d.cats . {&#39;NEGATIVE&#39;: 0.020245620980858803, &#39;NEUTRAL&#39;: 0.9727445840835571, &#39;POSITIVE&#39;: 0.007009787950664759} . max(d.cats, key=lambda x: d.cats[x]) . &#39;NEUTRAL&#39; . label_map = {&#39;POSITIVE&#39;:1, &#39;NEGATIVE&#39;:-1, &#39;NEUTRAL&#39;:0} . preds = [] for doc in textcat_mod.pipe(list_of_test_texts, n_threads=4, batch_size=100): pred_cls = max(doc.cats, key=lambda x: doc.cats[x]) preds.append(label_map[pred_cls]) . len(preds), len(list_of_test_texts) . (1387, 1387) . Let&#39;s form the submission . sub_df = pd.DataFrame( preds, index=test_data.index, columns=[&#39;sentiment_class&#39;] ) . sub_df.shape . (1387, 1) . sub_df.head() . sentiment_class . id . 1.246628e+18 0 | . 1.245898e+18 0 | . 1.244717e+18 -1 | . 1.245730e+18 -1 | . 1.244636e+18 0 | . sub_df.to_csv(DRIVE_PATH.parent/&#39;submission.csv&#39;) . The submitted predictions scored a mere 39/100 in weighted f1-score, that&#39;s disappointing. -_- . Let&#39;s analyze the predictions . Prediction distribution . sub_df[&#39;sentiment_class&#39;].value_counts().plot(kind=&#39;bar&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fed20ed1b70&gt; . sub_df[&#39;sentiment_class&#39;].value_counts() . 0 847 1 277 -1 263 Name: sentiment_class, dtype: int64 . This looks very similar to the train data . train_data[&#39;sentiment_class&#39;].value_counts() . 0 1701 -1 769 1 765 Name: sentiment_class, dtype: int64 . What would have gone wrong?, I guess what I can do is try another method(traditional). Coming up in another post .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/eda/sentiment/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "relUrl": "/nlp/eda/sentiment/2020/05/30/Deep_learning_approach_for_text_classification_with_spacy.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "Mother's day Sentiment analysis",
            "content": "This notebook explores the data from HackerEarth Machine learning challenge for Mother&#39;s day, The following is the Problem description. . You work in an event management company. On Mother&#39;s Day, your company has organized an event where they want to cast positive Mother&#39;s Day related tweets in a presentation. Data engineers have already collected the data related to Mother&#39;s Day that must be categorized into positive, negative, and neutral tweets. You are appointed as a Machine Learning Engineer for this project. Your task is to build a model that helps the company classify these sentiments of the tweets into positive, negative, and neutral. . Download the data . import requests zip_file = requests.get(&#39;https://he-s3.s3.amazonaws.com/media/hackathon/hackerearth-test-draft-1-102/predicting-tweet-sentiments-231101b4/fa62f5d69a9f11ea.zip?Signature=v92IcNfljnopA9xQoCPCftwg1g0%3D&amp;Expires=1590318817&amp;AWSAccessKeyId=AKIA6I2ISGOYH7WWS3G5&#39;) with open(&#39;data.zip&#39;, &#39;wb&#39;) as f: f.write(zip_file.content) . !unzip data.zip . Archive: data.zip creating: dataset/ inflating: dataset/train.csv inflating: dataset/test.csv . %load_ext google.colab.data_table . Peek at the data . from pathlib import Path import pandas as pd DATA_PATH = Path(&#39;dataset/&#39;) train_data = pd.read_csv(DATA_PATH/&#39;train.csv&#39;, index_col=0) train_data.head(100) . original_text lang retweet_count original_author sentiment_class . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | en | 0 | BeenXXPired | 0 | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | en | 1 | FestiveFeeling | 0 | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | en | 0 | KrisAllenSak | -1 | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | en | 0 | Queenuchee | 0 | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | en | 0 | brittan17446794 | -1 | . ... ... | ... | ... | ... | ... | . 1.244092e+18 Happy Mothers Day from all of us at Kellyzola ... | en | 1 | design_pia | 0 | . 1.246529e+18 Happy Mothers Day katiebrooks_88 . I do my lev... | en | 1 | missny99 | 0 | . 1.244747e+18 RESPECT, GRATITUDE and ADORATION to all MOTHER... | en | 0 | EgbertsTreasure | 0 | . 1.245141e+18 It takes someone really brave to be a mother, ... | en | 0 | momaferd | -1 | . 1.245903e+18 On Mother&#39;s Day, I&#39;m sharing this video again ... | en | 0 | GotMommyBrain | -1 | . 100 rows × 5 columns . train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Float64Index: 3235 entries, 1.24502457848689e+18 to 1.24540908968687e+18 Data columns (total 5 columns): # Column Non-Null Count Dtype -- -- 0 original_text 3235 non-null object 1 lang 3231 non-null object 2 retweet_count 3231 non-null object 3 original_author 3235 non-null object 4 sentiment_class 3235 non-null int64 dtypes: int64(1), object(4) memory usage: 151.6+ KB . So there are four columns . 1. The tweet text 2. Language of the tweet 3. Number of Retweets 4. Sentiment group (+ve, -ve, neu) . There are also some missing values in lang and retweet_count columns. . Let&#39;s look at the number of languages of tweets this dataset has . train_data[&#39;lang&#39;].value_counts() . en 2994 pink Peruvian opal! via 4 Find More 2 &amp;gt 2 WORLDS OKAYEST MOTHER! &amp;lt 2 ... 0.4754834129 1 -0.0064143617 1 -0.3850425633 1 0.7885519508 1 -0.2758448854 1 Name: lang, Length: 232, dtype: int64 . I see that most tweets are in English, but it seems that some entries in the data are not actually indicating any language. . print(&#39;Total Other language tweets: &#39;, train_data.shape[0].item()-2994) . Total Other language tweets: 241 . Let&#39;s see if they are actually in different languages. . train_data.loc[train_data[&#39;lang&#39;] != &#39;en&#39;, :] . original_text lang retweet_count original_author sentiment_class . id . 1.244590e+18 Happy mothersday to all those celebrating toda... | -0.0138325017 | en | 11 | 0 | . 1.244823e+18 Exactly what my late mum aka hype mama would d... | -0.9677309496 | en | 0 | 0 | . 1.246515e+18 It&#39;s the world&#39;s most difficult job No sick le... | -0.3876905537 | en | 1 | 0 | . 1.244226e+18 Happy Mother’s Day! To all the amazing Mums ou... | 0.5309553602 | en | 0 | 0 | . 1.244419e+18 Happy Mothers Day , Mummy! Nearly 90 and still... | -0.045423609 | en | 2 | 0 | . ... ... | ... | ... | ... | ... | . 1.246356e+18 Happy Mothers Day All My Nigerian Massive Fami... | 0.2117897904 | en | 0 | 0 | . 1.245821e+18 HAPPY MOTHERS DAY ! HAPPY MOTHERS DAY !!Now th... | -0.8739088126 | en | 0 | 0 | . 1.246719e+18 Isan Elba celebrates Happy Mothers’ Day with h... | 0.4945825935 | en | 0 | 0 | . 1.245565e+18 Still miss my mom she passed 18th of March 201... | 0.6927740873 | 0 | 0 | 0 | . 1.245871e+18 I’m so thankful for my 5, healthy happy joy br... | 0.2522315249 | en | 1 | 0 | . 241 rows × 5 columns . I think all the tweets are in English, you can see that the en value representing English is misplaced in other columns retweet_count and original_author. They are either filled with random float numbers or some other tweet text, maybe the data was not scraped properly? . prHowever from what I know, I think we can ignore the columns other than the original_text column which has the tweet text, which is the most important for analysis for sentiment of text. You can see that almost all of the texts have some link embedded to them, They are not likely to help in getting to know the sentiment of the tweeter. . The pattern here is that most of the links either are images which start with pic.twitter.* or links referring to other sites like http://www.instagram.*, We should be able to identify the pattern with a Regular expression. Let&#39;s try to test the assumption. . Cleaning links . import re sample_tweet = &quot;&quot;&quot; Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... https://www. facebook.com/36426361058377 0/posts/1130869587256498/ … #happymothersday2020 pic.twitter.com/ZCZOF1xb6K wo&quot;&quot;&quot; . print(re.sub(&#39;(https?: / /.*|pic.*)[ r n]*&#39;, &#39;&#39;, sample_tweet)) . Happy Mothers Day Mothers are very valuable to the society because they build families that make up the general population of every nation. They also contribute immensely to nation building and capacity building as caregivers..... . This will remove most of the links, but it will also remove the text between the links like in the case above the #happymothersday2020 hashtag is removed. . Let&#39;s apply the regex to the texts . (train_data[&#39;original_text&#39;].replace({&#39;(https?: / /.*|pic.*)[ r n]*&#39;:&#39;&#39;}, regex=True).to_frame()).head() . original_text . id . 1.245025e+18 Happy #MothersDay to all you amazing mothers o... | . 1.245759e+18 Happy Mothers Day Mum - I&#39;m sorry I can&#39;t be t... | . 1.246087e+18 Happy mothers day To all This doing a mothers ... | . 1.244803e+18 Happy mothers day to this beautiful woman...ro... | . 1.244876e+18 Remembering the 3 most amazing ladies who made... | . Visualize the words . Prepare the mask . import numpy as np from io import BytesIO from PIL import Image from PIL.ImageOps import invert img_file = requests.get(&#39;http://www.agirlandagluegun.com/wp-content/uploads/blogger/-ox_bazyTgmQ/TcNwpMfduLI/AAAAAAAAOX4/hcxXcz0A8-A/s1600/scan0001.jpg&#39;) img = BytesIO(img_file.content) img_mask = np.array(Image.open(img)) # Check if the image was downloaded properly img_mask.shape . (718, 1600, 3) . # From https://www.datacamp.com/community/tutorials/wordcloud-python def transform_format(val): # Just trying to invert pixels if any(v == 255 for v in val): return 0 else: return 255 # Transform the mask into a new one that will work with the function: transformed_mask = np.ndarray((img_mask.shape[0], img_mask.shape[1]), np.int32) for i in range(len(img_mask)): transformed_mask[i] = list(map(transform_format, img_mask[i])) . from wordcloud import WordCloud, STOPWORDS import matplotlib.pyplot as plt wc = WordCloud(background_color=&quot;white&quot;, max_words=2000, mask=transformed_mask, stopwords=set(STOPWORDS), contour_width=1, contour_color=&#39;steelblue&#39;) # generate word cloud wc.generate(&#39; n&#39;.join(train_data[&#39;original_text&#39;].values.tolist())) . &lt;wordcloud.wordcloud.WordCloud at 0x7f0482536358&gt; . %matplotlib inline plt.figure(figsize=(18,18)) plt.axis(&quot;off&quot;) plt.imshow(wc) . &lt;matplotlib.image.AxesImage at 0x7f04825f85f8&gt; . Now we can try to model this text with any method that we would like! and that is coming up next. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "relUrl": "/nlp/eda/sentiment/2020/05/24/Hackerearth_mothers_day_sentiment.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Building தமிழ் language model",
            "content": "Introduction . In this post, I will try to model தமிழ் (Tamil), I have already prepared the data for the language model in the kaggle notebook here, A language model will be useful for many tasks such as text classification, information retrieval etc. . What is a language model? . From what I know, Language model is a machine&#39;s way of understanding a language, technically it is defined as a probability distribution over a sequence of words[^1], by helping the machine to understand language, we can use them text-based classifiers, chatbots and for other NLP tasks in that language. . How do we train a language model? . I recall the times when you was going for school, I was given language lessons for English and தமிழ், The languages were different in grammar, dialects, sounds etc., after some lessons about the words and letters present in the language, both of them were taught to all in the same manner. . We would have lessons in textbooks, of which most of them are stories, biographies and history. Most of the exercises at the end of each lesson are . Fill in the blanks like The ____ rises in the east. or சூரியன் உதிக்கும் திசை _____ | Write short descriptive answers for question based on the lesson. | Maybe longer essays on general What-if scenarios from the lesson. | . We know that to answer them, it required a decent understanding of the language&#39;s grammar, which in turn, is taught to the children by making them read and write the questions and answers. . Now how can we teach a machine to understand and learn the language?, We had textbooks to read and learn about the language, So the machine also needs data, like our textbooks (but not necessarily the same ones with which we learn) to learn the language. . So how can we ensure that the machine is learning properly?, We test it with Fill in the blanks kind of exercises and let it guess the next possible word for the sequence of words we give it. It will not be easy for the machine but a little bit of learning is enough to use the model for other purposes. . We cannot directly give the raw text data to the model, We need to convert them to sequence of words to help it learn the flow of the words . Things we need for a language model . A decent amount of raw text data, more about that here . | A language tokenizer, more about that here and here . | . This notebook is executed on kaggle, so the paths mentioned here will be needed to change if you run in your own environment. Setup libraries and paths . Set seed for reproducibility . Load text data from csv . LANG_DATA = pd.read_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, index_col=[0]) LANG_DATA.head() . We have the url, article_id and title as additional information about the text, Let&#39;s check the average length of the article text. . Exploration . The total articles we have are 131162 . Remove empty articles from the dataframe . LANG_DATA.dropna(axis=&#39;rows&#39;, inplace=True) LANG_DATA.info() . Length of articles . The average length of each article is 1370 words . We had one empty article, I suppose. . Prepare Text data for Language model . processor = SPProcessor(lang=&#39;ta&#39;, sp_model=DATA_PATH/&#39;tamil_tok.model&#39;, sp_vocab=DATA_PATH/&#39;tamil_tok.vocab&#39;) . Set batch size . bs = 16 . data_lm = (TextList.from_df(LANG_DATA, path=DATA_PATH, cols=&#39;text&#39;, processor=processor) # Split out some data in a random fashion for testing .split_by_rand_pct(0.1) # This is where we convert the raw text data to # sequences of words .label_for_lm() # We want to do a language model so we label accordingly .databunch(bs=bs)) . data_lm.sanity_check() . Let&#39;s save the language model data to skip the processing above next time. . data_lm.save(DATA_PATH/&#39;data_lm.pkl&#39;) . Let&#39;s have a look at the tokenized data from the sentencepiece tokenizer. . data_lm.show_batch() . bos means beginning of the sentence. | eos means end of the sentence. | xx maj used to indicate the next word begins with a capital in the original text. more about this can be found here and here | . Train the language model . Initialize model . config = awd_lstm_lm_config.copy() config[&#39;qrnn&#39;] = True config[&#39;n_hid&#39;] = 1550 config[&#39;n_layers&#39;] = 4 # This is a classification metric, # determines how well can the model # narrow the choice of words from it&#39;s # vocabulary for the next prediction. perplexity = Perplexity() learn = language_model_learner(data_lm, arch=AWD_LSTM, config=config, pretrained=False, metrics=[accuracy, perplexity], ).to_fp16() # gradient clipping learn.clip = 0.1 learn.model_dir=DATA_PATH . Find proper learning rate . learn.lr_find() . learn.recorder.plot(suggestion=True) . Get suggested learning rate . min_grad_lr = learn.recorder.min_grad_lr min_grad_lr . Start training . Stage - 1 . learn.fit_one_cycle(10, min_grad_lr, # Momentums, just a try! div_factor=10.0, pct_start=0.8, moms=(0.75,0.65), callbacks=[SaveModelCallback(learn, every=&#39;improvement&#39;, monitor=&#39;perplexity&#39;, name=&#39;best_st1&#39;), CSVLogger(learn, filename=DATA_PATH/&#39;history&#39;, append=True)]) . Save the intermediate results . learn.load(&#39;best_st1&#39;); learn.save(&#39;ta-wiki-stage1&#39;) learn.save_encoder(&#39;ta-wiki-enc-stage1&#39;) . You can chop and change the parameters, to get a better model, find the latest run of the notebook on kaggle, please upvote there if you liked this. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/20/tamil-language-model.html",
            "date": " • Apr 20, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "Testing out தமிழ் language tokenizer",
            "content": "You can find the notebook in which I built the tokenizers here . Introduction . This notebook is intended to experiment with different tokenizers built previously, with varying vocab_size values like 8000, 16000, 20000, 30000, So how do we exactly test a tokenizer?, this brings us to why do we need a tokenizer in the first place. . In English language we can easily find meaningful units of a sentence, by whitespaces. But it is not that easy in other languages, Like in தமிழ், Consider the following sentence, . இந்த தொழிற்சாலை பணப்பற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது., this loosely translates to something like The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. . We need to focus on the compound word பணப்பற்றாக்குறை which means lack of cash, that compound word is actually a combination of two words பணம் and பற்றாக்குறை representing Cash and Deficiency/lack of A tokenizer for தமிழ், should split the words into two as mentioned above. . Since Tokenization is the process of demarcating and possibly classifying sections of a string of input characters[1](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization) or in simple words identifying linguistically meaningful units [2](https://stackoverflow.com/questions/17314506/why-do-i-need-a-tokenizer-for-each-language) for further processing. We need it for a language model which essentially built to understand the language that it is built for. . Since we built the tokenizer in an unsupervised[3](https://github.com/google/sentencepiece#sentencepiece) way, There are no numerical ways of gauging the efficiency of the tokenization (AFAIK), so we are left with to try and tokenize some random sentences with the model, and check the tokenizer ourselves. . Sentences for testing tokenization . I have chosen some sentences at random, Check the comments above each for their translation in English. . sentences = [ # Sita is a mischievous girl &#39;சீதா ஒரு குறும்பு பெண்&#39;, # I remember my childhood &#39;எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது&#39;, # India has successfully tested the Agni-5 missile for the fourth time from Abdul Kalam Island (Wheeler Island) in Odisha. &#39;இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது.&#39;, # The European Union&#39;s Galileo satellite system is in operation. It is believed to be the world&#39;s most accurate high-precision positioning system. &#39;ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது.&#39;, # The factory has come in to operation after over 17 years of series of disruptions, including lack of cash. &#39;இந்த தொழிற்சாலை பணபற்றாக்குறை முதலான பல்வேறு இடைஞ்சல்களை தாண்டி 17 ஆண்டுகள் கழித்தே செயல்பாட்டுக்கு வந்துள்ளது.&#39;, # Citizens, witnesses and warriors mourn the death of their king. It is up to the department to regret any loss. &#39;தம் மன்னன் இறந்ததற்கு குடிமக்களும் சான்றோரும் வீரர்களும் வருந்திப் பாடுவது கையறுநிலை என்னும் துறையாகும். எந்த இழப்பையும் எண்ணி வருந்துவது கையறுநிலைத் துறைக்குரியது.&#39;, # The Poems from Sangam Tamil Literature portrays the trading feats of early Tamilian,Tamilians traded across seas and other countries &#39;சங்கத்தமிழ்க் கவிதைகள் பழந்தமிழர்தம் வணிகச்சிறப்பைப் பறைசாற்றி நிற்கின்றன. தமிழர் கடல்கடந்து, அயல்நாடுகளிலும் வணிகம் செய்தனர் என்ற செய்திகளைச் சங்கப்பாடல்கள்வழி அறிகின்றோம்.&#39;, # Everyone stood up to call for a national flag at a school event. &#39;பள்ளி நிகழ்ச்சி ஒன்றில் தேசியக் கொடி ஏற்றுமாறு அழைக்க அவரும் எழுந்தார் அனைவரும் எழுந்து நின்றனர்&#39;, ] . Let&#39;s try to tokenize each one of them. . Begin Experimentation . Initial setup . import sentencepiece as spm from pathlib import Path from IPython.core.display import display, HTML from string import Template sp = spm.SentencePieceProcessor() TOK_PATH = &#39;/kaggle/input/building-a-tokenizer-for-tamil-with-sentencepiece/tokenizer&#39; MODEL_PATHS = [p for p in Path(TOK_PATH).glob(&#39;*.model&#39;)] . Sentence Tokenization . I will try to comment on the tokenization with a limited knowledge of தமிழ் grammar, I will refer to the model by the vocab size they are built with (ie) 8000, 16000, 20000, 30000 . First sentence சீதா ஒரு குறும்பு பெண் . tokenize_and_display_results(sentences[0]) . tok_30000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_20000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . tok_8000_size | ▁சீதா | ▁ஒரு | ▁குறு | ம்பு | ▁பெண் | . tok_16000_size | ▁சீதா | ▁ஒரு | ▁குறும்ப | ு | ▁பெண் | . குறும்பு is actually not a compound word, so I think apart from the model with 8000 size all other models got it right. . Next sentence எனக்கு என் குழந்தைப் பருவம் நினைவிருக்கிறது . tokenize_and_display_results(sentences[1]) . tok_30000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_20000_size | ▁எனக்கு | ▁என் | ▁குழந்தைப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . tok_8000_size | ▁என | க்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவ | ம் | ▁நினைவ | ிருக்கிறது | . tok_16000_size | ▁எனக்கு | ▁என் | ▁குழந்தை | ப் | ▁பருவம் | ▁நினைவ | ிருக்கிறது | . In this sentence I remember my childhood the model with 20000 and 30000 got the tokenization right. 16000 was just close, because the letter ப் does not have a meaning on it&#39;s own. . Next sentence இந்தியா அக்னி-5 வகை ஏவுகணையை நான்காவது முறையாக வெற்றிகரமாக ஒடிசாவிலுள்ள அப்துல் கலாம் தீவிலிருந்து (வீலர் தீவு) சோதித்தது. . tokenize_and_display_results(sentences[2]) . tok_30000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசாவில | ுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_20000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . tok_8000_size | ▁இந்தியா | ▁அக் | னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | வ | ிலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோ | தி | த்தது | . | . tok_16000_size | ▁இந்தியா | ▁அக்னி | - | 5 | ▁வகை | ▁ஏவுகணை | யை | ▁நான்காவது | ▁முறையாக | ▁வெற்றிகரமாக | ▁ஒடிசா | விலுள்ள | ▁அப்துல் | ▁க | லாம் | ▁தீ | விலிருந்து | ▁ | ( | வீ | லர் | ▁தீவு | ) | ▁சோதி | த்தது | . | . This is interesting, We can probably say that 8000 vocab model is not doing so well. Looking at the others, both the 16000 vocab model and 20000 split ஒடிசாவிலுள்ள meaning to refer something in Odisha into ஒடிசா and விலுள்ள, I think the right split is ஒடிசாவில் and உள்ள, which the 30000 got right. . Next Sentence . ஐரோப்பிய ஒன்றியத்தின் கலிலியோ செயற்கைகோள் அமைப்பு செயல்பாட்டுக்கு வந்துள்ளது. இது உலகின் மிக துல்லியமான செய்மதி இடஞ்சுட்டலாக இருக்கும் என நம்பப்படுகிறது. . tokenize_and_display_results(sentences[3]) . tok_30000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_20000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_8000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லி | யோ | ▁செயற்கை | கோள | ் | ▁அமைப்பு | ▁செயல்பாட்ட | ுக்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லிய | மான | ▁செய் | மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . tok_16000_size | ▁ஐரோப்பிய | ▁ஒன்றியத்தின் | ▁கலி | லியோ | ▁செயற்கை | கோள் | ▁அமைப்பு | ▁செயல்பாட்டு | க்கு | ▁வந்துள்ளது | . | ▁இது | ▁உலகின் | ▁மிக | ▁துல்லியமான | ▁செய்மதி | ▁இட | ஞ்சு | ட்ட | லாக | ▁இருக்கும் | ▁என | ▁நம்பப்படுகிறது | . | . All of them may have got it wrong with Galileo which is split into கலி(Gali) and லியோ(leo), (Maybe I am wrong on this part) and in GPS இடஞ்சுட்டலாக may be called as இடம்சுட்டல்(can be loosely translated to Location Pointer) should have been split like இடம் and சுட்டல் and ஆக, Maybe I am expecting too much. . Conclusion . I will stop here for this blogpost, You can try the other sentences if you want by forking the notebook here . Please comment if you think something is wrong somewhere, share it if you have found this interesting. .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/19/Testing-sentencepiece-tokenizer-for-Tamil.html",
            "date": " • Apr 19, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "Wiki data extraction",
            "content": "This post explains how I downloaded and extracted wiki dump archive using wikiextractor. . This code was used on a kaggle environment, which can be found here. You can fork and change as per your needs. . Setup . Import required libraries . # For JSON data extraction import json # For path manipulations from pathlib import Path # For preprocessing import string # For deleting files and folders import shutil # To clone necessary files import git # To download the dump import requests as req # To use wikiextractor import subprocess # To clean and process data import pandas as pd . Setup output paths . DATA_PATH = Path(&#39;/kaggle/working/&#39;) EXTRACTED_PATH = DATA_PATH/&#39;extracted&#39; EXTRACTED_PATH.mkdir() . Data Extraction . Request file from wikipedia. . You can use a different link here. . bzip_file = req.get(&#39;https://dumps.wikimedia.org/tawiki/latest/tawiki-latest-pages-articles.xml.bz2&#39;) . Save request content to a file . with open(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;, &#39;wb&#39;) as f: f.write(bzip_file.content) . Clone wiki extractor from github . Thanks to attardi and GitPython . git.Git(str(DATA_PATH)).clone(&quot;https://github.com/attardi/wikiextractor.git&quot;) . Use wikiextractor to get data from the dump . This runs the wikiextractor cloned from github. . run_stat = subprocess.run( [&#39;python&#39;, # File to run str(DATA_PATH/&#39;wikiextractor/WikiExtractor.py&#39;), # Processing parameters to get as json &#39;-s&#39;, &#39;--json&#39;, # Directory to store Extracted text &#39;-o&#39;, str(DATA_PATH/&#39;extracted&#39;), # Archive file to extract from str(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;)] ) . Get list of files extracted from the extraction folder . files_extracted = [str(f) for f in EXTRACTED_PATH.rglob(&quot;*/*&quot;)] . Load json data from the files, since all files are stored as json we can load them like below, This gives us a list of dictionaries . lang_text = [json.loads(line) for _file in files_extracted for line in open(_file)] . or this . lang_text = [] for _file in files_extracted: with open(_file, &#39;r&#39;) as f: file_lines = f.readlines() for line in file_lines: lang_text.append(json.loads(line)) . Preprocessing . You can use any of the following, or skip the preprocessing altogether if you wish so. . Filter English words from text . Check each word after removing their punctuations, if it is an english word . filter_english = lambda text: &#39; &#39;.join([ word for word in text.split() if word.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)).isalpha() is False ]) . or . def filter_english(text): words = [] # Spltting words for word in text.split(): # Replace symbols trans_table = str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation) word = word.translate(trans_table) if not word.isalpha(): words.append(word) return &#39; &#39;.join(words) . Form dataframe and apply preprocessing . # Since we have a list of dictionaries. lang_df = pd.DataFrame(lang_text) lang_df[&#39;text&#39;] = lang_df[&#39;text&#39;].apply(filter_english) . Store the output in compressed format . lang_df.to_csv(DATA_PATH/&#39;filtered_data.csv.tar.gz&#39;, header=True) . The above saved file can be loaded with pd.read_csv. . You can find the full code for this in Github gist or with the output in kaggle. . Clean up the downloaded files, (if required) . shutil.rmtree(str(EXTRACTED_PATH)) shutil.rmtree(str(DATA_PATH/&#39;wikiextractor&#39;)) Path(DATA_PATH/&#39;tawiki-latest-pages-articles.xml.bz2&#39;).unlink() .",
            "url": "https://mani2106.github.io/Blog-Posts/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "relUrl": "/data-cleaning/language-model/2020/04/14/wiki-data-extraction.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "Building தமிழ் language tokenizer",
            "content": "You can find the blog post regarding extraction here and kaggle notebook with output here . Import required libraries . from pathlib import Path import sentencepiece as spm import pandas as pd . Read data from csv . lang_data = pd.read_csv(&#39;../input/tamil-wiki-data-extraction/filtered_data.csv.tar.gz&#39;, index_col=[0]) lang_data.head() . id url title text . 0 48482 | https://ta.wikipedia.org/wiki?curid=48482 | தென் துருவம் | தென் துருவம் தென் முனை தென் துருவம் என்பது புவ... | . 1 48485 | https://ta.wikipedia.org/wiki?curid=48485 | ஆர்க்டிக் வட்டம் | ஆர்க்டிக் வட்டம் ஆர்க்டிக் வட்டம் என்பது ஐந்து... | . 2 48486 | https://ta.wikipedia.org/wiki?curid=48486 | நாஞ்சில் நாடன் | நாஞ்சில் நாடன் நாஞ்சில் நாடன் பிறப்பு திசம்பர்... | . 3 48492 | https://ta.wikipedia.org/wiki?curid=48492 | டிக்கோயா | டிக்கோயா டிக்கோயா இலங்கையின் மத்திய மாகாணத்தின... | . 4 48493 | https://ta.wikipedia.org/wiki?curid=48493 | நள்ளிரவுச் சூரியன் | நள்ளிரவுச் சூரியன் நள்ளிரவுச் சூரியன் அல்லது த... | . lang_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 133412 entries, 0 to 133411 Data columns (total 4 columns): id 133412 non-null int64 url 133412 non-null object title 133412 non-null object text 133412 non-null object dtypes: int64(1), object(3) memory usage: 5.1+ MB . Setup paths . OUTPUT_DIR = Path(&#39;/kaggle/working&#39;) TEXTS_DIR = OUTPUT_DIR/&#39;texts&#39; TOK_DIR = OUTPUT_DIR/&#39;tokenizer&#39; # Create directories TOK_DIR.mkdir() TEXTS_DIR.mkdir() . Prepare texts . We can pass a list of files as a comma seperated string according to documentation, So we can store each article in a text file and pass the names in a comma seperated string. . for t in lang_data.itertuples(): file_name = Path(TEXTS_DIR/f&#39;text_{t.Index}.txt&#39;) file_name.touch() with file_name.open(&#39;w&#39;) as f: f.write(t.text) . len([t for t in TEXTS_DIR.iterdir()]), lang_data.shape[0] . (133412, 133412) . All the files have been converted to texts . Train sentencepiece model . Let&#39;s make a comma seperated string of filenames . files = &#39;,&#39;.join([str(t) for t in TEXTS_DIR.iterdir()]) files[:100] . &#39;/kaggle/working/texts/text_40902.txt,/kaggle/working/texts/text_44212.txt,/kaggle/working/texts/text&#39; . We must find the right vocab_size for the tokenizer, that can be done only by testing the tokenizer after building onw . for v in 8000, 16000, 20000, 30000: api_str = f&quot;&quot;&quot;--input={files} --vocab_size={v} --model_type=unigram --character_coverage=0.9995 --model_prefix={str(TOK_DIR)}/tok_{v}_size --max_sentence_length=20000&quot;&quot;&quot; print(&quot;Training with vocab set as:&quot;, v) spm.SentencePieceTrainer.train(api_str) . Training with vocab set as: 8000 Training with vocab set as: 16000 Training with vocab set as: 20000 Training with vocab set as: 30000 . Cleanup . !rm -rf /kaggle/working/texts/ . Let&#39;s test the models in another notebook, you can find the outputs in this kaggle notebook .",
            "url": "https://mani2106.github.io/Blog-Posts/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "relUrl": "/nlp/language-model/%E0%AE%A4%E0%AE%AE%E0%AE%BF%E0%AE%B4%E0%AF%8D/2020/04/14/building-a-tokenizer-for-tamil-with-sentencepiece.html",
            "date": " • Apr 14, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Building a Pokemon Classifier",
            "content": ". In this notebook, I used the pokemon images dataset from here but unfortuantely it is not available now. . Load required libraries . from fastai.vision import * from fastai.metrics import error_rate from fastai.callbacks.tracker import ReduceLROnPlateauCallback, SaveModelCallback from fastai.callbacks import CSVLogger . . Prepare Data for training . path = Path(&quot;.&quot;) . . Form data bunch object from the folders. . data = ImageDataBunch.from_folder(path, train=&quot;.&quot;, ds_tfms=get_transforms(), size=128, bs=64, valid_pct=0.2).normalize(imagenet_stats) . Check the number of different pokemon images that we have. . len(data.classes) . 928 . Creating a CNN model from architecture of resnet18. I could use a bigger model but I would not be able to serve them from Google or OneDrive because of the size. . Error Rate is 1-accuracy. | Using mixup for better regularization. | Converting the operations to be performed in a lower precision, more | . learn = cnn_learner(data, models.resnet18, metrics=error_rate).mixup().to_fp16() . Adding callbacks to monitor the training process and . Reduce the learning_rate by using the ReduceLROnPlateauCallback. | Saving the model on every improvement in error_rate | Log the training stats in a csv file. | . callbacks_list = [ ReduceLROnPlateauCallback(learn=learn, monitor=&#39;error_rate&#39;, factor=1e-6, patience=5, min_delta=1e-5), SaveModelCallback(learn, mode=&quot;min&quot;, every=&#39;improvement&#39;, monitor=&#39;error_rate&#39;, name=&#39;best&#39;), CSVLogger(learn=learn, append=True) ] . Start Training . Now, All the setup has been made, Let&#39;s train the model with default parameters, for 15 epochs. . learn.fit_one_cycle(15, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 7.173859 | 6.631011 | 0.985798 | 01:09 | . 1 | 6.246106 | 5.397111 | 0.870562 | 01:08 | . 2 | 5.001963 | 3.665833 | 0.672144 | 01:07 | . 3 | 4.327330 | 2.772682 | 0.540881 | 01:06 | . 4 | 3.941842 | 2.320177 | 0.469669 | 01:06 | . 5 | 3.648211 | 2.069086 | 0.420978 | 01:06 | . 6 | 3.423512 | 1.901359 | 0.372895 | 01:06 | . 7 | 3.328791 | 1.758360 | 0.343883 | 01:06 | . 8 | 3.140401 | 1.657776 | 0.326841 | 01:06 | . 9 | 3.044241 | 1.591135 | 0.313857 | 01:07 | . 10 | 2.940413 | 1.538893 | 0.300670 | 01:06 | . 11 | 2.759924 | 1.502491 | 0.290931 | 01:07 | . 12 | 2.781063 | 1.474272 | 0.283628 | 01:06 | . 13 | 2.761597 | 1.457427 | 0.282816 | 01:06 | . 14 | 2.700450 | 1.459171 | 0.280179 | 01:07 | . Better model found at epoch 0 with error_rate value: 0.9857983589172363. Better model found at epoch 1 with error_rate value: 0.870561957359314. Better model found at epoch 2 with error_rate value: 0.6721444725990295. Better model found at epoch 3 with error_rate value: 0.5408805012702942. Better model found at epoch 4 with error_rate value: 0.46966931223869324. Better model found at epoch 5 with error_rate value: 0.4209778904914856. Epoch 6: reducing lr to 2.599579409433508e-09 Better model found at epoch 6 with error_rate value: 0.37289512157440186. Better model found at epoch 7 with error_rate value: 0.34388312697410583. Better model found at epoch 8 with error_rate value: 0.3268411457538605. Better model found at epoch 9 with error_rate value: 0.3138567805290222. Better model found at epoch 10 with error_rate value: 0.30066952109336853. Better model found at epoch 11 with error_rate value: 0.29093122482299805. Epoch 12: reducing lr to 2.606527959586539e-10 Better model found at epoch 12 with error_rate value: 0.2836275100708008. Better model found at epoch 13 with error_rate value: 0.28281599283218384. Better model found at epoch 14 with error_rate value: 0.2801785469055176. . Now that we have got some decent accuracy let us try to save the model and interpret from it. . In the following cell, I . Load the best weights saved by the callbacks during training. | Convert the model back to use 32 bit precision. | Export the model as a whole. | Export the weights alone. | . learn.load(&quot;best&quot;); learn.to_fp32() learn.export(&quot;pokemon_resnet18_st1.pkl&quot;) learn.save(&quot;pokemon_resnet18_st1_wgts&quot;) . Model Interpretation . It is very important that we get to know what the model has learnt from the training process. We can do that with the help of ClassificationInterpretation class from the fastai library. . interp = ClassificationInterpretation.from_learner(learn) # Get the instances where the model has made the most error (by loss value) in the validation set. losses,idxs = interp.top_losses() # Check whether the values are all of same length as the validation set len(data.valid_ds)==len(losses)==len(idxs) . True . Interpret the images where the model made errors during the validation. . The cell below shows . the image. | the model&#39;s prediction of that image. | the actual label of that image. | the loss and probability(the extent to which the model is sure about it&#39;s prediction). | . . You can notice that the image has some of it&#39;s regions blighted, as far I know these are the regions that the model looked at to make the prediction for the corresponding image. . interp.plot_top_losses(9, figsize=(15,11)) . Let us also see which pokemon have confused the model the most. . interp.most_confused(min_val=3) . [(&#39;Sharpedo(Mega)&#39;, &#39;Sharpedo&#39;, 7), (&#39;Moltres&#39;, &#39;Rapidash&#39;, 4), (&#39;Thundurus(Incarnate)&#39;, &#39;Thundurus(Therian)&#39;, 4), (&#39;Charizard(Mega Y)&#39;, &#39;Charizard&#39;, 3), (&#39;Greninja&#39;, &#39;Greninja(Ash)&#39;, 3), (&#39;Groudon(Primal)&#39;, &#39;Incineroar&#39;, 3), (&#39;Latias(Mega)&#39;, &#39;Latios(Mega)&#39;, 3), (&#39;Nidoran(Female)&#39;, &#39;Nidorina&#39;, 3)] . Apart from the 2nd one in this list, You can see why the model was confused generally, most of it&#39;s confusion stem from the evolved species of the same pokemon. . . Let&#39;s try to train the model a little bit differently this time. . learn.load(&#39;best&#39;); . Till now we have been training only the tail region of the model (i.e.) only the last two/ three layers of our model, so essentially this model is almost same as the model which was pretrained on 1000 categories of the ImageNet dataset with some minor tweaks for our problem here. We have some options to improve the model, which are . Train all the layers so that the model can adapt to the current classification problem. We do that by unfreeze(). | Train with a very low learning rate so that it does&#39;nt forget the learnings from the pretrained weights. | . . Let&#39;s see how well we can improve the model. . learn.to_fp16() learn.unfreeze() . Before we start training again, We need to figure out at what speed the neural network should learn, this is controlled by the learning rate parameter and finding a value for is crucial to the training process. . Luckily the fastai&#39;s lr_find method will help us do just the same. . learn.lr_find(start_lr=1e-20) # Plot the learning rates and the corresponding losses. learn.recorder.plot(suggestion=True) # Get the suggested learning rate min_grad_lr = learn.recorder.min_grad_lr . Min numerical gradient: 9.77E-17 Min loss divided by 10: 6.46E-09 . Use the same callbacks as before and train for 30 epochs. . learn.fit_one_cycle(30, min_grad_lr, callbacks=callbacks_list) . epoch train_loss valid_loss error_rate time . 0 | 2.648827 | 1.461440 | 0.280179 | 01:08 | . 1 | 2.687755 | 1.460599 | 0.282004 | 01:08 | . 2 | 2.646746 | 1.471151 | 0.281802 | 01:07 | . 3 | 2.647440 | 1.466154 | 0.284033 | 01:07 | . 4 | 2.687051 | 1.459437 | 0.280179 | 01:07 | . 5 | 2.656536 | 1.468453 | 0.284236 | 01:07 | . 6 | 2.646480 | 1.469294 | 0.280787 | 01:08 | . 7 | 2.707206 | 1.462577 | 0.281802 | 01:08 | . 8 | 2.650942 | 1.462410 | 0.283222 | 01:07 | . 9 | 2.657768 | 1.457848 | 0.279976 | 01:07 | . 10 | 2.689249 | 1.459695 | 0.281193 | 01:07 | . 11 | 2.656215 | 1.463556 | 0.282613 | 01:07 | . 12 | 2.715505 | 1.461581 | 0.282410 | 01:09 | . 13 | 2.689469 | 1.462295 | 0.282410 | 01:08 | . 14 | 2.685328 | 1.460551 | 0.283222 | 01:08 | . 15 | 2.624705 | 1.458205 | 0.283222 | 01:10 | . 16 | 2.675736 | 1.468264 | 0.283628 | 01:11 | . 17 | 2.641450 | 1.461090 | 0.281193 | 01:10 | . 18 | 2.662758 | 1.455160 | 0.283425 | 01:12 | . 19 | 2.662972 | 1.459052 | 0.283019 | 01:13 | . 20 | 2.711507 | 1.464223 | 0.282207 | 01:13 | . 21 | 2.697404 | 1.463553 | 0.283425 | 01:13 | . 22 | 2.643310 | 1.462558 | 0.280584 | 01:12 | . 23 | 2.657411 | 1.463225 | 0.285048 | 01:12 | . 24 | 2.679297 | 1.467203 | 0.283425 | 01:13 | . 25 | 2.654091 | 1.464559 | 0.281599 | 01:12 | . 26 | 2.619208 | 1.465727 | 0.283222 | 01:12 | . 27 | 2.622938 | 1.466129 | 0.280990 | 01:12 | . 28 | 2.646025 | 1.465645 | 0.284236 | 01:13 | . 29 | 2.679323 | 1.458704 | 0.284033 | 01:13 | . Better model found at epoch 0 with error_rate value: 0.2801785469055176. Better model found at epoch 9 with error_rate value: 0.27997565269470215. Epoch 11: reducing lr to 9.288489603500534e-23 Epoch 17: reducing lr to 5.97347999592849e-23 Epoch 29: reducing lr to 3.9089488838232423e-28 . We can see that the model has improved slightly but not much, other ways that we can try are . Try using a different architecture rather than resnet18. | Add more Image augmentation methods (even though fastai has some reasonable defaults). | . Persist the environment so that we would be able to deploy the model without any problems . !pip freeze &gt; resnet18.txt . Try the model . Curious to try out the model?, I have built a small Flask web app which is hosted here. You can find the code for the same in my github repo. . . The website may take some time to load since it was hosted on a free tier heroku dyno. . That&#39;s it for this post, Please share it if you have found it useful. Don&#39;t hesitate to leave a comment if you find that any of my explanation needs some clarification. .",
            "url": "https://mani2106.github.io/Blog-Posts/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "relUrl": "/pokemon-classifer/image-classification/fastai/2019/06/01/Fast_ai_lesson_2_pokemon_classifier.html",
            "date": " • Jun 1, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "A passionate Data Scientist working on unstructured data to deliver business value, focussing on NLP and Computer vision. I always like to do things hands on. . Old pieces of my work can be found here. . You can also contact me via email @ manimaran_p@outlook.com .",
          "url": "https://mani2106.github.io/Blog-Posts/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mani2106.github.io/Blog-Posts/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}